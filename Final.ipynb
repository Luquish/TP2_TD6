{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Configuración de Parámetros Globales\n",
    "# -----------------------------------\n",
    "random_state = 43992294  # Semilla para reproducibilidad de resultados\n",
    "\n",
    "# -----------------------------------\n",
    "# Importaciones de la Biblioteca Estándar\n",
    "# -----------------------------------\n",
    "import ast  # Manipulación de estructuras de datos en formato de cadena\n",
    "import gc   # Recolección de basura para manejo de memoria\n",
    "import os   # Interacción con el sistema operativo\n",
    "import warnings  # Gestión de advertencias\n",
    "\n",
    "# -----------------------------------\n",
    "# Importaciones de Terceros\n",
    "# -----------------------------------\n",
    "import numpy as np  # Cálculos numéricos eficientes\n",
    "import pandas as pd  # Manipulación y análisis de datos\n",
    "import matplotlib.pyplot as plt  # Visualización de datos\n",
    "import seaborn as sns  # Visualizaciones estadísticas avanzadas\n",
    "from tqdm.auto import tqdm  # Barras de progreso automáticas adaptables a diferentes entornos\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier  # Modelo de Random Forest\n",
    "from sklearn.impute import SimpleImputer  # Imputación de valores faltantes\n",
    "from sklearn.metrics import roc_auc_score, roc_curve  # Métricas de evaluación de modelos\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score  # Técnicas de validación y división de datos\n",
    "from sklearn.neighbors import NearestNeighbors  # Algoritmo de vecinos más cercanos\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler  # Transformaciones de datos\n",
    "from sklearn.compose import ColumnTransformer  # Composición de transformaciones por columna\n",
    "from sklearn.pipeline import Pipeline  # Construcción de pipelines de preprocesamiento y modelado\n",
    "\n",
    "import category_encoders as ce  # Codificadores avanzados para variables categóricas\n",
    "import xgboost as xgb  # Librería optimizada para gradient boosting\n",
    "from xgboost import XGBClassifier  # Clasificador de XGBoost\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK  # Optimización de hiperparámetros con Hyperopt\n",
    "\n",
    "# -----------------------------------\n",
    "# Importaciones Locales\n",
    "# -----------------------------------\n",
    "from tools import (\n",
    "    agrupar_categorias_cython,\n",
    "    custom_one_hot_encoder_cython,\n",
    "    boolean_features_ohe_cython,\n",
    "    agrupar_edades_cython,\n",
    "    expand_action_list_0_cython\n",
    ")  # Funciones optimizadas en Cython para procesamiento de datos específico\n",
    "\n",
    "# -----------------------------------\n",
    "# Configuración de Estilo de Gráficos\n",
    "# -----------------------------------\n",
    "sns.set_theme(style=\"whitegrid\")  # Establece el estilo de seaborn para gráficos más estéticos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "\n",
    "# Funcion para agregar filas que contienen categorías desconocidas o raras (poca frecuencia) al conjunto de entrenamiento\n",
    "def augment_train_data(main_train_df, supplementary_df, umbral_raras=100):\n",
    "    \"\"\"\n",
    "    Agrega filas del dataset suplementario al conjunto de entrenamiento principal\n",
    "    basándose en categorías desconocidas y raras, evitando la duplicación de filas.\n",
    "\n",
    "    Parámetros:\n",
    "    - main_train_df (pd.DataFrame): DataFrame principal de entrenamiento.\n",
    "    - supplementary_df (pd.DataFrame): DataFrame suplementario del cual se extraerán las filas.\n",
    "    - umbral_raras (int): Umbral de frecuencia para considerar una categoría como rara.\n",
    "\n",
    "    Retorna:\n",
    "    - main_train_df (pd.DataFrame): DataFrame de entrenamiento actualizado.\n",
    "    - categorias_desconocidas (dict): Diccionario actualizado de categorías desconocidas.\n",
    "    - categorias_raras (dict): Diccionario actualizado de categorías raras.\n",
    "    \"\"\"\n",
    "    # Definir columnas que no deseas tratar como categóricas\n",
    "    columns_to_exclude = ['auction_list_0', 'action_list_1', 'action_list_2', 'auction_time']\n",
    "\n",
    "    # Identificar columnas categóricas excluyendo las especificadas\n",
    "    categorical_features = main_train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    categorical_features = [col for col in categorical_features if col not in columns_to_exclude]\n",
    "    \n",
    "    # Crear un diccionario para almacenar las categorías desconocidas por columna\n",
    "    categorias_desconocidas = {}\n",
    "    \n",
    "    # Iterar a través de cada columna categórica para identificar categorías desconocidas\n",
    "    for columna in categorical_features:\n",
    "        # Obtener las categorías únicas en el conjunto de entrenamiento\n",
    "        categorias_train = set(main_train_df[columna].dropna().unique())\n",
    "        \n",
    "        # Obtener las categorías únicas en el dataset suplementario\n",
    "        categorias_suplementario = set(supplementary_df[columna].dropna().unique())\n",
    "        \n",
    "        # Identificar las categorías en el dataset suplementario que no están en el entrenamiento\n",
    "        desconocidas = categorias_suplementario - categorias_train\n",
    "        \n",
    "        # Almacenar las categorías desconocidas en el diccionario como una lista\n",
    "        categorias_desconocidas[columna] = list(desconocidas)\n",
    "    \n",
    "    # Inicializar el diccionario para almacenar las categorías raras por columna\n",
    "    categorias_raras = {}\n",
    "    \n",
    "    # Identificar categorías raras en el conjunto de entrenamiento\n",
    "    for columna in categorical_features:\n",
    "        # Contar la frecuencia de cada categoría\n",
    "        frecuencia = main_train_df[columna].value_counts()\n",
    "        \n",
    "        # Identificar categorías que aparecen menos de umbral_raras veces\n",
    "        raras = frecuencia[frecuencia < umbral_raras].index.tolist()\n",
    "        \n",
    "        # Almacenar en el diccionario\n",
    "        categorias_raras[columna] = raras\n",
    "    \n",
    "    # Crear una máscara booleana para filas con categorías desconocidas o raras\n",
    "    mask_desconocidas = pd.Series([False] * len(supplementary_df))\n",
    "    mask_raras = pd.Series([False] * len(supplementary_df))\n",
    "    \n",
    "    for columna in categorical_features:\n",
    "        # Actualizar la máscara para categorías desconocidas\n",
    "        if categorias_desconocidas[columna]:\n",
    "            mask_desconocidas = mask_desconocidas | supplementary_df[columna].isin(categorias_desconocidas[columna])\n",
    "        \n",
    "        # Actualizar la máscara para categorías raras\n",
    "        if categorias_raras[columna]:\n",
    "            mask_raras = mask_raras | supplementary_df[columna].isin(categorias_raras[columna])\n",
    "    \n",
    "    # Combinar ambas máscaras\n",
    "    mask_total = mask_desconocidas | mask_raras\n",
    "    \n",
    "    # Filtrar filas únicas a agregar\n",
    "    filas_a_agregar = supplementary_df[mask_total].drop_duplicates()\n",
    "    \n",
    "    # Mostrar información de agregación\n",
    "    total_agregadas = len(filas_a_agregar)\n",
    "    print(f\"\\nAgregando {total_agregadas} filas del dataset suplementario basadas en categorías desconocidas o raras.\")\n",
    "    \n",
    "    # Agregar las filas al conjunto de entrenamiento\n",
    "    main_train_df = pd.concat([main_train_df, filas_a_agregar], ignore_index=True)\n",
    "    \n",
    "    # Actualizar los diccionarios eliminando las categorías que ya han sido agregadas\n",
    "    for columna in categorical_features:\n",
    "        # Actualizar categorías desconocidas\n",
    "        if categorias_desconocidas[columna]:\n",
    "            categorias_agregadas = filas_a_agregar[columna].unique().tolist()\n",
    "            categorias_desconocidas[columna] = [cat for cat in categorias_desconocidas[columna] if cat not in categorias_agregadas]\n",
    "        \n",
    "        # Actualizar categorías raras\n",
    "        if categorias_raras[columna]:\n",
    "            # Recontar la frecuencia después de agregar\n",
    "            frecuencia = main_train_df[columna].value_counts()\n",
    "            nuevas_raras = frecuencia[frecuencia < umbral_raras].index.tolist()\n",
    "            categorias_raras[columna] = nuevas_raras\n",
    "    \n",
    "    return main_train_df, categorias_desconocidas, categorias_raras\n",
    "\n",
    "# Función para para crear columnas binarias a partir de las columnas que contienen variables en listas ('auction_list_0', 'action_list_1' y 'action_list_2')\n",
    "def expand_list_dummies_cython(df, column, delimiter='|', prefix=None, suffix=None):\n",
    "    \"\"\"\n",
    "    Expande una columna que contiene listas en múltiples columnas binarias usando un one-hot encoder optimizado con Cython.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame de pandas.\n",
    "    - column (str): Nombre de la columna a expandir.\n",
    "    - delimiter (str): Delimitador a usar en get_dummies (por defecto '|').\n",
    "    - prefix (str, optional): Prefijo para las nuevas columnas binarias.\n",
    "    - suffix (str, optional): Sufijo para las nuevas columnas binarias.\n",
    "    \n",
    "    Returns:\n",
    "    - df_expanded (pd.DataFrame): DataFrame con las nuevas columnas binarias añadidas y la columna original eliminada.\n",
    "    \"\"\"\n",
    "    print(f\"Comenzando la expansión de la columna: '{column}'\")\n",
    "    \n",
    "    # Copiar el DataFrame para evitar modificar el original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Reemplazar NaN por listas vacías\n",
    "    print(f\"Reemplazando NaN en la columna '{column}' por listas vacías.\")\n",
    "    df[column] = df[column].fillna('[]')\n",
    "    \n",
    "    # Definir la función de parsing con impresión de errores\n",
    "    def parse_list(x):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(x)\n",
    "            if isinstance(parsed, list):\n",
    "                # Convertir todos los elementos a strings\n",
    "                return [str(item) for item in parsed]\n",
    "            else:\n",
    "                # Si no es una lista, tratar como un solo elemento\n",
    "                return [str(x)]\n",
    "        except (ValueError, SyntaxError):\n",
    "            # En caso de error al parsear, retornar una lista vacía\n",
    "            return []\n",
    "    \n",
    "    # Aplicar la función de parsing con una barra de progreso\n",
    "    df[column] = df[column].progress_apply(parse_list)\n",
    "    \n",
    "    # Convertir la columna en una lista de listas para pasarla a la función Cythonizada\n",
    "    data_list = df[column].tolist()\n",
    "    \n",
    "    # Llamar a la función optimizada en Cython\n",
    "    unique_categories, binary_matrix = custom_one_hot_encoder_cython(data_list)\n",
    "    print(f\"Codificación completada. {len(unique_categories)} categorías únicas encontradas.\")\n",
    "    \n",
    "    # Crear un DataFrame binario usando la matriz devuelta por Cython\n",
    "    binary_df = pd.DataFrame(binary_matrix, index=df.index, columns=unique_categories)\n",
    "\n",
    "    if prefix:\n",
    "        binary_df = binary_df.add_prefix(f\"{prefix}_\")\n",
    "    if suffix:\n",
    "        binary_df = binary_df.add_suffix(f\"_{suffix}\")\n",
    "    \n",
    "    # Concatenar las columnas binarias al DataFrame original con una barra de progreso\n",
    "    print(\"Concatenando las columnas binarias al DataFrame original.\")\n",
    "    for col in tqdm(binary_df.columns, desc=\"Concatenando columnas binarias\"):\n",
    "        if col in df.columns:\n",
    "            df[col] = np.where((df[col] == 1) | (binary_df[col] == 1), 1, 0)\n",
    "        else:\n",
    "            df[col] = binary_df[col]\n",
    "    \n",
    "    # Eliminar la columna original ya que ha sido expandida\n",
    "    print(f\"Eliminando la columna original '{column}' del DataFrame.\")\n",
    "    df = df.drop(columns=[column])\n",
    "    \n",
    "    print(f\"Expansión de la columna '{column}' completada exitosamente.\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Función para simular One-hot encode con las columnas booleanas\n",
    "def boolean_features_ohe(df, columns_to_encode=['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2']):\n",
    "    \"\"\"\n",
    "    Realiza one-hot encoding en columnas booleanas especificadas utilizando una función optimizada con Cython.\n",
    "    Además, muestra el progreso del procesamiento utilizando tqdm y añade comentarios explicativos.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame original que contiene las columnas booleanas a codificar.\n",
    "    - columns_to_encode (list): Lista de nombres de columnas booleanas a codificar.\n",
    "\n",
    "    Retorna:\n",
    "    - df_expanded (pd.DataFrame): DataFrame con las nuevas columnas codificadas añadidas y las columnas booleanas originales eliminadas.\n",
    "    \"\"\"\n",
    "    # Copiar el DataFrame para evitar modificar el original\n",
    "    df = df.copy()\n",
    "    print(\"Inicio del proceso de one-hot encoding para las columnas booleanas especificadas.\")\n",
    "\n",
    "    # Paso 1: Encontrar todos los valores únicos en las columnas a codificar\n",
    "    unique_values_set = set()\n",
    "    print(\"Recopilando valores únicos de las columnas a codificar:\")\n",
    "    for col in tqdm(columns_to_encode, desc=\"Procesando columnas para valores únicos\"):\n",
    "        unique_vals_col = df[col].dropna().unique()\n",
    "        unique_values_set.update(unique_vals_col)\n",
    "    unique_values = sorted(unique_values_set)\n",
    "    print(f\"Valores únicos encontrados: {unique_values}\")\n",
    "\n",
    "    # Paso 2: Convertir las columnas a listas de listas para ser procesadas en Cython\n",
    "    list_data = []\n",
    "    print(\"Convirtiendo las columnas booleanas a listas de listas para Cython:\")\n",
    "    for col in tqdm(columns_to_encode, desc=\"Convertir columnas a listas\"):\n",
    "        column_list = df[col].astype(str).tolist()  # Mantener los valores como strings\n",
    "        list_data.append(column_list)\n",
    "    print(\"Conversión completada.\")\n",
    "\n",
    "    # Paso 3: Procesar los datos con la función optimizada en Cython\n",
    "    print(\"Realizando one-hot encoding utilizando la función optimizada en Cython:\")\n",
    "    ohe_result = boolean_features_ohe_cython(list_data, unique_values)\n",
    "    print(\"One-hot encoding completado.\")\n",
    "\n",
    "    # Paso 4: Convertir el resultado de Cython a un DataFrame, alineando el índice con df\n",
    "    print(\"Creando el DataFrame de columnas codificadas:\")\n",
    "    ohe_df = pd.DataFrame(ohe_result, columns=unique_values, index=df.index)\n",
    "    print(f\"DataFrame de one-hot encoding creado con {len(ohe_df.columns)} columnas y {ohe_df.shape[0]} filas.\")\n",
    "\n",
    "    # Paso 5: Concatenar las nuevas columnas codificadas al DataFrame original\n",
    "    print(\"Concatenando las columnas codificadas al DataFrame original:\")\n",
    "    df_expanded = pd.concat([df, ohe_df], axis=1)\n",
    "    print(f\"Concatenación completada. El DataFrame ahora tiene {df_expanded.shape[1]} columnas y {df_expanded.shape[0]} filas.\")\n",
    "\n",
    "    # Paso 6: Eliminar las columnas booleanas originales del DataFrame\n",
    "    print(\"Eliminando las columnas booleanas originales del DataFrame:\")\n",
    "    df_expanded.drop(columns=columns_to_encode, inplace=True)\n",
    "    print(f\"Columnas eliminadas: {columns_to_encode}\")\n",
    "\n",
    "    print(\"Proceso de one-hot encoding finalizado exitosamente.\\n\")\n",
    "\n",
    "    return df_expanded\n",
    "\n",
    "# Función para extender características temporales\n",
    "def time_features_extension(df):\n",
    "    \"\"\"\n",
    "    Procesa las características temporales del DataFrame y agrega nuevas columnas derivadas relacionadas con el tiempo y festividades.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convertir 'auction_time' de timestamp a una fecha legible\n",
    "    df['auction_time'] = pd.to_datetime(df['auction_time'], unit='s')\n",
    "\n",
    "    # Reemplazar NaN en 'timezone_offset' por 0\n",
    "    df['timezone_offset'] = df['timezone_offset'].fillna(0)\n",
    "\n",
    "    # Ajustar la hora según el 'timezone_offset' para obtener la hora local\n",
    "    df['auction_time_local'] = df.apply(\n",
    "        lambda row: row['auction_time'] + pd.DateOffset(hours=row['timezone_offset']), axis=1\n",
    "    )\n",
    "\n",
    "    # Crear la columna 'week_day' (1 para lunes, 7 para domingo)\n",
    "    df['week_day'] = df['auction_time_local'].dt.weekday + 1\n",
    "\n",
    "    # Crear la columna 'moment_of_the_day' (1 para temprano, 2 para tarde, 3 para noche)\n",
    "    df['moment_of_the_day'] = pd.cut(df['auction_time_local'].dt.hour, \n",
    "                                     bins=[0, 12, 18, 24], labels=[1, 2, 3], include_lowest=True, right=False)\n",
    "\n",
    "    # Eliminar las columnas originales 'auction_time', 'timezone_offset' y 'auction_time_local'\n",
    "    df.drop(columns=['auction_time', 'timezone_offset', 'auction_time_local'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Función para agrupar de edades en rangos numéricos\n",
    "def age_group(df, columna_edad):\n",
    "    \"\"\"\n",
    "    Agrupa las edades en rangos numéricos utilizando Cython para mejorar el rendimiento.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene la columna de edades.\n",
    "    - columna_edad (str): Nombre de la columna que contiene las edades.\n",
    "\n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame con la nueva columna 'age_group' que representa el rango de edad.\n",
    "    \"\"\"\n",
    "    # Convertir la columna de edad a una lista\n",
    "    edades = df[columna_edad].tolist()\n",
    "\n",
    "    # Usar la función Cythonizada para agrupar las edades\n",
    "    df['age_group'] = agrupar_edades_cython(edades)\n",
    "\n",
    "    # Eliminar la columna original de edades\n",
    "    df.drop(columns=[columna_edad], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Función para agrupar action_list_0 a auction_list_0\n",
    "def expand_action_list_0(df):\n",
    "    \"\"\"\n",
    "    Expande la columna 'action_list_0' en valores únicos y marca con 1 las columnas existentes o las crea si es necesario.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene la columna 'action_list_0' y otras columnas de listas ya expandidas.\n",
    "\n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame actualizado con las columnas de valores únicos de 'action_list_0'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convertir la columna 'action_list_0' y las columnas existentes a listas\n",
    "    action_list_0 = df['action_list_0'].tolist()\n",
    "    existing_columns = df.columns.tolist()\n",
    "    \n",
    "    # Inicializar la matriz actual\n",
    "    current_matrix = df.values.tolist()\n",
    "\n",
    "    # Llamar a la función Cythonizada\n",
    "    updated_matrix = expand_action_list_0_cython(action_list_0, existing_columns, current_matrix)\n",
    "\n",
    "    # Convertir la matriz actualizada de vuelta a un DataFrame\n",
    "    df_updated = pd.DataFrame(updated_matrix, columns=existing_columns)\n",
    "\n",
    "    # Eliminar la columna 'action_list_0'\n",
    "    df_updated.drop(columns=['action_list_0'], inplace=True)\n",
    "\n",
    "    return df_updated\n",
    "\n",
    "# Función para concatenar las categorias de cada nivel y reducir a una columna\n",
    "def create_level_combination(df):\n",
    "    \"\"\"\n",
    "    Creates a new column 'level_combination' by concatenating the first three characters \n",
    "    of each 'action_categorical' level columns and removes the original level columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame containing the columns:\n",
    "      'action_categorical_0', 'action_categorical_1', 'action_categorical_2', \n",
    "      'action_categorical_3', 'action_categorical_4'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with the new 'level_combination' column and without the original level columns.\n",
    "    \"\"\"\n",
    "    level_columns = [\n",
    "        'action_categorical_0',\n",
    "        'action_categorical_1',\n",
    "        'action_categorical_2',\n",
    "        'action_categorical_3',\n",
    "        'action_categorical_4'\n",
    "    ]\n",
    "    df['level_combination'] = df[level_columns].astype(str).apply(\n",
    "        lambda x: ''.join([s[:3] for s in x]), axis=1\n",
    "    )\n",
    "    df.drop(columns=level_columns, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Función para guardar el area de las publicidades (Heigh x Width) a columna\n",
    "def hxw_column(df):\n",
    "    \"\"\"\n",
    "    Crea una nueva columna 'hxw' multiplicando 'creative_height' y 'creative_width'.\n",
    "    Si alguno de los dos tiene un NaN, 'hxw' se establece en 0.\n",
    "    Elimina las columnas originales 'creative_height' y 'creative_width'.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene las columnas 'creative_height' y 'creative_width'.\n",
    "    \n",
    "    Retorna:\n",
    "    - pd.DataFrame: DataFrame con la nueva columna 'hxw' añadida y las columnas originales eliminadas.\n",
    "    \"\"\"\n",
    "    df['hxw'] = df['creative_height'] * df['creative_width']\n",
    "    df.loc[df['creative_height'].isna() | df['creative_width'].isna(), 'hxw'] = 0\n",
    "    df.drop(columns=['creative_height', 'creative_width'], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Función para para pasar generos a numeros\n",
    "def encode_gender(df):\n",
    "    \"\"\"\n",
    "    Reemplaza los valores de la columna 'gender' de la siguiente manera:\n",
    "    'f' -> 1, 'm' -> 2, 'o' -> 0 y NaN -> -1.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene la columna 'gender'.\n",
    "    \n",
    "    Retorna:\n",
    "    - pd.DataFrame: DataFrame con la columna 'gender' codificada.\n",
    "    \"\"\"\n",
    "    df['gender'] = df['gender'].map({'f': 1, 'm': 2, 'o': 0}).fillna(-1).astype(int)\n",
    "    return df\n",
    "\n",
    "# Función para pasar creative_categorical_11, creative_categorical_9 y creative_categorical_10 a dos columnas\n",
    "def creatives2unique(df):\n",
    "    \"\"\"\n",
    "    Crea o actualiza dos columnas en el DataFrame, una para cada valor único en las columnas\n",
    "    'creative_categorical_11', 'creative_categorical_10', y 'creative_categorical_9'.\n",
    "    Si las columnas ya existen, actualiza los valores a 1 donde ese valor aparece en alguna\n",
    "    de las tres columnas en esa fila.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene las columnas 'creative_categorical_11',\n",
    "                         'creative_categorical_10', y 'creative_categorical_9'.\n",
    "\n",
    "    Retorna:\n",
    "    - pd.DataFrame: DataFrame con las nuevas columnas añadidas o actualizadas.\n",
    "    \"\"\"\n",
    "    unique_values = {'65dcab89', '43c867fd'}\n",
    "    columns_to_check = [\n",
    "        'creative_categorical_11',\n",
    "        'creative_categorical_10',\n",
    "        'creative_categorical_9'\n",
    "    ]\n",
    "\n",
    "    for val in unique_values:\n",
    "        if val in df.columns:\n",
    "            # Si la columna ya existe, actualizamos los valores a 1 donde corresponde\n",
    "            df[val] = df[val] | df[columns_to_check].eq(val).any(axis=1).astype(int)\n",
    "        else:\n",
    "            # Si no existe, creamos la columna con 1 donde corresponde\n",
    "            df[val] = df[columns_to_check].eq(val).any(axis=1).astype(int)\n",
    "\n",
    "    # Eliminar las columnas originales\n",
    "    df.drop(columns=columns_to_check, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Función para juntar todas las variables categoricas menos 'device_id'  y hacer OHE\n",
    "def process_combineta(df):\n",
    "    \"\"\"\n",
    "    Procesa las columnas proporcionadas en combineta, creando un set con valores únicos,\n",
    "    y generando columnas binarias para cada uno de esos valores. Si la columna ya existe,\n",
    "    actualiza las filas con un 1 donde corresponda. Luego, elimina las columnas originales.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame original.\n",
    "    - combineta_columns (list): Lista de columnas a procesar.\n",
    "\n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame con las columnas binarias añadidas y las columnas originales eliminadas.\n",
    "    \"\"\"\n",
    "    combineta_columns = ['creative_categorical_0', 'creative_categorical_5', 'auction_categorical_0', 'auction_categorical_1', 'auction_categorical_11', 'auction_categorical_7', 'auction_categorical_8', 'auction_categorical_9', 'action_categorical_6', 'action_categorical_7', 'auction_categorical_3', 'auction_categorical_4', 'auction_categorical_5', 'auction_categorical_6', 'auction_categorical_10', 'auction_categorical_12', 'creative_categorical_1', 'creative_categorical_12', 'creative_categorical_2', 'creative_categorical_3', 'creative_categorical_4', 'creative_categorical_6', 'creative_categorical_7', 'creative_categorical_8']\n",
    "\n",
    "    # Unir todas las columnas de combineta en una sola columna de listas\n",
    "    df['combined_combineta'] = df[combineta_columns].astype(str).agg(\n",
    "        lambda x: '[' + ', '.join([f\"'{str(item).strip()}'\" for item in x if item != 'nan']) + ']', axis=1)\n",
    "\n",
    "    # Usar la función expand_list_dummies_cython para descomponer la lista y crear las columnas binarias\n",
    "    df = expand_list_dummies_cython(df, 'combined_combineta')\n",
    "    \n",
    "    df.drop(columns=combineta_columns, inplace=True, errors='raise')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# # Función para procesamiento optimizado de un DataFrame\n",
    "def process_optimized(df):\n",
    "    \"\"\"\n",
    "    Aplica una serie de transformaciones al DataFrame utilizando una función Cython optimizada.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame a procesar.\n",
    "    \n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame procesado.\n",
    "    \"\"\"\n",
    "    # Definir el número total de pasos para la barra de progreso\n",
    "    total_steps = 14\n",
    "    \n",
    "    # Inicializar la barra de progreso\n",
    "    with tqdm(total=total_steps, desc=\"Procesando DataFrame\", unit=\"paso\") as pbar:\n",
    "        \n",
    "        print(\"Comenzando el procesamiento optimizado del DataFrame.\")\n",
    "        print(\"Eliminando columnas innecesarias.\")\n",
    "        df = df.drop('action_categorical_5', axis=1)\n",
    "        df = df.drop('auction_categorical_2', axis=1)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Expansión de columnas temporales\")\n",
    "        df = time_features_extension(df)\n",
    "        pbar.update(1)\n",
    "\n",
    "        df = age_group(df, 'auction_age')\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Agrupando columnas de nivel\")\n",
    "        df = create_level_combination(df)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Modificando columnas de genero\")\n",
    "        df = encode_gender(df)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        print(\"Modificando columna de video\")\n",
    "        df['has_video'] = df['has_video'].apply(lambda x: 1 if x == True else 0)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Juntando medidas\")\n",
    "        df = hxw_column(df)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Expansión de columnas booleanas.\")\n",
    "        df = boolean_features_ohe(df)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Creando columnas de creatividad\")\n",
    "        df = creatives2unique(df)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        columns_to_expand = ['auction_list_0', 'action_list_1']\n",
    "\n",
    "        # Bucle para realizar las operaciones\n",
    "        for col in columns_to_expand:\n",
    "            print(f\"Expansión de columnas de listas para {col}.\")\n",
    "            \n",
    "            # Crear la variable 'idx_position' con la última columna antes de la expansión\n",
    "            idx_position = df.columns.get_loc(df.columns[-1])\n",
    "            \n",
    "            # Expansión de la columna\n",
    "            df = expand_list_dummies_cython(df, col)\n",
    "            pbar.update(1)\n",
    "\n",
    "            if col == 'action_list_1':\n",
    "                df = expand_list_dummies_cython(df,'action_list_2')\n",
    "                pbar.update(1)\n",
    "            \n",
    "            if col == 'auction_list_0':\n",
    "                # Bucle para recorrer las columnas que empiezan con 'AND' o 'APL'\n",
    "                if 'AND-APL' not in df.columns:\n",
    "                    df['AND-APL'] = 0  # Inicializar la columna 'AND-APL'\n",
    "\n",
    "                for column in df.columns:\n",
    "                    if column.startswith('AND') or column.startswith('APL'):\n",
    "                        # Poner un 1 en 'AND-APL' si la columna actual tiene un 1 en esa fila\n",
    "                        df['AND-APL'] = df['AND-APL'] | df[column]\n",
    "\n",
    "                # Eliminar todas las columnas que empiezan con 'AND' o 'APL' excepto la columna 'AND-APL'\n",
    "                columns_to_drop = [column for column in df.columns if (column.startswith('AND') or column.startswith('APL')) and column != 'AND-APL']\n",
    "                df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "                print(\"Complementamos con la columna 'action_list_0'\")\n",
    "                df = expand_action_list_0(df)\n",
    "                pbar.update(1)\n",
    "\n",
    "            # Crear la lista de columnas numéricas a partir de la siguiente columna después de 'idx'\n",
    "            categorical_num = df.iloc[:, idx_position:].select_dtypes(include=['number']).columns\n",
    "\n",
    "            # Eliminar columnas numéricas con menos de 1000 valores iguales a 1\n",
    "            for column in categorical_num:\n",
    "                if (df[column] == 1).sum() < 1000 and column != 'AND-APL':\n",
    "                    df.drop(column, axis=1, inplace=True)\n",
    "    \n",
    "        \n",
    "        print(\"Agrupando categorias poco frecuentes\")\n",
    "\n",
    "        categorical_str = df.select_dtypes(include=['object']).columns\n",
    "        categorical_str = categorical_str[categorical_str != 'device_id']\n",
    "        \n",
    "        categorical_num = df.select_dtypes(include=['number']).columns\n",
    "        \n",
    "        # Convertir a matriz bidimensional\n",
    "        data_matrix = df[categorical_str].values.tolist()\n",
    "        data_matrix_cython = [list(row) for row in data_matrix]\n",
    "        \n",
    "        # Llamar a la función Cythonizada\n",
    "        df_cython_data = agrupar_categorias_cython(\n",
    "            categorical_features=categorical_str.tolist(),\n",
    "            data=data_matrix_cython,\n",
    "            umbral=1000  # Umbral de frecuencia para considerar una categoría como rara\n",
    "        )\n",
    "\n",
    "        # Reasignar los datos al DataFrame\n",
    "        for i, col in enumerate(categorical_str):\n",
    "            df[col] = [row[i] for row in df_cython_data]\n",
    "\n",
    "        idx_position = df.columns.get_loc(df.columns[-1])\n",
    "        \n",
    "        df = process_combineta(df)\n",
    "\n",
    "        # Crear la lista de columnas numéricas a partir de la siguiente columna después de 'idx'\n",
    "        categorical_num = df.iloc[:, idx_position:].select_dtypes(include=['number']).columns\n",
    "\n",
    "        # Eliminar columnas numéricas con menos de 1000 valores iguales a 1\n",
    "        for column in categorical_num:\n",
    "            if (df[column] == 1).sum() < 1000:\n",
    "                df.drop(column, axis=1, inplace=True)\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Función para calcular la estadística de Cramér's V\n",
    "def cramers_v(confusion_matrix):\n",
    "    \"\"\"\n",
    "    Calcula la estadística de Cramér's V para medir la asociación entre dos variables categóricas.\n",
    "\n",
    "    Parameters:\n",
    "    - confusion_matrix: Matriz de confusión (tabla de contingencia) entre dos variables.\n",
    "\n",
    "    Returns:\n",
    "    - Cramér's V: Valor entre 0 y 1 que indica la fuerza de la asociación.\n",
    "    \"\"\"\n",
    "    # Calcular el estadístico chi-cuadrado\n",
    "    chi2 = chi2_contingency(confusion_matrix, correction=False)[0]\n",
    "    # Número total de observaciones\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    # Obtener el número de filas y columnas de la matriz de confusión\n",
    "    r, k = confusion_matrix.shape\n",
    "    if min(r, k) == 1:\n",
    "        return np.nan  # Evitar dividir por cero\n",
    "    # Calcular Cramér's V\n",
    "    return np.sqrt(chi2 / (n * (min(r, k) - 1)))\n",
    "\n",
    "# Función para manejar valores nulos\n",
    "def handle_none(value):\n",
    "    return 'None' if value is None else str(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis e Ingenieria de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenamos datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_21 = pd.read_csv('data/ctr_21.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_20 = pd.read_csv('data/ctr_20.csv')\n",
    "train_data_19 = pd.read_csv('data/ctr_19.csv')\n",
    "train_data_18 = pd.read_csv('data/ctr_18.csv')\n",
    "train_data_17 = pd.read_csv('data/ctr_17.csv')\n",
    "train_data_16 = pd.read_csv('data/ctr_16.csv')\n",
    "train_data_15 = pd.read_csv('data/ctr_15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplementary_datasets = [\n",
    "    ('1', train_data_20),\n",
    "    ('2', train_data_19),\n",
    "    ('3', train_data_18),\n",
    "    ('4', train_data_17),\n",
    "    ('5', train_data_16),\n",
    "    ('6', train_data_15)\n",
    "]\n",
    "\n",
    "train_data = train_data_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nombre, dataset in supplementary_datasets:\n",
    "    print(f\"\\nProcesando dataset {nombre}/{len(supplementary_datasets)}\")\n",
    "    train_data, categorias_desconocidas, categorias_raras = augment_train_data(train_data, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape[0])\n",
    "\n",
    "print(train_data_20.shape[0] + train_data_19.shape[0] + train_data_18.shape[0] + train_data_17.shape[0] + train_data_16.shape[0] + train_data_15.shape[0] + train_data_21.shape[0])\n",
    "\n",
    "del train_data_20, train_data_19, train_data_18, train_data_17, train_data_16, train_data_15, train_data_21\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analizamos Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_data_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorias numericas\n",
    "numeric_columns = train_data.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Categorias categóricas\n",
    "categorical_features = train_data.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribucion de 'Label' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir la cantidad de filas del dataset combinado\n",
    "print(f\"Cantidad de filas en el dataset combinado: {train_data.shape[0]}\")\n",
    "print(f\"Cantidad de columnas en el dataset combinado: {train_data.shape[1]}\")\n",
    "\n",
    "# Ver porcentaje de clics vs no clics en la columna Label\n",
    "label_counts = train_data['Label'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPorcentaje de clics (1) y no clics (0):\")\n",
    "print(label_counts)\n",
    "\n",
    "# Cantidad de clics (1) y no clics (0)\n",
    "label_counts_abs = train_data['Label'].value_counts()\n",
    "print(\"\\nCantidad de clics (1) y no clics (0):\")\n",
    "print(label_counts_abs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlaciones con 'Label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar las columnas numéricas\n",
    "numeric_columns = train_data.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Verificar que 'Label' esté en las columnas numéricas\n",
    "if 'Label' not in numeric_columns:\n",
    "    train_data['Label'] = train_data['Label'].astype(float)\n",
    "    numeric_columns = train_data.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Identificar las columnas categóricas\n",
    "categorical_columns = train_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Remover 'Label' de las listas de características si está presente\n",
    "if 'Label' in numeric_columns:\n",
    "    numeric_columns.remove('Label')\n",
    "if 'Label' in categorical_columns:\n",
    "    categorical_columns.remove('Label')\n",
    "\n",
    "# Análisis de correlación con 'Label' para características numéricas\n",
    "correlation_with_label = train_data[numeric_columns + ['Label']].corr()['Label'].abs().sort_values(ascending=False)\n",
    "\n",
    "# Eliminar 'Label' de la lista\n",
    "correlation_with_label = correlation_with_label.drop('Label')\n",
    "\n",
    "# Crear un DataFrame para las correlaciones numéricas\n",
    "corr_num_df = pd.DataFrame({\n",
    "    'Feature': correlation_with_label.index,\n",
    "    'CorrelationValue': correlation_with_label.values,\n",
    "    'Type': 'Numeric'\n",
    "})\n",
    "\n",
    "# Análisis de asociación con 'Label' para características categóricas usando Cramer's V\n",
    "cramers_v_results = []\n",
    "\n",
    "for col in categorical_columns:\n",
    "    confusion_matrix = pd.crosstab(train_data[col], train_data['Label'])\n",
    "    if confusion_matrix.shape[0] < 2 or confusion_matrix.shape[1] < 2:\n",
    "        # Evitar cálculos de Cramer's V cuando no hay suficientes categorías\n",
    "        cramers_v_val = np.nan\n",
    "    else:\n",
    "        cramers_v_val = cramers_v(confusion_matrix)\n",
    "    cramers_v_results.append({'Feature': col, 'CorrelationValue': cramers_v_val, 'Type': 'Categorical'})\n",
    "\n",
    "# Crear un DataFrame para las correlaciones categóricas\n",
    "corr_cat_df = pd.DataFrame(cramers_v_results).dropna().sort_values(by='CorrelationValue', ascending=False)\n",
    "\n",
    "# Combinar ambos DataFrames\n",
    "combined_corr_df = pd.concat([corr_num_df, corr_cat_df], ignore_index=True)\n",
    "\n",
    "# Ordenar por la magnitud de la correlación\n",
    "combined_corr_df = combined_corr_df.sort_values(by='CorrelationValue', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Análisis de datos faltantes\n",
    "missing_data = train_data.isnull().sum() / len(train_data) * 100\n",
    "missing_data = missing_data.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nPorcentaje de Datos Faltantes por Característica:\")\n",
    "for feature, percentage in missing_data.items():\n",
    "    if percentage > 0:  # Solo imprimir características con datos faltantes\n",
    "        print(f\"{feature}: {percentage:.2f}%\")\n",
    "\n",
    "# Visualizar las mayores correlaciones con 'Label'\n",
    "plt.figure(figsize=(14, 12))\n",
    "top_correlations = combined_corr_df.head(25)\n",
    "sns.barplot(x=top_correlations['CorrelationValue'], y=top_correlations['Feature'], hue=top_correlations['Type'])\n",
    "plt.title('Top 25 Características Correlacionadas con Label')\n",
    "plt.xlabel('Magnitud de la Correlación')\n",
    "plt.ylabel('Características')\n",
    "plt.legend(title='Tipo de Característica')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de datos faltantes (sin cambios)\n",
    "missing_data = train_data.isnull().sum() / len(train_data) * 100\n",
    "missing_data = missing_data[missing_data > 10].sort_values(ascending=False)  # Filtrar las columnas con más del 10% de datos faltantes\n",
    "\n",
    "# Visualizar datos faltantes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=missing_data.values, y=missing_data.index)\n",
    "plt.title('Percentage of Missing Data by Numerical Feature (>10%)')\n",
    "plt.xlabel('Percentage Missing')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"\\nPercentage of Missing Data by Feature (>10% missing):\")\n",
    "for feature, percentage in missing_data.items():\n",
    "    print(f\"{feature}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valores unicos y su frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train_data.columns:\n",
    "    if train_data[column].value_counts() < 1000:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario para almacenar los valores únicos de cada columna\n",
    "unique_values = {col: set(train_data[col].dropna().unique()) for col in categorical_features}\n",
    "\n",
    "# Diccionario para almacenar las columnas que tienen valores comunes\n",
    "common_columns = {}\n",
    "\n",
    "# Comparar las columnas entre sí para ver qué valores comparten\n",
    "for i in range(len(categorical_features)):\n",
    "    for j in range(i + 1, len(categorical_features)):\n",
    "        col1 = categorical_features[i]\n",
    "        col2 = categorical_features[j]\n",
    "        \n",
    "        # Ver los valores que se repiten entre las dos columnas\n",
    "        common_values = unique_values[col1].intersection(unique_values[col2])\n",
    "        \n",
    "        if common_values:\n",
    "            # Almacenar las columnas con valores comunes\n",
    "            if (col1, col2) not in common_columns:\n",
    "                common_columns[(col1, col2)] = common_values\n",
    "\n",
    "# Ver las columnas que tienen valores comunes y sus unique values\n",
    "for col_pair, common_vals in common_columns.items():\n",
    "    col1, col2 = col_pair\n",
    "    print(f\"\\nValores comunes entre {col1} y {col2}: {common_vals}\")\n",
    "    print(f\"Valores únicos de {col1}: {len(unique_values[col1])}\")\n",
    "    print(f\"Valores únicos de {col2}: {len(unique_values[col2])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame de ejemplo\n",
    "data = {\n",
    "    'auction_boolean_0': ['47980dda', '43c867fd', None, '79ceee49', '47980dda', 'unknown'],\n",
    "    'auction_boolean_1': ['79ceee49', None, '79ceee49', None, '1', '0'],\n",
    "    'auction_boolean_2': ['65dcab89', None, '43c867fd', '65dcab89', '65dcab89', '43c867fd'],\n",
    "    'Label': [0, 1, 0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "# Crear el DataFrame\n",
    "train_data_combined = pd.DataFrame(data)\n",
    "\n",
    "# Seleccionar solo las columnas relevantes\n",
    "df = train_data_combined[['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2', 'Label']]\n",
    "\n",
    "print(\"DataFrame Original:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Convertir el DataFrame a una lista de listas\n",
    "data_list = df.values.tolist()\n",
    "\n",
    "# Definir columnas categóricas y columnas a excluir\n",
    "categorical_features = ['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2']\n",
    "columns_to_exclude = []  # No se excluye ninguna en este ejemplo\n",
    "\n",
    "# Aplicar la función de agrupación de categorías en Cython\n",
    "data_processed = agrupar_categorias_cython(categorical_features, data_list, umbral=2)\n",
    "\n",
    "# Convertir la lista de listas de nuevo a DataFrame para visualizar\n",
    "df_processed = pd.DataFrame(data_processed, columns=['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2', 'Label'])\n",
    "\n",
    "print(\"DataFrame después de Agrupar Categorías con Cython:\")\n",
    "print(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### action_categorical_5 (motivos para eliminarlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_to_check = '6bc0e29c'\n",
    "filtered_rows = train_data[train_data['action_categorical_5'] == value_to_check]\n",
    "\n",
    "# Verificar si el valor aparece en otras columnas categóricas\n",
    "for col in categorical_features:\n",
    "    if col != 'action_categorical_5':  # Evitar verificar la columna original\n",
    "        matching_rows = filtered_rows[filtered_rows[col] == value_to_check]\n",
    "        if not matching_rows.empty:\n",
    "            print(f\"El valor '{value_to_check}' también aparece en la columna '{col}'\")\n",
    "\n",
    "# Ver filas donde el valor aparece solo en action_categorical_5\n",
    "only_in_action_5 = filtered_rows[~filtered_rows[categorical_features].isin([value_to_check]).any(axis=1)]\n",
    "print(f\"Filas donde el valor '{value_to_check}' solo aparece en 'action_categorical_5':\")\n",
    "print(only_in_action_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_to_check = '79ceee49'\n",
    "filtered_rows = train_data[train_data['action_categorical_5'] == value_to_check]\n",
    "\n",
    "# Verificar si el valor aparece en otras columnas categóricas\n",
    "for col in categorical_features:\n",
    "    if col != 'action_categorical_5':  # Evitar verificar la columna original\n",
    "        matching_rows = filtered_rows[filtered_rows[col] == value_to_check]\n",
    "        if not matching_rows.empty:\n",
    "            print(f\"El valor '{value_to_check}' también aparece en la columna '{col}'\")\n",
    "\n",
    "# Ver filas donde el valor aparece solo en action_categorical_5\n",
    "only_in_action_5 = filtered_rows[~filtered_rows[categorical_features].isin([value_to_check]).any(axis=1)]\n",
    "print(f\"Filas donde el valor '{value_to_check}' solo aparece en 'action_categorical_5':\")\n",
    "print(only_in_action_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### auction_categorical_2 (motivos para eliminarlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show unique values of auction_categorical_2\n",
    "print(\"Unique values of 'auction_categorical_2':\")\n",
    "print(train_data['auction_categorical_2'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Levels features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_features = ['action_categorical_0', 'action_categorical_1', 'action_categorical_2', 'action_categorical_3', 'action_categorical_4']\n",
    "\n",
    "print(f\"Valores únicos y su frecuencia:\")\n",
    "for column in train_data.columns:\n",
    "    if column in level_features:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle para contar los NaNs en cada columna\n",
    "for column in level_features:\n",
    "    num_nans = train_data[column].isna().sum()\n",
    "    print(f\"Columna {column} tiene {num_nans} valores NaN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una columna con las combinaciones únicas de las 5 columnas pero tomando solo las dos primeras letras de cada valor\n",
    "truncated_cols = train_data[level_features].applymap(lambda x: x[:3])\n",
    "\n",
    "# Concatenar las columnas truncadas\n",
    "train_data['combination'] = truncated_cols.apply(lambda row: ''.join(row.values), axis=1)\n",
    "\n",
    "# Obtener las combinaciones únicas y sus frecuencias\n",
    "combination_counts = train_data['combination'].value_counts().reset_index()\n",
    "combination_counts.columns = ['Combination', 'Frequency']\n",
    "\n",
    "# Mostrar las primeras 10 combinaciones más comunes\n",
    "print(combination_counts.head(25))\n",
    "print(f\"De: {len(combination_counts)} combinaciones únicas.\")\n",
    "\n",
    "# Graficar las combinaciones más frecuentes\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_combinations = combination_counts.head(25)  # Mostrar las 25 combinaciones más comunes\n",
    "sns.barplot(x='Frequency', y='Combination', data=top_combinations)\n",
    "plt.title(\"Top 25 Combinations of Action Categorical Columns out of {} Unique Combinations\".format(len(combination_counts)))\n",
    "plt.xlabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame de ejemplo\n",
    "df = train_data[level_features].head(5).copy()\n",
    "\n",
    "print(\"DataFrame Original:\")\n",
    "print(df)\n",
    "\n",
    "# Aplicar la función de combinación de niveles\n",
    "df_processed = create_level_combination(df)\n",
    "\n",
    "print(\"\\nDataFrame después de la Combinación de Niveles:\")\n",
    "print(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boolean Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_features = ['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2']\n",
    "\n",
    "for column in train_data.columns:\n",
    "    if column in boolean_features:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle para contar los NaNs en cada columna\n",
    "for column in boolean_features:\n",
    "    num_nans = train_data[column].isna().sum()\n",
    "    print(f\"Columna {column} tiene {num_nans} valores NaN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame de ejemplo\n",
    "data = {\n",
    "    'auction_boolean_0': ['47980dda', '43c867fd', None, '79ceee49'],\n",
    "    'auction_boolean_1': ['79ceee49', None, '79ceee49', None],\n",
    "    'auction_boolean_2': ['65dcab89', None, '43c867fd', '65dcab89'],\n",
    "    'Label': [0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "# Crear el DataFrame\n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "# Imprimir el DataFrame original\n",
    "print(\"DataFrame Original:\")\n",
    "print(data)\n",
    "\n",
    "# Aplicar la función personalizada de one-hot encoding\n",
    "data_encoded = boolean_features_ohe(data)\n",
    "\n",
    "# Imprimir el DataFrame resultante\n",
    "print(\"\\nDataFrame después del One-Hot Encoding personalizado:\")\n",
    "print(data_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_features = ['auction_time', 'auction_age', 'timezone_offset']\n",
    "\n",
    "for column in train_data.columns:\n",
    "    if column in time_features:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle para contar los NaNs en cada columna\n",
    "for column in time_features:\n",
    "    num_nans = train_data[column].isna().sum()\n",
    "    print(f\"Columna {column} tiene {num_nans} valores NaN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegúrate de tener la columna 'auction_age' en tu train\n",
    "if 'auction_age' in train_data.columns:\n",
    "    # Obtener los valores únicos y su frecuencia\n",
    "    unique_ages = train_data['auction_age'].value_counts().sort_index()\n",
    "\n",
    "    # Imprimir cada edad y su frecuencia\n",
    "    for age, frequency in unique_ages.items():\n",
    "        print(f\"Edad: {age}, Frecuencia: {frequency}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_features_extension\n",
    "\n",
    "data = {\n",
    "    'auction_time': [\n",
    "        1545676800,  # 24 de Diciembre de 2018, 22:00:00\n",
    "        1483228800,  # 31 de Diciembre de 2016, 23:00:00\n",
    "        1288396800,  # 30 de Octubre de 2010, 10:00:00\n",
    "        1412899200,  # 10 de Octubre de 2014, 15:00:00\n",
    "        1483574400   # 5 de Enero de 2017, 09:00:00\n",
    "    ],\n",
    "    'timezone_offset': [1, -2, 3, 0, 5]  # Diferentes zonas horarias\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = time_features_extension(df)\n",
    "\n",
    "# Imprimir el DataFrame resultante\n",
    "print(\"\\nDataFrame después de procesar el tiempo de subasta:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age_group\n",
    "\n",
    "data = {\n",
    "    'auction_age': [\n",
    "        -1, 15, 25, 35, 50, 65, 105, 80, 18, 99, 0, 579\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = age_group(df, 'auction_age')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_features = ['auction_list_0', 'action_list_1', 'action_list_2', 'action_list_0']\n",
    "\n",
    "for column in train_data.columns:\n",
    "    if column in list_features:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle para contar los NaNs en cada columna\n",
    "for column in list_features:\n",
    "    num_nans = train_data[column].isna().sum()\n",
    "    print(f\"Columna {column} tiene {num_nans} valores NaN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame de prueba más pequeño\n",
    "data = train_data[list_features].head(5)\n",
    "\n",
    "print('Antes de utilizar la función expand_list_dummies_cython:')\n",
    "print(data)\n",
    "\n",
    "data = expand_list_dummies_cython(data, 'auction_list_0')\n",
    "data = expand_list_dummies_cython(data, 'action_list_1')\n",
    "data = expand_list_dummies_cython(data, 'action_list_2')\n",
    "\n",
    "print('Antes de utilizar la función expand_action_list_0:')\n",
    "print(data)\n",
    "\n",
    "print()\n",
    "\n",
    "print('Después de utilizar la función expand_action_list_0:')\n",
    "data = expand_action_list_0(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pixels features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels_features = ['creative_height', 'creative_width']\n",
    "\n",
    "for column in train_data.columns:\n",
    "    if column in pixels_features:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle para contar los NaNs en cada columna\n",
    "for column in pixels_features:\n",
    "    num_nans = train_data[column].isna().sum()\n",
    "    print(f\"Columna {column} tiene {num_nans} valores NaN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar las filas donde ambas columnas tienen valores NaN\n",
    "nans_both_columns = train_data[pixels_features].isna().all(axis=1).sum()\n",
    "\n",
    "print(f\"El número de filas donde ambas columnas tienen valores NaN es: {nans_both_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volver a filtrar las filas donde ambas columnas son NaN\n",
    "filtered_rows = train_data[train_data['creative_height'].isna() & train_data['creative_width'].isna()]\n",
    "\n",
    "# Calcular el porcentaje de cada valor de 'Label' en las filas donde ambas columnas tienen valores NaN\n",
    "label_counts = filtered_rows['Label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Mostrar los resultados\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar las filas donde tanto creative_height como creative_width son NaN\n",
    "subset_nan = train_data[train_data['creative_height'].isna() & train_data['creative_width'].isna()]\n",
    "\n",
    "# Recorrer todas las columnas y mostrar particularidades en las 1,304,272 filas\n",
    "for column in train_data.columns:\n",
    "    unique_values = subset_nan[column].nunique()\n",
    "    num_nans = subset_nan[column].isna().sum()\n",
    "    \n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos: {unique_values}\")\n",
    "    print(f\"Cantidad de NaNs: {num_nans}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame de prueba más pequeño\n",
    "data = train_data[pixels_features].head(5)\n",
    "\n",
    "print('Antes de utilizar la función hxm_column:')\n",
    "print(data)\n",
    "\n",
    "data = hxw_column(data)\n",
    "\n",
    "print()\n",
    "print('Después de utilizar la función hxm_column:')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creative features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = [\n",
    "    'creative_categorical_11',\n",
    "    'creative_categorical_10',\n",
    "    'creative_categorical_9'\n",
    "]\n",
    "unique_vals = {'65dcab89', '43c867fd'}\n",
    "\n",
    "mask = train_data[columns_to_check].apply(lambda row: set(row).issubset(unique_vals) and len(set(row)) == 1, axis=1)\n",
    "\n",
    "train_data[mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No podemos eliminar las columnas pero si podemos crear dos nuevas que tomen los nombres de los dos valores unicos de estas columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_data[columns_to_check].head(5)\n",
    "\n",
    "print('Antes de utilizar la función creatives2unique:')\n",
    "print(data)\n",
    "\n",
    "data = creatives2unique(data)\n",
    "\n",
    "print()\n",
    "print('Después de utilizar la función creatives2unique:')\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Device features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id = ['device_id', 'device_id_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train_data.columns:\n",
    "    if column in device_id:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = {col: set(train_data[col].dropna().unique()) for col in device_id}\n",
    "\n",
    "common_columns = {}\n",
    "\n",
    "for i in range(len(device_id)):\n",
    "    for j in range(i + 1, len(device_id)):\n",
    "        col1 = device_id[i]\n",
    "        col2 = device_id[j]\n",
    "        \n",
    "        common_values = unique_values[col1].intersection(unique_values[col2])\n",
    "        \n",
    "        if common_values:\n",
    "            if (col1, col2) not in common_columns:\n",
    "                common_columns[(col1, col2)] = common_values\n",
    "\n",
    "for col_pair, common_vals in common_columns.items():\n",
    "    col1, col2 = col_pair\n",
    "    print(f\"\\nValores comunes entre {col1} y {col2}: {common_vals}\")\n",
    "    print(f\"Valores únicos de {col1}: {len(unique_values[col1])}\")\n",
    "    print(f\"Valores únicos de {col2}: {len(unique_values[col2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features restantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Auction categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auction_categorical_variable = [\"auction_categorical_2\", \"auction_categorical_3\", \"auction_categorical_4\", \"auction_categorical_5\", \"auction_categorical_6\", \"auction_categorical_10\", \"auction_categorical_12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train_data.columns:\n",
    "    if column in auction_categorical_variable:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables = [\n",
    "    'action_categorical_6', 'action_categorical_7', 'auction_categorical_3', 'auction_categorical_4', 'auction_categorical_5', 'auction_categorical_6', 'auction_categorical_10', 'auction_categorical_12', 'creative_categorical_1', 'creative_categorical_12', 'creative_categorical_2', 'creative_categorical_3', 'creative_categorical_4', 'creative_categorical_6', 'creative_categorical_7', 'creative_categorical_8'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train_data.columns:\n",
    "    if column in categorical_variables:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = {col: set(train_data[col].dropna().unique()) for col in categorical_variables}\n",
    "\n",
    "common_columns = {}\n",
    "\n",
    "for i in range(len(categorical_variables)):\n",
    "    for j in range(i + 1, len(categorical_variables)):\n",
    "        col1 = categorical_variables[i]\n",
    "        col2 = categorical_variables[j]\n",
    "        \n",
    "        common_values = unique_values[col1].intersection(unique_values[col2])\n",
    "        \n",
    "        if common_values:\n",
    "            if (col1, col2) not in common_columns:\n",
    "                common_columns[(col1, col2)] = common_values\n",
    "\n",
    "for col_pair, common_vals in common_columns.items():\n",
    "    col1, col2 = col_pair\n",
    "    print(f\"\\nValores comunes entre {col1} y {col2}: {common_vals}\")\n",
    "    print(f\"Valores únicos de {col1}: {len(unique_values[col1])}\")\n",
    "    print(f\"Valores únicos de {col2}: {len(unique_values[col2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Entitiy IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_id = ['auction_categorical_0', 'auction_categorical_1', 'auction_categorical_11', 'auction_categorical_7', 'auction_categorical_8', 'auction_categorical_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train_data.columns:\n",
    "    if column in entity_id:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = {col: set(train_data[col].dropna().unique()) for col in entity_id}\n",
    "\n",
    "common_columns = {}\n",
    "\n",
    "for i in range(len(entity_id)):\n",
    "    for j in range(i + 1, len(entity_id)):\n",
    "        col1 = entity_id[i]\n",
    "        col2 = entity_id[j]\n",
    "        \n",
    "        common_values = unique_values[col1].intersection(unique_values[col2])\n",
    "        \n",
    "        if common_values:\n",
    "            if (col1, col2) not in common_columns:\n",
    "                common_columns[(col1, col2)] = common_values\n",
    "\n",
    "for col_pair, common_vals in common_columns.items():\n",
    "    col1, col2 = col_pair\n",
    "    print(f\"\\nValores comunes entre {col1} y {col2}: {common_vals}\")\n",
    "    print(f\"Valores únicos de {col1}: {len(unique_values[col1])}\")\n",
    "    print(f\"Valores únicos de {col2}: {len(unique_values[col2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Business ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_columns = ['creative_categorical_0', 'creative_categorical_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train_data.columns:\n",
    "    if column in business_columns:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = {col: set(train_data[col].dropna().unique()) for col in business_columns}\n",
    "\n",
    "common_columns = {}\n",
    "\n",
    "for i in range(len(business_columns)):\n",
    "    for j in range(i + 1, len(business_columns)):\n",
    "        col1 = business_columns[i]\n",
    "        col2 = business_columns[j]\n",
    "        \n",
    "        common_values = unique_values[col1].intersection(unique_values[col2])\n",
    "        \n",
    "        if common_values:\n",
    "            if (col1, col2) not in common_columns:\n",
    "                common_columns[(col1, col2)] = common_values\n",
    "\n",
    "for col_pair, common_vals in common_columns.items():\n",
    "    col1, col2 = col_pair\n",
    "    print(f\"\\nValores comunes entre {col1} y {col2}: {common_vals}\")\n",
    "    print(f\"Valores únicos de {col1}: {len(unique_values[col1])}\")\n",
    "    print(f\"Valores únicos de {col2}: {len(unique_values[col2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC-AUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data.drop(columns='Label'),  # Características\n",
    "    train_data['Label'],               # Variable objetivo\n",
    "    test_size=0.2,                     # 20% para validación\n",
    "    stratify=train_data['Label'],      # Estratificación basada en la variable objetivo\n",
    "    random_state=random_state          # Semilla para reproducibilidad\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar columnas categóricas y numéricas en train_data\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numeric_features = X_train.select_dtypes(include=['number']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesador común para imputación y codificación\n",
    "preprocessor_ohe = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Imputar valores numéricos con la media\n",
    "        ('num', SimpleImputer(strategy='mean'), numeric_features),\n",
    "        \n",
    "        # Imputar valores categóricos con 'Desconocido' y aplicar One-Hot Encoding\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='Desconocido')),\n",
    "            ('onehot', OneHotEncoder(drop='first', sparse_output=True, handle_unknown='ignore'))\n",
    "        ]), categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Ajustar el preprocesador en el conjunto de entrenamiento y transformar ambos conjuntos\n",
    "X_train_ohe = preprocessor_ohe.fit_transform(X_train)\n",
    "X_val_ohe = preprocessor_ohe.transform(X_val)\n",
    "\n",
    "# Obtener los nombres de las columnas después de la transformación\n",
    "ohe_columns = preprocessor_ohe.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "processed_columns_ohe = numeric_features + list(ohe_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el preprocesador con Target Encoding e imputación\n",
    "encoder_te = ce.TargetEncoder(cols=categorical_features)\n",
    "\n",
    "# Ajustar y transformar el conjunto de entrenamiento y prueba\n",
    "X_train_te = encoder_te.fit_transform(X_train, y_train)\n",
    "X_val_te = encoder_te.transform(X_val)\n",
    "\n",
    "# Escalar las características numéricas\n",
    "scaler_te = StandardScaler()\n",
    "X_train_te[numeric_features] = scaler_te.fit_transform(X_train_te[numeric_features])\n",
    "X_val_te[numeric_features] = scaler_te.transform(X_val_te[numeric_features])\n",
    "\n",
    "del X_train, X_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el espacio de búsqueda para XGBoost con OHE\n",
    "space_xgb_ohe = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 10)), \n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]), \n",
    "    'gamma': hp.uniform('gamma', 0, 0.5),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0.5, 2),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 1),  # L1 regularización\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 1),  # L2 regularización\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1),  # Muestreo a nivel de split\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.5, 1),  # Muestreo por nodo\n",
    "    'grow_policy': hp.choice('grow_policy', ['depthwise', 'lossguide']),  # Estrategia de crecimiento\n",
    "    'tree_method': hp.choice('tree_method', ['auto', 'approx', 'hist'])  # Métodos de construcción del árbol\n",
    "}\n",
    "\n",
    "# Definir el espacio de búsqueda para XGBoost con TE\n",
    "space_xgb_te = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 10)), \n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]), \n",
    "    'gamma': hp.uniform('gamma', 0, 0.5),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0.5, 2),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 1),  # L1 regularización\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 1),  # L2 regularización\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1),  # Muestreo a nivel de split\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.5, 1),  # Muestreo por nodo\n",
    "    'grow_policy': hp.choice('grow_policy', ['depthwise', 'lossguide']),  # Estrategia de crecimiento\n",
    "    'tree_method': hp.choice('tree_method', ['auto', 'approx', 'hist'])  # Métodos de construcción del árbol\n",
    "}\n",
    "\n",
    "# Definir el espacio de búsqueda para Random Forest con TE\n",
    "space_rf_te = {\n",
    "    'n_estimators': hp.choice('n_estimators_rf_te', range(100, 1000)),\n",
    "    'max_depth': hp.choice('max_depth_rf_te', range(3, 20)),\n",
    "    'min_samples_split': hp.uniform('min_samples_split_rf_te', 0.1, 1.0),\n",
    "    'min_samples_leaf': hp.uniform('min_samples_leaf_rf_te', 0.1, 0.5),\n",
    "    'bootstrap': hp.choice('bootstrap_rf_te', [True, False]),\n",
    "    'max_features': hp.choice('max_features_rf_te', ['sqrt', 'log2'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función objetivo para XGBoost con OHE\n",
    "def objective_xgb_ohe(params):\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['min_child_weight'] = int(params['min_child_weight'])\n",
    "    \n",
    "    model = XGBClassifier(\n",
    "        max_depth=params['max_depth'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        n_estimators=params['n_estimators'],\n",
    "        gamma=params['gamma'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        scale_pos_weight=params['scale_pos_weight'],\n",
    "        reg_alpha=params['reg_alpha'],  # L1 regularización\n",
    "        reg_lambda=params['reg_lambda'],  # L2 regularización\n",
    "        colsample_bylevel=params['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "        colsample_bynode=params['colsample_bynode'],  # Muestreo por nodo\n",
    "        grow_policy=params['grow_policy'],  # Estrategia de crecimiento\n",
    "        tree_method=params['tree_method'],  # Método de construcción del árbolobjective='binary:logistic',\n",
    "        random_state=random_state,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='auc',\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train_ohe, \n",
    "        y_train, \n",
    "        eval_set=[(X_val_ohe, y_val)],\n",
    "        early_stopping_rounds=10,\n",
    "        verbose=False\n",
    "    )\n",
    "    preds = model.predict_proba(X_val_ohe)[:,1]\n",
    "    auc = roc_auc_score(y_val, preds)\n",
    "    return {'loss': -auc, 'status': STATUS_OK}\n",
    "\n",
    "# Definir la función objetivo para XGBoost con TE\n",
    "def objective_xgb_te(params):\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['min_child_weight'] = int(params['min_child_weight'])\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        max_depth=params['max_depth'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        n_estimators=params['n_estimators'],\n",
    "        gamma=params['gamma'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        scale_pos_weight=params['scale_pos_weight'],\n",
    "        reg_alpha=params['reg_alpha'],  # L1 regularización\n",
    "        reg_lambda=params['reg_lambda'],  # L2 regularización\n",
    "        colsample_bylevel=params['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "        colsample_bynode=params['colsample_bynode'],  # Muestreo por nodo\n",
    "        grow_policy=params['grow_policy'],  # Estrategia de crecimiento\n",
    "        tree_method=params['tree_method'],  # Método de construcción del árbolobjective='binary:logistic',\n",
    "        random_state=random_state,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='auc',\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train_te, \n",
    "        y_train, \n",
    "        eval_set=[(X_val_te, y_val)],\n",
    "        early_stopping_rounds=10,\n",
    "        verbose=False\n",
    "    )\n",
    "    preds = model.predict_proba(X_val_te)[:,1]\n",
    "    auc = roc_auc_score(y_val, preds)\n",
    "    return {'loss': -auc, 'status': STATUS_OK}\n",
    "\n",
    "# Definir la función objetivo para Random Forest con TE\n",
    "def objective_rf_te(params):\n",
    "    model = RandomForestClassifier(\n",
    "        random_state=random_state,\n",
    "        **params\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train_te, \n",
    "        y_train\n",
    "    )\n",
    "    preds = model.predict_proba(X_val_te)[:,1]\n",
    "    auc = roc_auc_score(y_val, preds)\n",
    "    return {'loss': -auc, 'status': STATUS_OK}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar Trials para cada modelo y encoding\n",
    "trials_xgb_ohe = Trials()\n",
    "trials_rf_ohe = Trials()\n",
    "trials_xgb_te = Trials()\n",
    "trials_rf_te = Trials()\n",
    "\n",
    "# Optimización Hyperopt para XGBoost con OHE\n",
    "best_xgb_ohe = fmin(\n",
    "    fn=objective_xgb_ohe,\n",
    "    space=space_xgb_ohe,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=5,\n",
    "    trials=trials_xgb_ohe,\n",
    "    rstate=np.random.default_rng(random_state)\n",
    ")\n",
    "\n",
    "# Optimización Hyperopt para XGBoost con TE\n",
    "best_xgb_te = fmin(\n",
    "    fn=objective_xgb_te,\n",
    "    space=space_xgb_te,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=5,\n",
    "    trials=trials_xgb_te,\n",
    "    rstate=np.random.default_rng(random_state)\n",
    ")\n",
    "\n",
    "# Optimización Hyperopt para Random Forest con TE\n",
    "best_rf_te = fmin(\n",
    "    fn=objective_rf_te,\n",
    "    space=space_rf_te,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=5,\n",
    "    trials=trials_rf_te,\n",
    "    rstate=np.random.default_rng(random_state)\n",
    ")\n",
    "\n",
    "print(\"Mejores hiperparámetros XGBoost OHE:\", best_xgb_ohe)\n",
    "print(\"Mejores hiperparámetros XGBoost TE:\", best_xgb_te)\n",
    "print(\"Mejores hiperparámetros Random Forest TE:\", best_rf_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar los hiperparámetros obtenidos para XGBoost con OHE\n",
    "tree_method_options = ['auto', 'exact', 'approx', 'hist']\n",
    "grow_policy_options = ['depthwise', 'lossguide']\n",
    "n_estimators_options = [100, 200, 300, 400, 500]\n",
    "\n",
    "# Entrenar el modelo final XGBoost con OHE\n",
    "final_model_xgb_ohe = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    random_state=random_state,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='auc',\n",
    "    n_estimators=n_estimators_options[best_xgb_ohe['n_estimators']],\n",
    "    max_depth=best_xgb_ohe['max_depth'],\n",
    "    learning_rate=best_xgb_ohe['learning_rate'],\n",
    "    subsample=best_xgb_ohe['subsample'],\n",
    "    colsample_bytree=best_xgb_ohe['colsample_bytree'],\n",
    "    min_child_weight=best_xgb_ohe['min_child_weight'],\n",
    "    gamma=best_xgb_ohe['gamma'],\n",
    "    scale_pos_weight=best_xgb_ohe['scale_pos_weight'],\n",
    "    reg_alpha=best_xgb_ohe['reg_alpha'],  # L1 regularización\n",
    "    reg_lambda=best_xgb_ohe['reg_lambda'],  # L2 regularización\n",
    "    colsample_bylevel=best_xgb_ohe['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "    colsample_bynode=best_xgb_ohe['colsample_bynode'],  # Muestreo por nodo\n",
    "    grow_policy=grow_policy_options[best_xgb_ohe['grow_policy']],  # Estrategia de crecimiento\n",
    "    tree_method=tree_method_options[best_xgb_ohe['tree_method']],  # Método de construcción del árbol\n",
    ")\n",
    "\n",
    "final_model_xgb_ohe.fit(\n",
    "    X_train_ohe, \n",
    "    y_train, \n",
    "    eval_set=[(X_val_ohe, y_val)],\n",
    "    early_stopping_rounds=10,\n",
    "    verbose=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar los hiperparámetros obtenidos para XGBoost con TE\n",
    "tree_method_options = ['auto', 'exact', 'approx', 'hist']\n",
    "grow_policy_options = ['depthwise', 'lossguide']\n",
    "n_estimators_options = [100, 200, 300, 400, 500]\n",
    "\n",
    "# Entrenar el modelo final XGBoost con TE\n",
    "final_model_xgb_te = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    random_state=random_state,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='auc',\n",
    "    n_estimators=n_estimators_options[best_xgb_ohe['n_estimators']],\n",
    "    max_depth=best_xgb_ohe['max_depth'],\n",
    "    learning_rate=best_xgb_ohe['learning_rate'],\n",
    "    subsample=best_xgb_ohe['subsample'],\n",
    "    colsample_bytree=best_xgb_ohe['colsample_bytree'],\n",
    "    min_child_weight=best_xgb_ohe['min_child_weight'],\n",
    "    gamma=best_xgb_ohe['gamma'],\n",
    "    scale_pos_weight=best_xgb_ohe['scale_pos_weight'],\n",
    "    reg_alpha=best_xgb_ohe['reg_alpha'],  # L1 regularización\n",
    "    reg_lambda=best_xgb_ohe['reg_lambda'],  # L2 regularización\n",
    "    colsample_bylevel=best_xgb_ohe['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "    colsample_bynode=best_xgb_ohe['colsample_bynode'],  # Muestreo por nodo\n",
    "    grow_policy=grow_policy_options[best_xgb_ohe['grow_policy']],  # Estrategia de crecimiento\n",
    "    tree_method=tree_method_options[best_xgb_ohe['tree_method']],  # Método de construcción del árbol\n",
    ")\n",
    "\n",
    "final_model_xgb_te.fit(\n",
    "    X_train_te, \n",
    "    y_train, \n",
    "    eval_set=[(X_val_te, y_val)],\n",
    "    early_stopping_rounds=10,\n",
    "    verbose=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar los hiperparámetros obtenidos para Random Forest con TE\n",
    "best_params_rf_te = {\n",
    "    'n_estimators': best_rf_te['n_estimators_rf_te'] + 100,\n",
    "    'max_depth': best_rf_te['max_depth_rf_te'] + 3,\n",
    "    'min_samples_split': best_rf_te['min_samples_split_rf_te'],\n",
    "    'min_samples_leaf': best_rf_te['min_samples_leaf_rf_te'],\n",
    "    'bootstrap': [True, False][best_rf_te['bootstrap_rf_te']],\n",
    "    'max_features': ['sqrt', 'log2'][best_rf_te['max_features_rf_te']]\n",
    "}\n",
    "\n",
    "# Entrenar el modelo final Random Forest con TE\n",
    "final_model_rf_te = RandomForestClassifier(\n",
    "    random_state=random_state,\n",
    "    **best_params_rf_te\n",
    ")\n",
    "\n",
    "final_model_rf_te.fit(\n",
    "    X_train_te, \n",
    "    y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular las predicciones y ROC AUC para XGBoost con OHE\n",
    "preds_xgb_ohe = final_model_xgb_ohe.predict_proba(X_val_ohe)[:,1]\n",
    "fpr_xgb_ohe, tpr_xgb_ohe, _ = roc_curve(y_val, preds_xgb_ohe)\n",
    "auc_xgb_ohe = roc_auc_score(y_val, preds_xgb_ohe)\n",
    "\n",
    "# Calcular las predicciones y ROC AUC para XGBoost con TE\n",
    "preds_xgb_te = final_model_xgb_te.predict_proba(X_val_te)[:,1]\n",
    "fpr_xgb_te, tpr_xgb_te, _ = roc_curve(y_val, preds_xgb_te)\n",
    "auc_xgb_te = roc_auc_score(y_val, preds_xgb_te)\n",
    "\n",
    "# Calcular las predicciones y ROC AUC para Random Forest con TE\n",
    "preds_rf_te = final_model_rf_te.predict_proba(X_val_te)[:,1]\n",
    "fpr_rf_te, tpr_rf_te, _ = roc_curve(y_val, preds_rf_te)\n",
    "auc_rf_te = roc_auc_score(y_val, preds_rf_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcional: Guardar el gráfico\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# XGBoost con One-Hot Encoding\n",
    "plt.plot(fpr_xgb_ohe, tpr_xgb_ohe, label=f'XGBoost OHE (AUC = {auc_xgb_ohe:.2f})')\n",
    "\n",
    "# XGBoost con Target Encoding\n",
    "plt.plot(fpr_xgb_te, tpr_xgb_te, label=f'XGBoost TE (AUC = {auc_xgb_te:.2f})')\n",
    "\n",
    "# Random Forest con Target Encoding\n",
    "plt.plot(fpr_rf_te, tpr_rf_te, label=f'Random Forest TE (AUC = {auc_rf_te:.2f})')\n",
    "\n",
    "# Línea diagonal\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "# Configuración del gráfico\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Curvas ROC AUC de Modelos')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.savefig('roc_auc_curves.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del mejor modelo: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_ohe, X_val_ohe, X_train_te, X_val_te, final_model_xgb_ohe, final_model_xgb_te, final_model_rf_te, y_train, y_val, train_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_data_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_combined, _ = train_test_split(\n",
    "    train_data, \n",
    "    train_size=1200000, \n",
    "    random_state=random_state, \n",
    "    stratify=train_data['Label']\n",
    ")\n",
    "\n",
    "# Imprimir la cantidad de filas del dataset combinado\n",
    "print(f\"Cantidad de filas en el dataset combinado: {train_data_combined.shape[0]}\")\n",
    "print(f\"Cantidad de columnas en el dataset combinado: {train_data_combined.shape[1]}\")\n",
    "\n",
    "# Ver porcentaje de clics vs no clics en la columna Label\n",
    "label_counts = train_data_combined['Label'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPorcentaje de clics (1) y no clics (0):\")\n",
    "print(label_counts)\n",
    "\n",
    "# Cantidad de clics (1) y no clics (0)\n",
    "label_counts_abs = train_data_combined['Label'].value_counts()\n",
    "print(\"\\nCantidad de clics (1) y no clics (0):\")\n",
    "print(label_counts_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y validación con estratificación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data_combined.drop(columns='Label'),\n",
    "    train_data_combined['Label'],             \n",
    "    test_size=0.01,                           \n",
    "    stratify=train_data_combined['Label'],    \n",
    "    random_state=random_state                 \n",
    ")\n",
    "\n",
    "# Verificar la proporción de clases en el conjunto de entrenamiento y validación\n",
    "print(\"Proporción en el conjunto de entrenamiento:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"Proporción en el conjunto de validación:\")\n",
    "print(y_val.value_counts(normalize=True))\n",
    "\n",
    "del train_data_combined, train_data, y_val, X_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = process_optimized(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/ctr_test.csv')\n",
    "\n",
    "X_test = process_optimized(test_data)\n",
    "\n",
    "# Excluir 'id' de las columnas de X_test para el reordenamiento\n",
    "common_columns = [col for col in X_test.columns if col != 'id']\n",
    "\n",
    "# Asegúrate de que las columnas en X_train coincidan con las de X_test (sin 'id')\n",
    "missing_cols = set(common_columns) - set(X_train.columns)\n",
    "for col in missing_cols:\n",
    "    X_train[col] = 0\n",
    "\n",
    "# Eliminar columnas extra en X_train que no están en X_test (sin contar 'id')\n",
    "extra_cols = set(X_train.columns) - set(common_columns)\n",
    "X_train.drop(columns=extra_cols, inplace=True)\n",
    "\n",
    "# Reordenar X_train para que tenga el mismo orden de columnas que X_test (sin 'id')\n",
    "X_train = X_train[common_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las columnas categóricas y numéricas\n",
    "categorical_features_to_encode = X_train.select_dtypes(include=['object']).columns\n",
    "numeric_features = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Preprocesador común para imputación y codificación\n",
    "common_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Imputar valores numéricos con la media\n",
    "        ('num', SimpleImputer(strategy='mean'), numeric_features),\n",
    "        \n",
    "        # Imputar valores categóricos con 'Desconocido' y aplicar One-Hot Encoding\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='Desconocido')),\n",
    "            ('onehot', OneHotEncoder(drop='first', sparse_output=True, handle_unknown='ignore'))\n",
    "        ]), categorical_features_to_encode)\n",
    "    ],\n",
    "    remainder='drop'  # Excluir columnas no especificadas\n",
    ")\n",
    "\n",
    "# Definir el modelo XGBoost\n",
    "model_xgb = XGBClassifier(\n",
    "    random_state=random_state, \n",
    "    use_label_encoder=False, \n",
    "    eval_metric='auc'\n",
    ")\n",
    "\n",
    "# Pipeline para XGBoost (mantiene matrices dispersas)\n",
    "pipeline_xgb = Pipeline(steps=[\n",
    "    ('preprocessor', common_preprocessor),\n",
    "    ('classifier', model_xgb),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el espacio de búsqueda para los hiperparámetros de XGBoost\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 10)), \n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]), \n",
    "    'gamma': hp.uniform('gamma', 0, 0.5),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0.5, 2),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 1),  # L1 regularización\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 1),  # L2 regularización\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1),  # Muestreo a nivel de split\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.5, 1),  # Muestreo por nodo\n",
    "    'grow_policy': hp.choice('grow_policy', ['depthwise', 'lossguide']),  # Estrategia de crecimiento\n",
    "    'tree_method': hp.choice('tree_method', ['auto', 'approx', 'hist'])  # Métodos de construcción del árbol\n",
    "}\n",
    "\n",
    "# Función objetivo para Hyperopt utilizando un conjunto de validación fijo\n",
    "def objective_xgb(params):\n",
    "    \n",
    "    # Asegurar que los parámetros sean del tipo correcto\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['min_child_weight'] = int(params['min_child_weight'])\n",
    "    \n",
    "    # Definir el modelo con los hiperparámetros actuales\n",
    "    model_xgb = XGBClassifier(\n",
    "        max_depth=params['max_depth'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        n_estimators=params['n_estimators'],\n",
    "        gamma=params['gamma'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        scale_pos_weight=params['scale_pos_weight'],\n",
    "        reg_alpha=params['reg_alpha'],  # L1 regularización\n",
    "        reg_lambda=params['reg_lambda'],  # L2 regularización\n",
    "        colsample_bylevel=params['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "        colsample_bynode=params['colsample_bynode'],  # Muestreo por nodo\n",
    "        grow_policy=params['grow_policy'],  # Estrategia de crecimiento\n",
    "        tree_method=params['tree_method'],  # Método de construcción del árbol\n",
    "        use_label_encoder=False,  # Evitar advertencias en versiones más recientes de XGBoost\n",
    "        eval_metric='auc',\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Pipeline para XGBoost (mantiene matrices dispersas)\n",
    "    pipeline_xgb = Pipeline(steps=[\n",
    "        ('preprocessor', common_preprocessor),\n",
    "        ('classifier', model_xgb),\n",
    "    ])\n",
    "    \n",
    "    # Definir la validación cruzada\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Calcular el AUC utilizando validación cruzada\n",
    "    auc_scores = cross_val_score(\n",
    "        pipeline_xgb, \n",
    "        X_train, \n",
    "        y_train, \n",
    "        cv=cv, \n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1  # Utilizar todos los núcleos disponibles\n",
    "    )\n",
    "    mean_auc = auc_scores.mean()\n",
    "    \n",
    "    # Opcional: imprimir los hiperparámetros y el AUC actual\n",
    "    print(f\"Hiperparámetros: {params}, AUC CV: {mean_auc:.4f}\")\n",
    "    \n",
    "    # Retornar el valor a minimizar (1 - AUC)\n",
    "    return {'loss': 1 - mean_auc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la optimización\n",
    "trials = Trials()\n",
    "\n",
    "best_xgb = fmin(\n",
    "    fn=objective_xgb,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=1,\n",
    "    trials=trials,\n",
    "    rstate=np.random.default_rng(random_state)  # Asegurar reproducibilidad\n",
    ")\n",
    "\n",
    "# No es necesario volver a mapear los hiperparámetros aquí, ya se hizo dentro de la función objetivo\n",
    "print(\"Mejores hiperparámetros para XGBoost:\")\n",
    "print(best_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices\n",
    "tree_method_options = ['auto', 'exact', 'approx', 'hist']\n",
    "grow_policy_options = ['depthwise', 'lossguide']\n",
    "n_estimators_options = [100, 200, 300, 400, 500]\n",
    "\n",
    "# Reconstruir el modelo de XGBoost con los mejores hiperparámetros\n",
    "best_model_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=n_estimators_options[best_xgb['n_estimators']],\n",
    "    max_depth=best_xgb['max_depth'],\n",
    "    learning_rate=best_xgb['learning_rate'],\n",
    "    subsample=best_xgb['subsample'],\n",
    "    colsample_bytree=best_xgb['colsample_bytree'],\n",
    "    min_child_weight=best_xgb['min_child_weight'],\n",
    "    gamma=best_xgb['gamma'],\n",
    "    scale_pos_weight=best_xgb['scale_pos_weight'],\n",
    "    reg_alpha=best_xgb['reg_alpha'],  # L1 regularización\n",
    "    reg_lambda=best_xgb['reg_lambda'],  # L2 regularización\n",
    "    colsample_bylevel=best_xgb['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "    colsample_bynode=best_xgb['colsample_bynode'],  # Muestreo por nodo\n",
    "    grow_policy=grow_policy_options[best_xgb['grow_policy']],  # Estrategia de crecimiento\n",
    "    tree_method=tree_method_options[best_xgb['tree_method']],  # Método de construcción del árbol\n",
    "    random_state=random_state,\n",
    "    use_label_encoder=False,  # Evitar advertencias en versiones más recientes de XGBoost\n",
    "    eval_metric=\"auc\"  # Establecer AUC como métrica de evaluación\n",
    ")\n",
    "\n",
    "# Crear un nuevo pipeline reutilizando el preprocesador original y el mejor modelo\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', common_preprocessor),\n",
    "    ('classifier', model_xgb),\n",
    "])\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "xgb_pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer el clasificador (modelo XGBoost) del pipeline\n",
    "model = xgb_pipeline.named_steps['classifier']\n",
    "\n",
    "# Obtener las importancias de las características\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Obtener el preprocesador del pipeline\n",
    "preprocessor = xgb_pipeline.named_steps['preprocessor']\n",
    "\n",
    "# Obtener los nombres de las características transformadas\n",
    "try:\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "except AttributeError:\n",
    "    categorical_features = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features_to_encode)\n",
    "    numeric_features = numeric_features  # Ya definido anteriormente\n",
    "\n",
    "    # Combinar los nombres de las características\n",
    "    feature_names = np.concatenate([numeric_features, categorical_features])\n",
    "\n",
    "# Crear un DataFrame con las importancias de las características\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Ordenar las características por importancia descendente\n",
    "feature_importances = feature_importances.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Mostrar las 20 principales características\n",
    "print(feature_importances.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar el tamaño del gráfico\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Graficar las importancias de las características\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances.head(20))\n",
    "\n",
    "# Añadir título y etiquetas\n",
    "plt.title('Importancias de las Características - XGBoost')\n",
    "plt.xlabel('Importancia')\n",
    "plt.ylabel('Características')\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicciones sobre el test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir en el conjunto de testeo\n",
    "y_preds_xgb_test = xgb_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_xgb = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_xgb_test})\n",
    "submission_df_xgb[\"id\"] = submission_df_xgb[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros\n",
    "file_name_xgb = (\n",
    "    f\"xgboost_preds_ne_{best_xgb['n_estimators']}_\"\n",
    "    f\"md_{handle_none(best_xgb['max_depth'])}_\"\n",
    "    f\"lr_{round(best_xgb['learning_rate'], 2)}_\"\n",
    "    f\"ss_{round(best_xgb['subsample'], 2)}_\"\n",
    "    f\"csb_{round(best_xgb['colsample_bytree'], 2)}_\"\n",
    "    f\"cb_level_{round(best_xgb['colsample_bylevel'], 2)}_\"\n",
    "    f\"cb_node_{round(best_xgb['colsample_bynode'], 2)}_\"\n",
    "    f\"mcw_{round(best_xgb['min_child_weight'], 2)}_\"\n",
    "    f\"gamma_{round(best_xgb['gamma'], 2)}_\"\n",
    "    f\"ra_{round(best_xgb['reg_alpha'], 2)}_\"\n",
    "    f\"rl_{round(best_xgb['reg_lambda'], 2)}_\"\n",
    "    f\"spw_{round(best_xgb['scale_pos_weight'], 2)}_\"\n",
    "    f\"gp_{grow_policy_options[best_xgb['grow_policy']]}_\"\n",
    "    f\"tm_{tree_method_options[best_xgb['tree_method']]}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_xgb.to_csv(os.path.join(\"submits\", file_name_xgb), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_xgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hipotesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_data_combined.csv')\n",
    "\n",
    "train_data, _ = train_test_split(\n",
    "    train_data, \n",
    "    train_size=0.20, \n",
    "    random_state=random_state, \n",
    "    stratify=train_data['Label']\n",
    ")\n",
    "\n",
    "test_data = pd.read_csv('data/ctr_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar columnas categóricas y numéricas en train_data\n",
    "categorical_features = train_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numeric_features = train_data.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Excluir columnas que no se usarán para preprocesamiento\n",
    "# Asumiendo que 'Label' está en train_data y 'id' en test_data\n",
    "if 'Label' in numeric_features:\n",
    "    numeric_features.remove('Label')\n",
    "if 'id' in test_data.columns:\n",
    "    test_data = test_data.drop('id', axis=1)\n",
    "\n",
    "print(\"Características categóricas:\", categorical_features)\n",
    "print(\"Características numéricas:\", numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el preprocesador\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Imputar valores numéricos con la media y escalar\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean'))\n",
    "        ]), numeric_features),\n",
    "        \n",
    "        # Imputar valores categóricos con 'Desconocido' y aplicar One-Hot Encoding\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='Desconocido')),\n",
    "            ('onehot', OneHotEncoder(drop='first', sparse=False, handle_unknown='ignore'))\n",
    "        ]), categorical_features)\n",
    "    ],\n",
    "    remainder='drop'  # Excluir columnas no especificadas\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar las etiquetas del conjunto de entrenamiento\n",
    "y_train = train_data['Label']\n",
    "X_train = train_data.drop(['Label'], axis=1)\n",
    "\n",
    "del train_data\n",
    "gc.collect()\n",
    "\n",
    "# Ajustar el preprocesador en el conjunto de entrenamiento y transformar ambos conjuntos\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "del X_train\n",
    "gc.collect()\n",
    "\n",
    "X_test_processed = preprocessor.transform(test_data)\n",
    "\n",
    "del test_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a DataFrame para facilitar la manipulación\n",
    "ohe_columns = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "processed_columns = numeric_features + list(ohe_columns)\n",
    "\n",
    "X_train_processed = pd.DataFrame(X_train_processed, columns=processed_columns)\n",
    "X_test_processed = pd.DataFrame(X_test_processed, columns=processed_columns)\n",
    "\n",
    "print(\"X_train_processed Shape:\", X_train_processed.shape)\n",
    "print(\"X_test_processed Shape:\", X_test_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el número de vecinos\n",
    "k = 5\n",
    "\n",
    "# Inicializar y ajustar el modelo NearestNeighbors en el conjunto de entrenamiento procesado\n",
    "nbrs = NearestNeighbors(n_neighbors=k, metric='cosine')\n",
    "nbrs.fit(X_train_processed)\n",
    "\n",
    "# Encontrar los índices de los vecinos más cercanos para cada fila de prueba\n",
    "distances, indices = nbrs.kneighbors(X_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_data_combined.csv')\n",
    "\n",
    "# Obtener los índices únicos de las filas de entrenamiento que son similares a alguna de las filas de prueba\n",
    "similar_train_indices = np.unique(indices.flatten())\n",
    "\n",
    "# Seleccionar las filas similares del conjunto de entrenamiento original\n",
    "similar_train_data = train_data.iloc[similar_train_indices].copy()\n",
    "\n",
    "# Mostrar la forma del conjunto de datos similar\n",
    "print(f\"Filas similares encontradas: {similar_train_data.shape[0]}\")\n",
    "\n",
    "# Mostrar las primeras filas seleccionadas\n",
    "print(similar_train_data.head())\n",
    "\n",
    "del train_data, X_train_processed, X_test_processed\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(similar_train_data.shape[1])\n",
    "print(similar_train_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de hipotesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_data_combined.csv')\n",
    "\n",
    "# Tomar las filas similares de train_data\n",
    "X_val = similar_train_data.copy()\n",
    "y_val = X_val['Label']\n",
    "\n",
    "# Tomar las filas no similares de train_data\n",
    "X_train = train_data.drop(index=similar_train_indices).copy()\n",
    "y_train = X_train['Label']\n",
    "\n",
    "# Verificar la proporción de clases en el conjunto de entrenamiento y validación\n",
    "print(\"Proporción en el conjunto de entrenamiento:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"Proporción en el conjunto de validación:\")\n",
    "print(y_val.value_counts(normalize=True))\n",
    "\n",
    "del similar_train_data, train_data, similar_train_indices\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = process_optimized(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las columnas categóricas y numéricas\n",
    "categorical_features_to_encode = X_train.select_dtypes(include=['object']).columns\n",
    "numeric_features = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Preprocesador común para imputación y codificación\n",
    "common_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Imputar valores numéricos con la media\n",
    "        ('num', SimpleImputer(strategy='mean'), numeric_features),\n",
    "        \n",
    "        # Imputar valores categóricos con 'Desconocido' y aplicar One-Hot Encoding\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='Desconocido')),\n",
    "            ('onehot', OneHotEncoder(drop='first', sparse_output=True, handle_unknown='ignore'))\n",
    "        ]), categorical_features_to_encode)\n",
    "    ],\n",
    "    remainder='drop'  # Excluir columnas no especificadas\n",
    ")\n",
    "\n",
    "# Definir el modelo XGBoost\n",
    "model_xgb = XGBClassifier(\n",
    "    random_state=random_state, \n",
    "    use_label_encoder=False, \n",
    "    eval_metric='auc'\n",
    ")\n",
    "\n",
    "# Pipeline para XGBoost (mantiene matrices dispersas)\n",
    "pipeline_xgb = Pipeline(steps=[\n",
    "    ('preprocessor', common_preprocessor),\n",
    "    ('classifier', model_xgb),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el espacio de búsqueda para los hiperparámetros de XGBoost\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 10)), \n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]), \n",
    "    'gamma': hp.uniform('gamma', 0, 0.5),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0.5, 2),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 1),  # L1 regularización\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 1),  # L2 regularización\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1),  # Muestreo a nivel de split\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.5, 1),  # Muestreo por nodo\n",
    "    'grow_policy': hp.choice('grow_policy', ['depthwise', 'lossguide']),  # Estrategia de crecimiento\n",
    "    'tree_method': hp.choice('tree_method', ['auto', 'approx', 'hist'])  # Métodos de construcción del árbol\n",
    "}\n",
    "\n",
    "# Función objetivo para Hyperopt utilizando el conjunto de validación\n",
    "def objective_xgb(params):\n",
    "    \n",
    "    # Asegurar que los parámetros sean del tipo correcto\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['min_child_weight'] = int(params['min_child_weight'])\n",
    "    \n",
    "    # Definir el modelo con los hiperparámetros actuales\n",
    "    model = XGBClassifier(\n",
    "        max_depth=params['max_depth'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        n_estimators=params['n_estimators'],\n",
    "        gamma=params['gamma'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        scale_pos_weight=params['scale_pos_weight'],\n",
    "        reg_alpha=params['reg_alpha'],  # L1 regularización\n",
    "        reg_lambda=params['reg_lambda'],  # L2 regularización\n",
    "        colsample_bylevel=params['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "        colsample_bynode=params['colsample_bynode'],  # Muestreo por nodo\n",
    "        grow_policy=params['grow_policy'],  # Estrategia de crecimiento\n",
    "        tree_method=params['tree_method'],  # Método de construcción del árbol\n",
    "        use_label_encoder=False,  # Evitar advertencias en versiones más recientes de XGBoost\n",
    "        eval_metric='auc',\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Pipeline para XGBoost\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', common_preprocessor),\n",
    "        ('classifier', model),\n",
    "    ])\n",
    "    \n",
    "    # Entrenar el pipeline en el conjunto de entrenamiento\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predecir probabilidades en el conjunto de validación\n",
    "    y_pred_proba = pipeline.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Calcular el AUC en el conjunto de validación\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    # Opcional: imprimir los hiperparámetros y el AUC actual\n",
    "    print(f\"Hiperparámetros: {params}, AUC: {auc:.4f}\")\n",
    "    \n",
    "    # Retornar el valor a minimizar (1 - AUC)\n",
    "    return {'loss': 1 - auc, 'status': STATUS_OK}\n",
    "\n",
    "# Ejecutar la optimización\n",
    "trials = Trials()\n",
    "\n",
    "best_xgb = fmin(\n",
    "    fn=objective_xgb,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=10,\n",
    "    trials=trials,\n",
    "    rstate=np.random.default_rng(random_state)\n",
    ")\n",
    "\n",
    "# No es necesario volver a mapear los hiperparámetros aquí, ya se hizo dentro de la función objetivo\n",
    "print(\"Mejores hiperparámetros para XGBoost:\")\n",
    "print(best_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PRUEBA CON TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/ctr_test.csv')\n",
    "\n",
    "X_test = process_optimized(test_data)\n",
    "\n",
    "del test_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluir 'id' de las columnas de X_test para el reordenamiento\n",
    "common_columns = [col for col in X_test.columns if col != 'id']\n",
    "\n",
    "# Asegúrate de que las columnas en X_train coincidan con las de X_test (sin 'id')\n",
    "missing_cols = set(common_columns) - set(X_train.columns)\n",
    "for col in missing_cols:\n",
    "    X_train[col] = 0\n",
    "\n",
    "# Eliminar columnas extra en X_train que no están en X_test (sin contar 'id')\n",
    "extra_cols = set(X_train.columns) - set(common_columns)\n",
    "X_train.drop(columns=extra_cols, inplace=True)\n",
    "\n",
    "# Reordenar X_train para que tenga el mismo orden de columnas que X_test (sin 'id')\n",
    "X_train = X_train[common_columns]\n",
    "\n",
    "# Verificar el número de columnas en X_train\n",
    "print(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices\n",
    "tree_method_options = ['auto', 'exact', 'approx', 'hist']\n",
    "grow_policy_options = ['depthwise', 'lossguide']\n",
    "n_estimators_options = [100, 200, 300, 400, 500]\n",
    "\n",
    "# Reconstruir el modelo de XGBoost con los mejores hiperparámetros\n",
    "best_model_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=n_estimators_options[best_xgb['n_estimators']],\n",
    "    max_depth=best_xgb['max_depth'],\n",
    "    learning_rate=best_xgb['learning_rate'],\n",
    "    subsample=best_xgb['subsample'],\n",
    "    colsample_bytree=best_xgb['colsample_bytree'],\n",
    "    min_child_weight=best_xgb['min_child_weight'],\n",
    "    gamma=best_xgb['gamma'],\n",
    "    scale_pos_weight=best_xgb['scale_pos_weight'],\n",
    "    reg_alpha=best_xgb['reg_alpha'],  # L1 regularización\n",
    "    reg_lambda=best_xgb['reg_lambda'],  # L2 regularización\n",
    "    colsample_bylevel=best_xgb['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "    colsample_bynode=best_xgb['colsample_bynode'],  # Muestreo por nodo\n",
    "    grow_policy=grow_policy_options[best_xgb['grow_policy']],  # Estrategia de crecimiento\n",
    "    tree_method=tree_method_options[best_xgb['tree_method']],  # Método de construcción del árbol\n",
    "    random_state=random_state,\n",
    "    use_label_encoder=False,  # Evitar advertencias en versiones más recientes de XGBoost\n",
    "    eval_metric=\"auc\"  # Establecer AUC como métrica de evaluación\n",
    ")\n",
    "\n",
    "# Crear un nuevo pipeline reutilizando el preprocesador original y el mejor modelo\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', common_preprocessor),\n",
    "    ('classifier', model_xgb),\n",
    "])\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "xgb_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir en el conjunto de testeo\n",
    "y_preds_xgb_test = xgb_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_xgb = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_xgb_test})\n",
    "submission_df_xgb[\"id\"] = submission_df_xgb[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros\n",
    "file_name_xgb = (\n",
    "    f\"xgboost_preds_ne_{best_xgb['n_estimators']}_\"\n",
    "    f\"md_{handle_none(best_xgb['max_depth'])}_\"\n",
    "    f\"lr_{round(best_xgb['learning_rate'], 2)}_\"\n",
    "    f\"ss_{round(best_xgb['subsample'], 2)}_\"\n",
    "    f\"csb_{round(best_xgb['colsample_bytree'], 2)}_\"\n",
    "    f\"cb_level_{round(best_xgb['colsample_bylevel'], 2)}_\"\n",
    "    f\"cb_node_{round(best_xgb['colsample_bynode'], 2)}_\"\n",
    "    f\"mcw_{round(best_xgb['min_child_weight'], 2)}_\"\n",
    "    f\"gamma_{round(best_xgb['gamma'], 2)}_\"\n",
    "    f\"ra_{round(best_xgb['reg_alpha'], 2)}_\"\n",
    "    f\"rl_{round(best_xgb['reg_lambda'], 2)}_\"\n",
    "    f\"spw_{round(best_xgb['scale_pos_weight'], 2)}_\"\n",
    "    f\"gp_{grow_policy_options[best_xgb['grow_policy']]}_\"\n",
    "    f\"tm_{tree_method_options[best_xgb['tree_method']]}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_xgb.to_csv(os.path.join(\"submits\", file_name_xgb), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_xgb}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDVI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
