{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 43992294\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.impute import SimpleImputer\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "import scipy.stats as ss\n",
    "import os\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import ast\n",
    "\n",
    "import warnings\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_y_agrupar(df, columna, umbral=100):\n",
    "    \"\"\"\n",
    "    Limpia y agrupa categorías en una columna específica de un DataFrame.\n",
    "    Las categorías con frecuencia menor que el umbral se agrupan en 'Otros_{columna}',\n",
    "    y los valores NaN se reemplazan por 'Desconocidos_{columna}'.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame de pandas.\n",
    "    - columna: Nombre de la columna a limpiar y agrupar.\n",
    "    - umbral: Número mínimo de observaciones para mantener una categoría.\n",
    "\n",
    "    Returns:\n",
    "    - df: DataFrame con la columna modificada.\n",
    "    \"\"\"\n",
    "    # Contar la frecuencia de cada categoría incluyendo NaN\n",
    "    conteo_categorias = df[columna].value_counts(dropna=False)\n",
    "    \n",
    "    # Identificar categorías con observaciones por debajo del umbral\n",
    "    categorias_pequenas = conteo_categorias[conteo_categorias < umbral].index\n",
    "\n",
    "    # Filtrar las categorías poco frecuentes excluyendo NaN\n",
    "    categorias_pequenas = [cat for cat in categorias_pequenas if pd.notna(cat)]\n",
    "    \n",
    "    # Crear valor de relleno para categorías poco frecuentes\n",
    "    valor_pequeno = f'Otros'\n",
    "\n",
    "    # Crear valor de relleno para NaNs\n",
    "    valor_desconocido = f'Desconocidos'\n",
    "\n",
    "    # Aplicar la asignación de categorías\n",
    "    df[columna] = df[columna].apply(\n",
    "        lambda x: valor_pequeno if x in categorias_pequenas \n",
    "                  else (valor_desconocido if pd.isna(x) else x)\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def cramers_v(confusion_matrix):\n",
    "    \"\"\"\n",
    "    Calcula la estadística de Cramér's V para medir la asociación entre dos variables categóricas.\n",
    "\n",
    "    Parameters:\n",
    "    - confusion_matrix: Matriz de confusión (tabla de contingencia) entre dos variables.\n",
    "\n",
    "    Returns:\n",
    "    - Cramér's V: Valor entre 0 y 1 que indica la fuerza de la asociación.\n",
    "    \"\"\"\n",
    "    # Calcular el estadístico chi-cuadrado\n",
    "    chi2 = chi2_contingency(confusion_matrix, correction=False)[0]\n",
    "    # Número total de observaciones\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    # Obtener el número de filas y columnas de la matriz de confusión\n",
    "    r, k = confusion_matrix.shape\n",
    "    if min(r, k) == 1:\n",
    "        return np.nan  # Evitar dividir por cero\n",
    "    # Calcular Cramér's V\n",
    "    return np.sqrt(chi2 / (n * (min(r, k) - 1)))\n",
    "\n",
    "\n",
    "def expand_list_dummies(df, column, delimiter='|', prefix=None, suffix=None):\n",
    "    \"\"\"\n",
    "    Expande una columna que contiene listas (o cadenas representando listas) en múltiples columnas binarias\n",
    "    utilizando pandas.get_dummies, una por cada categoría única.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame de pandas.\n",
    "    - column (str): Nombre de la columna a expandir.\n",
    "    - delimiter (str): Delimitador a usar en get_dummies. Por defecto es '|'.\n",
    "    - prefix (str, optional): Prefijo para las nuevas columnas binarias.\n",
    "    - suffix (str, optional): Sufijo para las nuevas columnas binarias.\n",
    "\n",
    "    Returns:\n",
    "    - categories (set): Conjunto de categorías únicas encontradas.\n",
    "    - df_expanded (pd.DataFrame): DataFrame con las nuevas columnas binarias añadidas y la columna original eliminada.\n",
    "    \"\"\"\n",
    "    # Copiar el DataFrame para evitar modificar el original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Definir el valor para valores faltantes\n",
    "    valor_desconocido = f'Desconocidos_{column}'\n",
    "    \n",
    "    # Reemplazar NaN por el valor desconocido\n",
    "    df[column] = df[column].fillna(f'[\"{valor_desconocido}\"]')\n",
    "    \n",
    "    # Convertir las cadenas que representan listas en listas reales de Python\n",
    "    def parse_list(x):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(x)\n",
    "            if isinstance(parsed, list):\n",
    "                return parsed\n",
    "            else:\n",
    "                return [x]\n",
    "        except (ValueError, SyntaxError):\n",
    "            return [x]\n",
    "    \n",
    "    df[column] = df[column].apply(parse_list)\n",
    "    \n",
    "    # Convertir listas en cadenas separadas por el delimitador\n",
    "    df['temp_combined'] = df[column].apply(lambda lst: delimiter.join(map(str, lst)))\n",
    "    \n",
    "    # Generar las columnas binarias usando get_dummies\n",
    "    dummies = df['temp_combined'].str.get_dummies(sep=delimiter)\n",
    "    \n",
    "    # Añadir prefijo y/o sufijo si se especifica\n",
    "    if prefix:\n",
    "        dummies = dummies.add_prefix(f\"{prefix}_\")\n",
    "    if suffix:\n",
    "        dummies = dummies.add_suffix(f\"_{suffix}\")\n",
    "    \n",
    "    # Concatenar las columnas binarias al DataFrame original\n",
    "    df_expanded = pd.concat([df.drop(columns=[column, 'temp_combined']), dummies], axis=1)\n",
    "    \n",
    "    # Obtener las categorías únicas\n",
    "    categories = set(dummies.columns)\n",
    "    \n",
    "    return categories, df_expanded\n",
    "\n",
    "\n",
    "def categorizar_hora(hora):\n",
    "    \"\"\"\n",
    "    Categoriza una hora dada en 'mañana', 'tarde' o 'noche'.\n",
    "\n",
    "    Parameters:\n",
    "    - hora: Cadena de tiempo en formato 'HH:MM:SS'.\n",
    "\n",
    "    Returns:\n",
    "    - Categoría de la parte del día: 'mañana', 'tarde' o 'noche'.\n",
    "    \"\"\"\n",
    "    # Extraer la hora como entero\n",
    "    hora = int(hora.split(':')[0])\n",
    "    if 0 <= hora < 12:\n",
    "        return 'mañana'\n",
    "    elif 12 <= hora < 18:\n",
    "        return 'tarde'\n",
    "    else:\n",
    "        return 'noche'\n",
    "\n",
    "\n",
    "def convertir_auction_time(df):\n",
    "    \"\"\"\n",
    "    Convierte la columna 'auction_time' de formato timestamp a categorías de parte del día ('mañana', 'tarde', 'noche')\n",
    "    y crea columnas binarias para cada categoría.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame de pandas.\n",
    "\n",
    "    Returns:\n",
    "    - Lista de categorías creadas.\n",
    "    - df: DataFrame con las nuevas columnas binarias añadidas y las columnas originales eliminadas.\n",
    "    \"\"\"\n",
    "    # Convertir 'auction_time' de timestamp a cadena de tiempo 'HH:MM:SS'\n",
    "    df['auction_time'] = pd.to_datetime(df['auction_time'], unit='s').dt.strftime('%H:%M:%S')\n",
    "    \n",
    "    # Crear una nueva columna con las categorías de parte del día\n",
    "    df['parte_del_dia'] = df['auction_time'].apply(categorizar_hora)\n",
    "    \n",
    "    # Crear columnas binarias para cada parte del día\n",
    "    df['mañana'] = (df['parte_del_dia'] == 'mañana').astype(int)\n",
    "    df['tarde'] = (df['parte_del_dia'] == 'tarde').astype(int)\n",
    "    df['noche'] = (df['parte_del_dia'] == 'noche').astype(int)\n",
    "    \n",
    "    # Eliminar las columnas intermedias si no se necesitan\n",
    "    df.drop(['parte_del_dia', 'auction_time'], axis=1, inplace=True)\n",
    "    \n",
    "    return ['mañana', 'tarde', 'noche'], df\n",
    "\n",
    "\n",
    "def IntFloatToStr(df):\n",
    "    \"\"\"\n",
    "    Limpia los datos del DataFrame convirtiendo cualquier columna que contenga una combinación de \n",
    "    strings con enteros o strings con floats a cadenas de texto. Los valores NaN se reemplazan por '0'.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame de pandas.\n",
    "\n",
    "    Returns:\n",
    "    - df: DataFrame limpio con las columnas convertidas a strings donde corresponda.\n",
    "    \"\"\"\n",
    "    # Recorrer todas las columnas del DataFrame\n",
    "    for column in tqdm(df.columns, desc=\"Limpiando datos\"):\n",
    "        unique_types = df[column].apply(type).unique()\n",
    "        \n",
    "        # Verificar si la columna contiene tanto strings como enteros o floats\n",
    "        if {str, int}.issubset(set(unique_types)) or {str, float}.issubset(set(unique_types)):\n",
    "            # Convertir todos los valores a string, reemplazando NaN por '0'\n",
    "            df[column] = df[column].apply(lambda x: str(x) if pd.notna(x) else '0')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def data_cleaning(df):\n",
    "    \"\"\"\n",
    "    Realiza la limpieza y preprocesamiento de un DataFrame siguiendo varios pasos:\n",
    "    - Convertir y expandir columnas específicas.\n",
    "    - Agrupar categorías poco frecuentes en columnas categóricas.\n",
    "    - Limpiar los datos finales.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame de pandas.\n",
    "\n",
    "    Returns:\n",
    "    - all_categories: Conjunto de todas las categorías únicas encontradas.\n",
    "    - df: DataFrame limpio y preprocesado.\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.drop(columns=['device_id'])\n",
    "    \n",
    "    all_categories = set()\n",
    "    \n",
    "    # Mostrar el tamaño inicial del DataFrame\n",
    "    print(f\"Paso inicial: Tamaño del DataFrame: {df.shape}\")\n",
    "\n",
    "    # Identificar columnas categóricas excluyendo algunas específicas\n",
    "    categorical_features = [\n",
    "        col for col in df.select_dtypes(include=['object']).columns.tolist()\n",
    "        if col not in ['auction_list_0', 'action_list_1', 'action_list_2', 'auction_time']\n",
    "    ]\n",
    "    print(f\"Paso 1: Columnas categóricas identificadas (excluidas algunas): {categorical_features}\")\n",
    "\n",
    "    # Agrupar categorías poco frecuentes en las columnas categóricas\n",
    "    for col in tqdm(categorical_features, desc=\"Agrupando categorías poco frecuentes\"):\n",
    "        print(f\"Procesando columna categórica: {col}\")\n",
    "        df = limpiar_y_agrupar(df, col, umbral=100)\n",
    "\n",
    "    # Mostrar el estado del DataFrame después de agrupar categorías poco frecuentes\n",
    "    print(f\"Paso 2: Tamaño del DataFrame después de agrupar categorías: {df.shape}\")\n",
    "\n",
    "    # Convertir y expandir la columna 'auction_time'\n",
    "    print(f\"Paso 3: Expandiendo la columna 'auction_time'\")\n",
    "    categories_time, df = convertir_auction_time(df)\n",
    "    all_categories.update(categories_time)\n",
    "    print(f\"Categorías extraídas de 'auction_time': {categories_time}\")\n",
    "\n",
    "    # Expandir la columna 'auction_list_0' usando expand_list_dummies\n",
    "    print(f\"Paso 4: Expandiendo la columna 'auction_list_0'\")\n",
    "    categories_auction, df = expand_list_dummies(\n",
    "        df, \n",
    "        column='auction_list_0', \n",
    "        delimiter='|', \n",
    "        prefix=None, \n",
    "        suffix='auction_list_0'\n",
    "    )\n",
    "    all_categories.update(categories_auction)\n",
    "    print(f\"Categorías extraídas de 'auction_list_0': {categories_auction}\")\n",
    "\n",
    "    # Expandir la columna 'action_list_1' usando expand_list_dummies\n",
    "    print(f\"Paso 5: Expandiendo la columna 'action_list_1'\")\n",
    "    categories_action1, df = expand_list_dummies(\n",
    "        df, \n",
    "        column='action_list_1', \n",
    "        delimiter='|', \n",
    "        prefix=None, \n",
    "        suffix='action_list_1'\n",
    "    )\n",
    "    all_categories.update(categories_action1)\n",
    "    print(f\"Categorías extraídas de 'action_list_1': {categories_action1}\")\n",
    "\n",
    "    # Expandir la columna 'action_list_2' usando expand_list_dummies\n",
    "    print(f\"Paso 6: Expandiendo la columna 'action_list_2'\")\n",
    "    categories_action2, df = expand_list_dummies(\n",
    "        df, \n",
    "        column='action_list_2', \n",
    "        delimiter='|', \n",
    "        prefix=None, \n",
    "        suffix='action_list_2'\n",
    "    )\n",
    "    all_categories.update(categories_action2)\n",
    "    print(f\"Categorías extraídas de 'action_list_2': {categories_action2}\")\n",
    "\n",
    "    # Mostrar el tamaño del DataFrame antes de la limpieza adicional\n",
    "    print(f\"Paso 7: Tamaño del DataFrame antes de la limpieza adicional: {df.shape}\")\n",
    "\n",
    "    # Realizar limpieza adicional de los datos\n",
    "    df = IntFloatToStr(df)\n",
    "\n",
    "    # Tamaño final del DataFrame\n",
    "    print(f\"Paso 8: Tamaño final del DataFrame: {df.shape}\")\n",
    "\n",
    "    return all_categories, df\n",
    "\n",
    "def ajustar_columnas_test(train_df, test_df):\n",
    "    \"\"\"\n",
    "    Asegura que las columnas del conjunto de test coincidan con las del conjunto de entrenamiento,\n",
    "    con la excepción de que 'Label' no se puede borrar de train y 'id' no se puede borrar de test.\n",
    "    \n",
    "    Si faltan columnas en el conjunto de test, se agregan con valor 0.\n",
    "    Si sobran columnas en el conjunto de test que no están en el de entrenamiento, se eliminan.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_df: DataFrame del conjunto de entrenamiento (con las columnas transformadas)\n",
    "    - test_df: DataFrame del conjunto de test (sin transformar)\n",
    "    \n",
    "    Returns:\n",
    "    - test_df: DataFrame del conjunto de test con las columnas ajustadas.\n",
    "    \"\"\"\n",
    "    # Encontrar las columnas que están en train pero no en test, excepto 'Label'\n",
    "    missing_cols = set(train_df.columns) - set(test_df.columns) - {'Label'}\n",
    "\n",
    "    # Encontrar las columnas que están en test pero no en train, excepto 'id'\n",
    "    extra_cols = set(test_df.columns) - set(train_df.columns) - {'id'}\n",
    "\n",
    "    # Agregar las columnas faltantes en el conjunto de test con valor 0\n",
    "    for col in missing_cols:\n",
    "        test_df[col] = 0\n",
    "\n",
    "    # Eliminar las columnas extra del conjunto de test, excepto 'id'\n",
    "    test_df = test_df.drop(columns=extra_cols)\n",
    "\n",
    "    # Asegurarse de que las columnas estén en el mismo orden que en el conjunto de entrenamiento\n",
    "    test_df = test_df[train_df.columns.difference(['Label']).tolist() + ['id']]\n",
    "\n",
    "    return test_df\n",
    "\n",
    "def ajustar_columnas_val(train_df, val_df):\n",
    "    # Encontrar las columnas que están en train pero no en test, excepto 'Label'\n",
    "    missing_cols = set(train_df.columns) - set(val_df.columns)\n",
    "\n",
    "    # Encontrar las columnas que están en test pero no en train, excepto 'id'\n",
    "    extra_cols = set(val_df.columns) - set(train_df.columns)\n",
    "\n",
    "    # Agregar las columnas faltantes en el conjunto de test con valor 0\n",
    "    for col in missing_cols:\n",
    "        val_df[col] = 0\n",
    "\n",
    "    # Eliminar las columnas extra del conjunto de test, excepto 'id'\n",
    "    val_df = val_df.drop(columns=extra_cols)\n",
    "\n",
    "    return val_df\n",
    "\n",
    "# Función para manejar None en hiperparámetros en el nombre del archivo\n",
    "def handle_none(value):\n",
    "    return 'None' if value is None else str(value)\n",
    "\n",
    "\n",
    "def concatenar(df1, df2):\n",
    "    # Encontrar las columnas que están en df1 pero no en df2\n",
    "    missing_cols = set(df1.columns) - set(df2.columns)\n",
    "\n",
    "    # Encontrar las columnas que están en df2 pero no en df1\n",
    "    extra_cols = set(df2.columns) - set(df1.columns)\n",
    "\n",
    "    # Añadir las columnas faltantes en df2 con valores 0\n",
    "    for col in missing_cols:\n",
    "        df2[col] = 0\n",
    "\n",
    "    # Añadir las columnas faltantes en df1 con valores 0\n",
    "    for col in extra_cols:\n",
    "        df1[col] = 0\n",
    "\n",
    "    # Asegurarse de que ambas tengan las mismas columnas en el mismo orden\n",
    "    df1 = df1[df2.columns]\n",
    "\n",
    "    # Concatenar los DataFrames\n",
    "    df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    return df_concat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_csv(\"train_data_cleaned_21.csv\", dtype={'action_categorical_6': str})\n",
    "\n",
    "train_data_cleaned_20 = pd.read_csv(\"train_data_cleaned_20.csv\", dtype={'action_categorical_6': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_combined = concatenar(train_data_cleaned_21, train_data_cleaned_20)\n",
    "train_data_combined = train_data_cleaned_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_csv(\"data/ctr_19.csv\")\n",
    "\n",
    "# Reducir el conjunto de datos mientras se mantiene la distribución de 'label'\n",
    "val_data, _ = train_test_split(val_data, train_size=50000, stratify=val_data['Label'], random_state=random_state)\n",
    "\n",
    "categories_val, val_data = data_cleaning(val_data)\n",
    "\n",
    "val_data = ajustar_columnas_val(train_data_combined, val_data)\n",
    "\n",
    "val_data = val_data[train_data_combined.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caracteristicas principales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir la cantidad de filas del dataset combinado\n",
    "print(f\"Cantidad de filas en el dataset combinado: {train_data_combined.shape[0]}\")\n",
    "print(f\"Cantidad de columnas en el dataset combinado: {train_data_combined.shape[1]}\")\n",
    "\n",
    "# Ver porcentaje de clics vs no clics en la columna Label\n",
    "label_counts = train_data_combined['Label'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPorcentaje de clics (1) y no clics (0):\")\n",
    "print(label_counts)\n",
    "\n",
    "# Cantidad de clics (1) y no clics (0)\n",
    "label_counts_abs = train_data_combined['Label'].value_counts()\n",
    "print(\"\\nCantidad de clics (1) y no clics (0):\")\n",
    "print(label_counts_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen estadístico de las características numéricas\n",
    "print(train_data_combined.describe())\n",
    "\n",
    "# Resumen de las características categóricas\n",
    "print(train_data_combined.describe(include='object'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver el porcentaje de valores faltantes por columna\n",
    "missing_data = train_data_combined.isnull().mean() * 100\n",
    "print(missing_data[missing_data > 0].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en numéricos y categóricos\n",
    "numeric_features = train_data_combined.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "categorical_features = train_data_combined.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columna por columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre cada columna y mostrar los valores únicos\n",
    "for column in train_data_combined.columns:\n",
    "    unique_values = train_data_combined[column].unique()\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos ({len(unique_values)}): {unique_values}\")\n",
    "    print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre cada columna y mostrar los valores únicos y la cantidad de veces que aparecen\n",
    "for column in train_data_combined.columns:\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos y su frecuencia:\")\n",
    "    print(train_data_combined[column].value_counts())\n",
    "    print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar un conjunto vacío para almacenar los entity ids únicos\n",
    "unique_entity_ids_2 = set()\n",
    "\n",
    "# Recorrer cada fila de la columna 'action_list_2'\n",
    "for lista in train_data_combined['action_list_2'].dropna():\n",
    "    # Convertir el string de la lista a una lista real y agregar cada valor al conjunto\n",
    "    unique_entity_ids_2.update(eval(lista))  # eval transforma la string en lista si es necesario\n",
    "\n",
    "# Convertir el conjunto en una lista ordenada\n",
    "unique_entity_ids_2 = sorted(unique_entity_ids_2)\n",
    "\n",
    "# Mostrar los entity ids únicos\n",
    "print(unique_entity_ids_2)\n",
    "print(len(unique_entity_ids_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar un conjunto vacío para almacenar los entity ids únicos\n",
    "unique_entity_ids = set()\n",
    "\n",
    "# Recorrer cada fila de la columna 'action_list_1'\n",
    "for lista in train_data_combined['action_list_1'].dropna():\n",
    "    # Convertir el string de la lista a una lista real y agregar cada valor al conjunto\n",
    "    unique_entity_ids.update(eval(lista))  # eval transforma la string en lista si es necesario\n",
    "\n",
    "# Convertir el conjunto en una lista ordenada\n",
    "unique_entity_ids = sorted(unique_entity_ids)\n",
    "\n",
    "# Mostrar los entity ids únicos\n",
    "print(unique_entity_ids)\n",
    "print(len(unique_entity_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la cantidad de valores NaN en cada columna\n",
    "nan_counts = train_data_combined.isna().sum()\n",
    "\n",
    "# Filtrar columnas que tienen al menos un NaN\n",
    "nan_counts_filtered = nan_counts[nan_counts > 0]\n",
    "\n",
    "# Mostrar las columnas con valores NaN de mayor a menor\n",
    "print(nan_counts_filtered.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories, train_data_combined = data_cleaning(train_data_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data_combined.drop(columns=['Label'])\n",
    "y_train = train_data_combined['Label']\n",
    "\n",
    "X_val = val_data.drop(columns=['Label'])\n",
    "y_val = val_data['Label']\n",
    "\n",
    "# Asegurarse de que X_train y X_val sean dataframes de pandas\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_val = pd.DataFrame(X_val)\n",
    "\n",
    "# Asegurarse de que y_train y y_val sean series de pandas\n",
    "y_train = pd.Series(y_train)\n",
    "y_val = pd.Series(y_val)\n",
    "\n",
    "categories = [col for col in train_data_combined.columns if col.endswith(('_auction_list_0', '_action_list_1', '_action_list_2'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y validación con estratificación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data_combined.drop(columns='Label'),  # Características\n",
    "    train_data_combined['Label'],               # Variable objetivo\n",
    "    test_size=0.2,                              # 20% para validación\n",
    "    stratify=train_data_combined['Label'],      # Estratificación basada en la variable objetivo\n",
    "    random_state=random_state                   # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "# Verificar la proporción de clases en el conjunto de entrenamiento y validación\n",
    "print(\"Proporción en el conjunto de entrenamiento:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"Proporción en el conjunto de validación:\")\n",
    "print(y_val.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las columnas categóricas a codificar\n",
    "categorical_features_to_encode = [col for col in X_train.select_dtypes(include=['object']).columns if col not in categories]\n",
    "\n",
    "# Crear el Target Encoder para las columnas categóricas\n",
    "target_encoder = ce.TargetEncoder(cols=categorical_features_to_encode)\n",
    "\n",
    "# Crear el imputador\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Crear el clasificador de Random Forest\n",
    "model_rf_te = RandomForestClassifier(random_state=random_state)\n",
    "\n",
    "model_xgb_te = XGBClassifier(random_state=random_state)\n",
    "\n",
    "# Crear el pipeline con TargetEncoder, imputación y Random Forest\n",
    "pipeline_rf_te = Pipeline(steps=[\n",
    "    ('target_encoder', target_encoder),  # Paso de Target Encoding\n",
    "    ('imputer', imputer),                # Paso de imputación\n",
    "    ('classifier', model_rf_te)          # Paso de Random Forest\n",
    "])\n",
    "\n",
    "pipeline_xgb_te = Pipeline(steps=[\n",
    "    ('target_encoder', target_encoder),  # Paso de Target Encoding\n",
    "    ('imputer', imputer),                # Paso de imputación\n",
    "    ('classifier', model_xgb_te)         # Paso de Xgboost\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Busqueda de hiperparametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "param_dist = [\n",
    "    {\n",
    "        'classifier__bootstrap': [True],\n",
    "        'classifier__max_samples': np.linspace(0.5, 1.0, num=10),\n",
    "        'classifier__n_estimators': [100, 200, 300, 400, 500],\n",
    "        'classifier__max_depth': [None, 10, 20, 30, 40, 50],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    {\n",
    "        'classifier__bootstrap': [False],\n",
    "        'classifier__max_samples': [None],\n",
    "        'classifier__n_estimators': [100, 200, 300, 400, 500],\n",
    "        'classifier__max_depth': [None, 10, 20, 30, 40, 50],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Definir el RandomizedSearchCV para Random Forest\n",
    "random_search_rf_te = RandomizedSearchCV(\n",
    "    estimator=pipeline_rf_te,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=1,  # Aumenta el número de iteraciones para una búsqueda más exhaustiva\n",
    "    scoring='roc_auc',\n",
    "    cv=3,  # Número de folds en la validación cruzada\n",
    "    verbose=2,\n",
    "    random_state=random_state,\n",
    "    n_jobs=-1  # Usar todos los núcleos disponibles\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el RandomizedSearchCV al conjunto de entrenamiento de Random Forest\n",
    "random_search_rf_te.fit(X_train, y_train)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros encontrados para Random Forest:\")\n",
    "print(random_search_rf_te.best_params_)\n",
    "\n",
    "# Predecir en el conjunto de validación con el mejor modelo encontrado\n",
    "best_model_rf = random_search_rf_te.best_estimator_\n",
    "y_pred_proba_rf = best_model_rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calcular el AUC en el conjunto de validación\n",
    "auc_rf = roc_auc_score(y_val, y_pred_proba_rf)\n",
    "print(f\"AUC en el conjunto de validación para Random Forest: {auc_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Espacio de búsqueda para los hiperparámetros de XGBoost\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 10)), \n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]), \n",
    "    'gamma': hp.uniform('gamma', 0, 0.5),\n",
    "    'min_child_weight': hp.uniform('min_child_weight', 1, 10),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0.5, 2),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 1),  # L1 regularización\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 1),  # L2 regularización\n",
    "    'max_delta_step': hp.uniform('max_delta_step', 0, 10),  # Para datos desbalanceados\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1),  # Muestreo a nivel de split\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.5, 1),  # Muestreo por nodo\n",
    "    'grow_policy': hp.choice('grow_policy', ['depthwise', 'lossguide']),  # Estrategia de crecimiento\n",
    "    'tree_method': hp.choice('tree_method', ['auto', 'approx', 'hist'])  # Métodos de construcción del árbol\n",
    "}\n",
    "\n",
    "# Función objetivo para Hyperopt\n",
    "def objective_xgb_te(params):\n",
    "\n",
    "    params_mapped = {\n",
    "        'max_depth': params['max_depth'],  # 'max_depth' ya está bien indexado\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'n_estimators': params['n_estimators'],  # Mapear n_estimators\n",
    "        'gamma': params['gamma'],\n",
    "        'min_child_weight': int(params['min_child_weight']),  # Asegurar que sea entero\n",
    "        'subsample': params['subsample'],\n",
    "        'colsample_bytree': params['colsample_bytree'],\n",
    "        'scale_pos_weight': params['scale_pos_weight'],\n",
    "        'reg_alpha': params['reg_alpha'],  # L1 regularización\n",
    "        'reg_lambda': params['reg_lambda'],  # L2 regularización\n",
    "        'max_delta_step': params['max_delta_step'],  # Para datos desbalanceados\n",
    "        'colsample_bylevel': params['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "        'colsample_bynode': params['colsample_bynode'],  # Muestreo por nodo\n",
    "        'grow_policy': params['grow_policy'],  # Estrategia de crecimiento\n",
    "        'tree_method': params['tree_method']  # Método de construcción del árbol\n",
    "    }\n",
    "\n",
    "    # Definir el modelo con los hiperparámetros actuales\n",
    "    model_xgb_te = XGBClassifier(\n",
    "        max_depth=int(params_mapped['max_depth']),\n",
    "        learning_rate=params_mapped['learning_rate'],\n",
    "        n_estimators=int(params_mapped['n_estimators']),\n",
    "        gamma=params_mapped['gamma'],\n",
    "        min_child_weight=params_mapped['min_child_weight'],\n",
    "        subsample=params_mapped['subsample'],\n",
    "        colsample_bytree=params_mapped['colsample_bytree'],\n",
    "        scale_pos_weight=params_mapped['scale_pos_weight'],\n",
    "        reg_alpha=params_mapped['reg_alpha'],  # L1 regularización\n",
    "        reg_lambda=params_mapped['reg_lambda'],  # L2 regularización\n",
    "        max_delta_step=params_mapped['max_delta_step'],  # Para datos desbalanceados\n",
    "        colsample_bylevel=params_mapped['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "        colsample_bynode=params_mapped['colsample_bynode'],  # Muestreo por nodo\n",
    "        grow_policy=params_mapped['grow_policy'],  # Estrategia de crecimiento\n",
    "        tree_method=params_mapped['tree_method'],  # Método de construcción del árbol\n",
    "        use_label_encoder=False,  # Evitar advertencias en versiones más recientes de XGBoost\n",
    "        eval_metric='auc',  # Métrica de evaluación\n",
    "        random_state=random_state  # Para reproducibilidad\n",
    "    )\n",
    "\n",
    "    pipeline_xgb_te = Pipeline(steps=[\n",
    "        ('target_encoder', target_encoder),  # Paso de Target Encoding\n",
    "        ('imputer', imputer),                # Paso de imputación\n",
    "        ('classifier', model_xgb_te)         # Paso de Xgboost\n",
    "    ])\n",
    "    \n",
    "    # Entrenar el modelo con el conjunto de entrenamiento preprocesado para XGBoost\n",
    "    pipeline_xgb_te.fit(X_train, y_train)\n",
    "    \n",
    "    # Hacer predicciones en el conjunto de validación preprocesado para XGBoost\n",
    "    y_pred_proba = pipeline_xgb_te.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Calcular el AUC en el conjunto de validación\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    # Imprimir los hiperparámetros y el AUC actual\n",
    "    print(f\"Hiperparámetros: {params_mapped}, AUC: {auc:.4f}\")\n",
    "    \n",
    "    # Retornar el valor negativo del AUC ya que Hyperopt minimiza por defecto\n",
    "    return {'loss': 1 - auc, 'status': STATUS_OK}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la optimización\n",
    "trials = Trials()\n",
    "\n",
    "best_xgb = fmin(\n",
    "    fn=objective_xgb_te,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=5,\n",
    "    trials=trials,\n",
    "    rstate=np.random.default_rng(random_state)  # Asegurar reproducibilidad\n",
    ")\n",
    "\n",
    "# No es necesario volver a mapear los hiperparámetros aquí, ya se hizo dentro de la función objetivo\n",
    "print(\"Mejores hiperparámetros para XGBoost:\")\n",
    "print(best_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos de testeo\n",
    "test_data = pd.read_csv(\"data/ctr_test.csv\")\n",
    "\n",
    "test_data_cleaned = pd.read_csv('test_data_cleaned.csv')\n",
    "\n",
    "# Ajustar las columnas de testeo para que coincidan con las de entrenamiento\n",
    "test_data_cleaned = ajustar_columnas_test(train_data_combined, test_data_cleaned)\n",
    "\n",
    "test_data_cleaned = test_data_cleaned[X_train.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los hiperparámetros a los tipos correctos\n",
    "random_search_rf_te.best_params_['classifier__n_estimators'] = int(random_search_rf_te.best_params_['classifier__n_estimators'])\n",
    "random_search_rf_te.best_params_['classifier__min_samples_split'] = int(random_search_rf_te.best_params_['classifier__min_samples_split'])\n",
    "random_search_rf_te.best_params_['classifier__min_samples_leaf'] = int(random_search_rf_te.best_params_['classifier__min_samples_leaf'])\n",
    "random_search_rf_te.best_params_['classifier__max_depth'] = int(random_search_rf_te.best_params_['classifier__max_depth']) if random_search_rf_te.best_params_['classifier__max_depth'] is not None else None\n",
    "\n",
    "# Reconstruir el modelo con los mejores hiperparámetros\n",
    "best_model_rf = RandomForestClassifier(\n",
    "    n_estimators=random_search_rf_te.best_params_['classifier__n_estimators'],\n",
    "    max_depth=random_search_rf_te.best_params_['classifier__max_depth'],  # Asegurarse de que max_depth sea un entero o None\n",
    "    min_samples_split=random_search_rf_te.best_params_['classifier__min_samples_split'],\n",
    "    min_samples_leaf=random_search_rf_te.best_params_['classifier__min_samples_leaf'],\n",
    "    bootstrap=random_search_rf_te.best_params_['classifier__bootstrap'],\n",
    "    random_state=random_state,\n",
    "    max_samples=random_search_rf_te.best_params_['classifier__max_samples']\n",
    ")\n",
    "\n",
    "# Crear un nuevo pipeline reutilizando el preprocesador original y el mejor modelo\n",
    "pipeline_rf_te_best = Pipeline(steps=[\n",
    "    ('target_encoder', target_encoder),\n",
    "    ('imputer', imputer),\n",
    "    ('classifier', best_model_rf)\n",
    "])\n",
    "\n",
    "# Entrenar el pipeline con los datos de entrenamiento\n",
    "pipeline_rf_te_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir en el conjunto de testeo\n",
    "y_preds_rf_test = pipeline_rf_te_best.predict_proba(test_data_cleaned)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_rf = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_rf_test})\n",
    "submission_df_rf[\"id\"] = submission_df_rf[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros, incluyendo \"random_forest\"\n",
    "file_name_rf = (\n",
    "    f\"random_forest_predictions_n_estimators_{random_search_rf_te.best_params_['classifier__n_estimators']}_\"\n",
    "    f\"max_depth_{random_search_rf_te.best_params_['classifier__max_depth']}_\"\n",
    "    f\"min_samples_split_{random_search_rf_te.best_params_['classifier__min_samples_split']}_\"\n",
    "    f\"max_samples_{random_search_rf_te.best_params_['classifier__max_samples']}_\"\n",
    "    f\"bootstrap_{random_search_rf_te.best_params_['classifier__bootstrap']}_\"\n",
    "    f\"min_samples_leaf_{random_search_rf_te.best_params_['classifier__min_samples_leaf']}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_rf.to_csv(os.path.join(\"submits\", file_name_rf), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y validación con estratificación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data_combined.drop(columns='Label'),  # Características\n",
    "    train_data_combined['Label'],               # Variable objetivo\n",
    "    test_size=0.2,                              # 20% para validación\n",
    "    stratify=train_data_combined['Label'],      # Estratificación basada en la variable objetivo\n",
    "    random_state=random_state                   # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "# Asegurarse de que X_train y X_val sean dataframes de pandas\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_val = pd.DataFrame(X_val)\n",
    "\n",
    "# Asegurarse de que y_train y y_val sean series de pandas\n",
    "y_train = pd.Series(y_train)\n",
    "y_val = pd.Series(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [col for col in train_data_combined.columns if col.endswith(('_auction_list_0', '_action_list_1', '_action_list_2'))]\n",
    "\n",
    "# Definir las columnas categóricas y numéricas\n",
    "categorical_features_to_encode = [col for col in X_train.select_dtypes(include=['object']).columns if col not in categories]\n",
    "numeric_features = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Preprocesador común para imputación y codificación\n",
    "common_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Imputar valores numéricos con la media\n",
    "        ('num', SimpleImputer(strategy='mean'), numeric_features),\n",
    "        \n",
    "        # Imputar valores categóricos con 'Desconocido' y aplicar One-Hot Encoding\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='Desconocido')),\n",
    "            ('onehot', OneHotEncoder(drop='first', sparse_output=True, handle_unknown='ignore'))\n",
    "        ]), categorical_features_to_encode)\n",
    "    ],\n",
    "    remainder='drop'  # Excluir columnas no especificadas\n",
    ")\n",
    "\n",
    "model_rf = RandomForestClassifier(random_state=random_state)\n",
    "\n",
    "# Definir el modelo XGBoost\n",
    "model_xgb = XGBClassifier(\n",
    "    random_state=random_state, \n",
    "    use_label_encoder=False, \n",
    "    eval_metric='auc'\n",
    ")\n",
    "\n",
    "# Pipeline para XGBoost (mantiene matrices dispersas)\n",
    "pipeline_xgb = Pipeline(steps=[\n",
    "    ('preprocessor', common_preprocessor),\n",
    "    ('classifier', model_xgb),\n",
    "])\n",
    "\n",
    "# Pipeline para Random Forest (convierte a denso y optimiza tipos de datos)\n",
    "pipeline_rf = Pipeline(steps=[\n",
    "    ('preprocessor', common_preprocessor),\n",
    "    ('to_dense', FunctionTransformer(lambda x: x.tocsr().astype('float32').toarray(), accept_sparse=True)),\n",
    "    ('classifier', model_rf),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Busqueda de hiperparametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "param_dist = [\n",
    "    {\n",
    "        'classifier__bootstrap': [True],\n",
    "        'classifier__max_samples': np.linspace(0.5, 1.0, num=10),\n",
    "        'classifier__n_estimators': [100, 200, 300, 400, 500],\n",
    "        'classifier__max_depth': [None, 10, 20, 30, 40, 50],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    {\n",
    "        'classifier__bootstrap': [False],\n",
    "        'classifier__max_samples': [None],\n",
    "        'classifier__n_estimators': [100, 200, 300, 400, 500],\n",
    "        'classifier__max_depth': [None, 10, 20, 30, 40, 50],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Definir el RandomizedSearchCV para Random Forest\n",
    "random_search_rf = RandomizedSearchCV(\n",
    "    estimator=pipeline_rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=1,  # Aumenta el número de iteraciones para una búsqueda más exhaustiva\n",
    "    scoring='roc_auc',\n",
    "    cv=3,  # Número de folds en la validación cruzada\n",
    "    verbose=2,\n",
    "    random_state=random_state,\n",
    "    n_jobs=-1  # Usar todos los núcleos disponibles\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el RandomizedSearchCV al conjunto de entrenamiento de Random Forest\n",
    "random_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros encontrados para Random Forest:\")\n",
    "print(random_search_rf.best_params_)\n",
    "\n",
    "# Predecir en el conjunto de validación con el mejor modelo encontrado\n",
    "best_model_rf = random_search_rf.best_estimator_\n",
    "y_pred_proba_rf = best_model_rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calcular el AUC en el conjunto de validación\n",
    "auc_rf = roc_auc_score(y_val, y_pred_proba_rf)\n",
    "print(f\"AUC en el conjunto de validación para Random Forest: {auc_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "space_rf = {\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]),\n",
    "    'max_depth': hp.choice('max_depth', [None, 10, 20, 30, 40, 50]),\n",
    "    'max_samples': hp.uniform('max_samples', 0.5, 1.0),\n",
    "    'min_samples_split': hp.choice('min_samples_split', [2, 5, 10]),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', [1, 2, 4]),\n",
    "    'bootstrap': hp.choice('bootstrap', [True, False])\n",
    "}\n",
    "\n",
    "# Función objetivo para Hyperopt\n",
    "def objective_rf(params):\n",
    "    # Asegurarse de que 'max_samples' solo se usa si 'bootstrap' es True\n",
    "    if not params['bootstrap']:\n",
    "        params.pop('max_samples', None)  # No se puede utilizar max_samples si bootstrap es False\n",
    "    \n",
    "    # Crear el clasificador con los parámetros actuales\n",
    "    model_rf = RandomForestClassifier(**params, random_state=random_state)\n",
    "    \n",
    "    # Crear el pipeline con el preprocesador común y el clasificador\n",
    "    pipeline_rf = Pipeline(steps=[\n",
    "        ('preprocessor', common_preprocessor),\n",
    "        ('to_dense', FunctionTransformer(lambda x: x.tocsr().astype('float32').toarray(), accept_sparse=True)),\n",
    "        ('classifier', model_rf),\n",
    "    ])\n",
    "    \n",
    "    # Entrenar el pipeline en el conjunto de entrenamiento\n",
    "    pipeline_rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predecir probabilidades en el conjunto de validación\n",
    "    y_pred_proba = pipeline_rf.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Calcular el AUC en el conjunto de validación\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    # Imprimir los hiperparámetros y el AUC actual\n",
    "    print(f\"Hiperparámetros: {params}, AUC: {auc:.4f}\")\n",
    "    \n",
    "    # Retornar el valor negativo del AUC ya que Hyperopt minimiza por defecto\n",
    "    return {'loss': 1 - auc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la optimización\n",
    "trials = Trials()  # Para guardar información sobre cada iteración\n",
    "best_rf = fmin(\n",
    "    fn=objective_rf, \n",
    "    space=space_rf, \n",
    "    algo=tpe.suggest, \n",
    "    max_evals=10,  # Número de evaluaciones aumentado para una búsqueda más exhaustiva\n",
    "    trials=trials,\n",
    "    rstate=np.random.default_rng(random_state)  # Asegurar reproducibilidad\n",
    ")\n",
    "\n",
    "# Definir las opciones de los parámetros que usan hp.choice\n",
    "max_depth_options = [None, 10, 20, 30, 40, 50]\n",
    "bootstrap_options = [True, False]\n",
    "max_samples_options = [None] + list(np.linspace(0.5, 1.0, num=6))  # [None, 0.5, 0.6, ..., 1.0]\n",
    "min_samples_split_options = [2, 5, 10]\n",
    "\n",
    "if not bootstrap_options[best_rf['bootstrap']]:\n",
    "    best_rf['max_samples'] = None  # No se puede utilizar max_samples si bootstrap es False\n",
    "\n",
    "# Mapear los índices de hp.choice a los valores reales\n",
    "best_params_rf_mapped = {\n",
    "    'classifier__n_estimators': int(best_rf['n_estimators']),\n",
    "    'classifier__max_depth': max_depth_options[best_rf['max_depth']],\n",
    "    'classifier__max_samples': best_rf['max_samples'],\n",
    "    'classifier__min_samples_split': min_samples_split_options[best_rf['min_samples_split']],\n",
    "    'classifier__min_samples_leaf': int(best_rf['min_samples_leaf']),\n",
    "    'classifier__bootstrap': bootstrap_options[best_rf['bootstrap']]\n",
    "}\n",
    "\n",
    "print(\"Mejores hiperparámetros mapeados para Random Forest:\")\n",
    "print(best_params_rf_mapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el espacio de búsqueda de hiperparámetros para XGBoost\n",
    "param_dist_xgb = {\n",
    "    'classifier__n_estimators': [100, 200, 300, 400, 500],\n",
    "    'classifier__max_depth': [3, 5, 10, 15, 20],  # Profundidad máxima del árbol\n",
    "    'classifier__learning_rate': np.linspace(0.01, 0.3, 10),  # Tasa de aprendizaje\n",
    "    'classifier__subsample': np.linspace(0.5, 1.0, num=5),  # Fracción de muestras a usar para cada árbol\n",
    "    'classifier__colsample_bytree': np.linspace(0.5, 1.0, num=5),  # Fracción de características a usar para cada árbol\n",
    "    'classifier__gamma': [0, 0.1, 0.2, 0.3],  # Regularización\n",
    "    'classifier__reg_alpha': [0, 0.01, 0.1, 1],  # Regularización L1\n",
    "    'classifier__reg_lambda': [1, 1.5, 2, 3]  # Regularización L2\n",
    "}\n",
    "\n",
    "# Definir el RandomizedSearchCV para XGBoost\n",
    "random_search_xgb = RandomizedSearchCV(\n",
    "    estimator=pipeline_xgb,\n",
    "    param_distributions=param_dist_xgb,\n",
    "    n_iter=3,  # Aumenta el número de combinaciones de hiperparámetros a probar\n",
    "    scoring='roc_auc',  # Métrica para evaluar\n",
    "    cv=3,  # Número de folds en la validación cruzada\n",
    "    verbose=2,\n",
    "    random_state=random_state,\n",
    "    n_jobs=-1  # Usar todos los núcleos disponibles\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el modelo a los datos de entrenamiento preprocesados para XGBoost\n",
    "random_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros encontrados para XGBoost:\")\n",
    "print(random_search_xgb.best_params_)\n",
    "\n",
    "# Predecir en el conjunto de validación con el mejor modelo encontrado\n",
    "best_model_xgb = random_search_xgb.best_estimator_\n",
    "y_pred_proba_xgb = best_model_xgb.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Calcular el AUC en el conjunto de validación\n",
    "auc_xgb = roc_auc_score(y_val, y_pred_proba_xgb)\n",
    "print(f\"AUC en el conjunto de validación para XGBoost: {auc_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Espacio de búsqueda para los hiperparámetros de XGBoost\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 10)), \n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]), \n",
    "    'gamma': hp.uniform('gamma', 0, 0.5),\n",
    "    'min_child_weight': hp.uniform('min_child_weight', 1, 10),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0.5, 2),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 1),  # L1 regularización\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 1),  # L2 regularización\n",
    "    'max_delta_step': hp.uniform('max_delta_step', 0, 10),  # Para datos desbalanceados\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1),  # Muestreo a nivel de split\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.5, 1),  # Muestreo por nodo\n",
    "    'grow_policy': hp.choice('grow_policy', ['depthwise', 'lossguide']),  # Estrategia de crecimiento\n",
    "    'tree_method': hp.choice('tree_method', ['auto', 'approx', 'hist'])  # Métodos de construcción del árbol\n",
    "}\n",
    "\n",
    "# Función objetivo para Hyperopt\n",
    "def objective_xgb(params):\n",
    "\n",
    "    params_mapped = {\n",
    "        'max_depth': params['max_depth'],  # 'max_depth' ya está bien indexado\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'n_estimators': params['n_estimators'],  # Mapear n_estimators\n",
    "        'gamma': params['gamma'],\n",
    "        'min_child_weight': int(params['min_child_weight']),  # Asegurar que sea entero\n",
    "        'subsample': params['subsample'],\n",
    "        'colsample_bytree': params['colsample_bytree'],\n",
    "        'scale_pos_weight': params['scale_pos_weight'],\n",
    "        'reg_alpha': params['reg_alpha'],  # L1 regularización\n",
    "        'reg_lambda': params['reg_lambda'],  # L2 regularización\n",
    "        'max_delta_step': params['max_delta_step'],  # Para datos desbalanceados\n",
    "        'colsample_bylevel': params['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "        'colsample_bynode': params['colsample_bynode'],  # Muestreo por nodo\n",
    "        'grow_policy': params['grow_policy'],  # Estrategia de crecimiento\n",
    "        'tree_method': params['tree_method']  # Método de construcción del árbol\n",
    "    }\n",
    "\n",
    "    # Definir el modelo con los hiperparámetros actuales\n",
    "    model_xgb = XGBClassifier(\n",
    "        max_depth=int(params_mapped['max_depth']),\n",
    "        learning_rate=params_mapped['learning_rate'],\n",
    "        n_estimators=int(params_mapped['n_estimators']),\n",
    "        gamma=params_mapped['gamma'],\n",
    "        min_child_weight=params_mapped['min_child_weight'],\n",
    "        subsample=params_mapped['subsample'],\n",
    "        colsample_bytree=params_mapped['colsample_bytree'],\n",
    "        scale_pos_weight=params_mapped['scale_pos_weight'],\n",
    "        reg_alpha=params_mapped['reg_alpha'],  # L1 regularización\n",
    "        reg_lambda=params_mapped['reg_lambda'],  # L2 regularización\n",
    "        max_delta_step=params_mapped['max_delta_step'],  # Para datos desbalanceados\n",
    "        colsample_bylevel=params_mapped['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "        colsample_bynode=params_mapped['colsample_bynode'],  # Muestreo por nodo\n",
    "        grow_policy=params_mapped['grow_policy'],  # Estrategia de crecimiento\n",
    "        tree_method=params_mapped['tree_method'],  # Método de construcción del árbol\n",
    "        use_label_encoder=False,  # Evitar advertencias en versiones más recientes de XGBoost\n",
    "        eval_metric='auc',  # Métrica de evaluación\n",
    "        random_state=random_state  # Para reproducibilidad\n",
    "    )\n",
    "\n",
    "    pipeline_xgb = Pipeline(steps=[\n",
    "        ('preprocessor', common_preprocessor),\n",
    "        ('classifier', model_xgb),\n",
    "    ])\n",
    "    \n",
    "    # Entrenar el modelo con el conjunto de entrenamiento preprocesado para XGBoost\n",
    "    pipeline_xgb.fit(X_train, y_train)\n",
    "    \n",
    "    # Hacer predicciones en el conjunto de validación preprocesado para XGBoost\n",
    "    y_pred_proba = pipeline_xgb.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Calcular el AUC en el conjunto de validación\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    # Imprimir los hiperparámetros y el AUC actual\n",
    "    print(f\"Hiperparámetros: {params_mapped}, AUC: {auc:.4f}\")\n",
    "    \n",
    "    # Retornar el valor negativo del AUC ya que Hyperopt minimiza por defecto\n",
    "    return {'loss': 1 - auc, 'status': STATUS_OK}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la optimización\n",
    "trials = Trials()\n",
    "\n",
    "best_xgb = fmin(\n",
    "    fn=objective_xgb,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=5,\n",
    "    trials=trials,\n",
    "    rstate=np.random.default_rng(random_state)  # Asegurar reproducibilidad\n",
    ")\n",
    "\n",
    "# No es necesario volver a mapear los hiperparámetros aquí, ya se hizo dentro de la función objetivo\n",
    "print(\"Mejores hiperparámetros para XGBoost:\")\n",
    "print(best_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pueba de Hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos de testeo\n",
    "test_data = pd.read_csv(\"data/ctr_test.csv\")\n",
    "\n",
    "X_test = pd.read_csv('test_data_cleaned.csv', dtype={'action_categorical_6': str})\n",
    "\n",
    "# # Ajustar las columnas de testeo para que coincidan con las de entrenamiento\n",
    "# X_test = ajustar_columnas_test(X_train, X_test)\n",
    "\n",
    "# X_test = X_test[X_train.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test[['auction_bidfloor', 'creative_height', 'creative_width', 'tarde', 'noche']]\n",
    "\n",
    "X_test = X_test[X_train.columns]\n",
    "\n",
    "print(X_test.shape[1])\n",
    "print(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los hiperparámetros a los tipos correctos\n",
    "random_search_rf.best_params_['classifier__n_estimators'] = int(random_search_rf.best_params_['classifier__n_estimators'])\n",
    "random_search_rf.best_params_['classifier__min_samples_split'] = int(random_search_rf.best_params_['classifier__min_samples_split'])\n",
    "random_search_rf.best_params_['classifier__min_samples_leaf'] = int(random_search_rf.best_params_['classifier__min_samples_leaf'])\n",
    "random_search_rf.best_params_['classifier__max_depth'] = int(random_search_rf.best_params_['classifier__max_depth']) if random_search_rf.best_params_['classifier__max_depth'] is not None else None\n",
    "\n",
    "# Reconstruir el modelo con los mejores hiperparámetros\n",
    "best_model_rf = RandomForestClassifier(\n",
    "    n_estimators=random_search_rf.best_params_['classifier__n_estimators'],\n",
    "    max_depth=random_search_rf.best_params_['classifier__max_depth'],  # Asegurarse de que max_depth sea un entero o None\n",
    "    min_samples_split=random_search_rf.best_params_['classifier__min_samples_split'],\n",
    "    min_samples_leaf=random_search_rf.best_params_['classifier__min_samples_leaf'],\n",
    "    bootstrap=random_search_rf.best_params_['classifier__bootstrap'],\n",
    "    random_state=random_state,\n",
    "    max_samples=random_search_rf.best_params_['classifier__max_samples']\n",
    ")\n",
    "\n",
    "# Crear un nuevo pipeline reutilizando el preprocesador original y el mejor modelo\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', common_preprocessor),  # Reutilizamos el preprocesador definido previamente\n",
    "    ('to_dense', FunctionTransformer(lambda x: x.tocsr().astype('float32').toarray(), accept_sparse=True)),\n",
    "    ('classifier', best_model_rf),\n",
    "])\n",
    "\n",
    "# Entrenar el pipeline con los datos de entrenamiento\n",
    "rf_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir en el conjunto de testeo\n",
    "y_preds_rf_test = rf_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_rf = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_rf_test})\n",
    "submission_df_rf[\"id\"] = submission_df_rf[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros, incluyendo \"random_forest\"\n",
    "file_name_rf = (\n",
    "    f\"random_forest_predictions_n_estimators_{random_search_rf.best_params_['classifier_n_estimators']}_\"\n",
    "    f\"max_depth_{random_search_rf.best_params_['classifier_max_depth']}_\"\n",
    "    f\"min_samples_split_{random_search_rf.best_params_['classifier_min_samples_split']}_\"\n",
    "    f\"max_samples_{random_search_rf.best_params_['classifier_max_samples']}_\"\n",
    "    f\"bootstrap_{random_search_rf.best_params_['classifier_bootstrap']}_\"\n",
    "    f\"min_samples_leaf_{random_search_rf.best_params_['classifier_min_samples_leaf']}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_rf.to_csv(os.path.join(\"submits\", file_name_rf), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_rf = RandomForestClassifier(\n",
    "    n_estimators=best_params_rf_mapped['classifier__n_estimators'],\n",
    "    max_depth=best_params_rf_mapped['classifier__max_depth'],\n",
    "    max_samples=best_params_rf_mapped['classifier__max_samples'],\n",
    "    min_samples_split=best_params_rf_mapped['classifier__min_samples_split'],\n",
    "    min_samples_leaf=best_params_rf_mapped['classifier__min_samples_leaf'],\n",
    "    bootstrap=best_params_rf_mapped['classifier__bootstrap'],\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# Crear un nuevo pipeline reutilizando el preprocesador original y el mejor modelo\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', common_preprocessor),  # Reutilizamos el preprocesador definido previamente\n",
    "    ('to_dense', FunctionTransformer(lambda x: x.tocsr().astype('float32').toarray(), accept_sparse=True)),\n",
    "    ('classifier', best_model_rf),\n",
    "])\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar predicciones de probabilidad en el conjunto de testeo preprocesado\n",
    "y_preds_rf_test = rf_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_xgb = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_rf_test})\n",
    "submission_df_xgb[\"id\"] = submission_df_xgb[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros, incluyendo \"random_forest\"\n",
    "file_name_rf = (\n",
    "    f\"random_forest_predictions_n_estimators_{best_params_rf_mapped['classifier__n_estimators']}_\"\n",
    "    f\"max_depth_{handle_none(best_params_rf_mapped['classifier__max_depth'])}_\"\n",
    "    f\"min_samples_split_{best_params_rf_mapped['classifier__min_samples_split']}_\"\n",
    "    f\"max_samples_{handle_none(best_params_rf_mapped['classifier__max_samples'])}_\"\n",
    "    f\"bootstrap_{best_params_rf_mapped['classifier__bootstrap']}_\"\n",
    "    f\"min_samples_leaf_{best_params_rf_mapped['classifier__min_samples_leaf']}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_rf.to_csv(os.path.join(\"submits\", file_name_rf), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los hiperparámetros a los tipos correctos\n",
    "random_search_xgb.best_params_['classifier__n_estimators'] = int(random_search_xgb.best_params_['classifier__n_estimators'])\n",
    "random_search_xgb.best_params_['classifier__min_samples_split'] = int(random_search_xgb.best_params_['classifier__min_samples_split'])\n",
    "random_search_xgb.best_params_['classifier__min_samples_leaf'] = int(random_search_xgb.best_params_['classifier__min_samples_leaf'])\n",
    "random_search_xgb.best_params_['classifier__max_depth'] = int(random_search_xgb.best_params_['classifier__max_depth']) if random_search_xgb.best_params_['classifier__max_depth'] is not None else None\n",
    "\n",
    "# Reconstruir el modelo con los mejores hiperparámetros\n",
    "best_model_xgb = RandomForestClassifier(\n",
    "    n_estimators=random_search_xgb['classifier__n_estimators'],\n",
    "    max_depth=random_search_xgb['classifier__max_depth'],\n",
    "    learning_rate=random_search_xgb['classifier__learning_rate'],\n",
    "    subsample=random_search_xgb['classifier__subsample'],\n",
    "    colsample_bytree=random_search_xgb['classifier__colsample_bytree'],\n",
    "    min_child_weight=random_search_xgb['classifier__min_child_weight'],\n",
    "    gamma=random_search_xgb['classifier__gamma'],\n",
    "    reg_alpha=random_search_xgb['classifier__reg_alpha'],\n",
    "    reg_lambda=random_search_xgb['classifier__reg_lambda'],\n",
    "    random_state=random_state,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"auc\"  # Establecer AUC como métrica de evaluación\n",
    ")\n",
    "\n",
    "# Crear un nuevo pipeline reutilizando el preprocesador original y el mejor modelo\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', common_preprocessor),  # Reutilizamos el preprocesador definido previamente\n",
    "    ('to_dense', FunctionTransformer(lambda x: x.tocsr().astype('float32').toarray(), accept_sparse=True)),\n",
    "    ('classifier', best_model_xgb),\n",
    "])\n",
    "\n",
    "# Entrenar el pipeline con los datos de entrenamiento\n",
    "xgb_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir en el conjunto de testeo\n",
    "y_preds_rf_test = xgb_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_rf = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_rf_test})\n",
    "submission_df_rf[\"id\"] = submission_df_rf[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros, incluyendo \"random_forest\"\n",
    "file_name_rf = (\n",
    "    f\"xgboost_predictions_n_estimators_{random_search_xgb['classifier__n_estimators']}_\"\n",
    "    f\"max_depth_{random_search_xgb['classifier__max_depth']}_\"\n",
    "    f\"learning_rate_{random_search_xgb['lclassifier__earning_rate']}_\"\n",
    "    f\"subsample_{random_search_xgb['classifier__subsample']}_\"\n",
    "    f\"colsample_bytree_{random_search_xgb['classifier__colsample_bytree']}_\"\n",
    "    f\"min_child_weight_{random_search_xgb['classifier__min_child_weight']}_\"\n",
    "    f\"gamma_{random_search_xgb['classifier__gamma']}_\"\n",
    "    f\"reg_alpha_{random_search_xgb['classifier__reg_alpha']}_\"\n",
    "    f\"reg_lambda_{random_search_xgb['classifier__reg_lambda']}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_rf.to_csv(os.path.join(\"submits\", file_name_rf), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices\n",
    "tree_method_options = ['auto', 'exact', 'approx', 'hist']\n",
    "grow_policy_options = ['depthwise', 'lossguide']\n",
    "\n",
    "\n",
    "# Reconstruir el modelo de XGBoost con los mejores hiperparámetros\n",
    "best_model_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=best_xgb['n_estimators'],\n",
    "    max_depth=best_xgb['max_depth'],\n",
    "    learning_rate=best_xgb['learning_rate'],\n",
    "    subsample=best_xgb['subsample'],\n",
    "    colsample_bytree=best_xgb['colsample_bytree'],\n",
    "    min_child_weight=best_xgb['min_child_weight'],\n",
    "    gamma=best_xgb['gamma'],\n",
    "    reg_alpha=best_xgb['reg_alpha'],  # L1 regularización\n",
    "    reg_lambda=best_xgb['reg_lambda'],  # L2 regularización\n",
    "    max_delta_step=best_xgb['max_delta_step'],  # Para datos desbalanceados\n",
    "    colsample_bylevel=best_xgb['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "    colsample_bynode=best_xgb['colsample_bynode'],  # Muestreo por nodo\n",
    "    grow_policy=grow_policy_options[best_xgb['grow_policy']],  # Estrategia de crecimiento\n",
    "    tree_method=tree_method_options[best_xgb['tree_method']],  # Método de construcción del árbol\n",
    "    random_state=random_state,\n",
    "    use_label_encoder=False,  # Evitar advertencias en versiones más recientes de XGBoost\n",
    "    eval_metric=\"auc\"  # Establecer AUC como métrica de evaluación\n",
    ")\n",
    "\n",
    "# Crear un nuevo pipeline reutilizando el preprocesador original y el mejor modelo\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', common_preprocessor),  # Reutilizamos el preprocesador definido previamente\n",
    "    ('classifier', best_model_xgb),\n",
    "])\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "xgb_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir en el conjunto de testeo\n",
    "y_preds_xgb_test = xgb_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_xgb = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_xgb_test})\n",
    "submission_df_xgb[\"id\"] = submission_df_xgb[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros\n",
    "file_name_xgb = (\n",
    "    f\"xgboost_preds_ne_{best_xgb['n_estimators']}_\"\n",
    "    f\"md_{handle_none(best_xgb['max_depth'])}_\"\n",
    "    f\"lr_{round(best_xgb['learning_rate'], 2)}_\"\n",
    "    f\"ss_{round(best_xgb['subsample'], 2)}_\"\n",
    "    f\"csb_{round(best_xgb['colsample_bytree'], 2)}_\"\n",
    "    f\"mcw_{round(best_xgb['min_child_weight'], 2)}_\"\n",
    "    f\"gamma_{round(best_xgb['gamma'], 2)}_\"\n",
    "    f\"ra_{round(best_xgb['reg_alpha'], 2)}_\"\n",
    "    f\"rl_{round(best_xgb['reg_lambda'], 2)}_\"\n",
    "    f\"mds_{round(best_xgb['max_delta_step'], 2)}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_xgb.to_csv(os.path.join(\"submits\", file_name_xgb), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_xgb}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDVI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
