{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 43992294\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.impute import SimpleImputer\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "import scipy.stats as ss\n",
    "import os\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import ast\n",
    "\n",
    "import warnings\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Cython imports\n",
    "from tools import agrupar_categorias_cython, custom_one_hot_encoder_cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desactivar warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def augment_train_data(main_train_df, supplementary_df, umbral_raras=100):\n",
    "    \"\"\"\n",
    "    Agrega filas del dataset suplementario al conjunto de entrenamiento principal\n",
    "    basándose en categorías desconocidas y raras, evitando la duplicación de filas.\n",
    "\n",
    "    Parámetros:\n",
    "    - main_train_df (pd.DataFrame): DataFrame principal de entrenamiento.\n",
    "    - supplementary_df (pd.DataFrame): DataFrame suplementario del cual se extraerán las filas.\n",
    "    - umbral_raras (int): Umbral de frecuencia para considerar una categoría como rara.\n",
    "\n",
    "    Retorna:\n",
    "    - main_train_df (pd.DataFrame): DataFrame de entrenamiento actualizado.\n",
    "    - categorias_desconocidas (dict): Diccionario actualizado de categorías desconocidas.\n",
    "    - categorias_raras (dict): Diccionario actualizado de categorías raras.\n",
    "    \"\"\"\n",
    "    # Definir columnas que no deseas tratar como categóricas\n",
    "    columns_to_exclude = ['auction_list_0', 'action_list_1', 'action_list_2', 'auction_time']\n",
    "\n",
    "    # Identificar columnas categóricas excluyendo las especificadas\n",
    "    categorical_features = main_train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    categorical_features = [col for col in categorical_features if col not in columns_to_exclude]\n",
    "    \n",
    "    # Crear un diccionario para almacenar las categorías desconocidas por columna\n",
    "    categorias_desconocidas = {}\n",
    "    \n",
    "    # Iterar a través de cada columna categórica para identificar categorías desconocidas\n",
    "    for columna in categorical_features:\n",
    "        # Obtener las categorías únicas en el conjunto de entrenamiento\n",
    "        categorias_train = set(main_train_df[columna].dropna().unique())\n",
    "        \n",
    "        # Obtener las categorías únicas en el dataset suplementario\n",
    "        categorias_suplementario = set(supplementary_df[columna].dropna().unique())\n",
    "        \n",
    "        # Identificar las categorías en el dataset suplementario que no están en el entrenamiento\n",
    "        desconocidas = categorias_suplementario - categorias_train\n",
    "        \n",
    "        # Almacenar las categorías desconocidas en el diccionario como una lista\n",
    "        categorias_desconocidas[columna] = list(desconocidas)\n",
    "    \n",
    "    # Inicializar el diccionario para almacenar las categorías raras por columna\n",
    "    categorias_raras = {}\n",
    "    \n",
    "    # Identificar categorías raras en el conjunto de entrenamiento\n",
    "    for columna in categorical_features:\n",
    "        # Contar la frecuencia de cada categoría\n",
    "        frecuencia = main_train_df[columna].value_counts()\n",
    "        \n",
    "        # Identificar categorías que aparecen menos de umbral_raras veces\n",
    "        raras = frecuencia[frecuencia < umbral_raras].index.tolist()\n",
    "        \n",
    "        # Almacenar en el diccionario\n",
    "        categorias_raras[columna] = raras\n",
    "    \n",
    "    # Crear una máscara booleana para filas con categorías desconocidas o raras\n",
    "    mask_desconocidas = pd.Series([False] * len(supplementary_df))\n",
    "    mask_raras = pd.Series([False] * len(supplementary_df))\n",
    "    \n",
    "    for columna in categorical_features:\n",
    "        # Actualizar la máscara para categorías desconocidas\n",
    "        if categorias_desconocidas[columna]:\n",
    "            mask_desconocidas = mask_desconocidas | supplementary_df[columna].isin(categorias_desconocidas[columna])\n",
    "        \n",
    "        # Actualizar la máscara para categorías raras\n",
    "        if categorias_raras[columna]:\n",
    "            mask_raras = mask_raras | supplementary_df[columna].isin(categorias_raras[columna])\n",
    "    \n",
    "    # Combinar ambas máscaras\n",
    "    mask_total = mask_desconocidas | mask_raras\n",
    "    \n",
    "    # Filtrar filas únicas a agregar\n",
    "    filas_a_agregar = supplementary_df[mask_total].drop_duplicates()\n",
    "    \n",
    "    # Mostrar información de agregación\n",
    "    total_agregadas = len(filas_a_agregar)\n",
    "    print(f\"\\nAgregando {total_agregadas} filas del dataset suplementario basadas en categorías desconocidas o raras.\")\n",
    "    \n",
    "    # Agregar las filas al conjunto de entrenamiento\n",
    "    main_train_df = pd.concat([main_train_df, filas_a_agregar], ignore_index=True)\n",
    "    \n",
    "    # Actualizar los diccionarios eliminando las categorías que ya han sido agregadas\n",
    "    for columna in categorical_features:\n",
    "        # Actualizar categorías desconocidas\n",
    "        if categorias_desconocidas[columna]:\n",
    "            categorias_agregadas = filas_a_agregar[columna].unique().tolist()\n",
    "            categorias_desconocidas[columna] = [cat for cat in categorias_desconocidas[columna] if cat not in categorias_agregadas]\n",
    "        \n",
    "        # Actualizar categorías raras\n",
    "        if categorias_raras[columna]:\n",
    "            # Recontar la frecuencia después de agregar\n",
    "            frecuencia = main_train_df[columna].value_counts()\n",
    "            nuevas_raras = frecuencia[frecuencia < umbral_raras].index.tolist()\n",
    "            categorias_raras[columna] = nuevas_raras\n",
    "    \n",
    "    return main_train_df, categorias_desconocidas, categorias_raras\n",
    "\n",
    "def expand_list_dummies_cython(df, column, delimiter='|', prefix=None, suffix=None):\n",
    "    \"\"\"\n",
    "    Expande una columna que contiene listas en múltiples columnas binarias usando un one-hot encoder optimizado con Cython.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame de pandas.\n",
    "    - column (str): Nombre de la columna a expandir.\n",
    "    - delimiter (str): Delimitador a usar en get_dummies (por defecto '|').\n",
    "    - prefix (str, optional): Prefijo para las nuevas columnas binarias.\n",
    "    - suffix (str, optional): Sufijo para las nuevas columnas binarias.\n",
    "    \n",
    "    Returns:\n",
    "    - df_expanded (pd.DataFrame): DataFrame con las nuevas columnas binarias añadidas y la columna original eliminada.\n",
    "    \"\"\"\n",
    "    # Copiar el DataFrame para evitar modificar el original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Nombre para el valor desconocido\n",
    "    unknown_value = f'Desconocido_{column[len(column)-1]}'\n",
    "    \n",
    "    # Reemplazar NaN por el valor desconocido\n",
    "    df[column] = df[column].fillna(unknown_value)\n",
    "    \n",
    "    # Convertir las cadenas que representan listas en listas reales de Python\n",
    "    def parse_list(x):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(x)\n",
    "            if isinstance(parsed, list) and len(parsed) == 0:\n",
    "                # Tratar listas vacías como 'Desconocido'\n",
    "                return [unknown_value]\n",
    "            elif isinstance(parsed, list):\n",
    "                # Convertir números a strings\n",
    "                return [str(item) for item in parsed]\n",
    "            else:\n",
    "                return [str(x)]\n",
    "        except (ValueError, SyntaxError):\n",
    "            return [unknown_value]\n",
    "    \n",
    "    df[column] = df[column].apply(parse_list)\n",
    "    \n",
    "    # Convertir la columna en una lista de listas para pasarla a la función Cythonizada\n",
    "    data_list = df[column].tolist()\n",
    "    \n",
    "    # Llamar a la función optimizada en Cython\n",
    "    unique_categories, binary_matrix = custom_one_hot_encoder_cython(data_list)\n",
    "    \n",
    "    # Crear un DataFrame binario usando la matriz devuelta por Cython\n",
    "    binary_df = pd.DataFrame(binary_matrix, index=df.index, columns=unique_categories)\n",
    "    \n",
    "    # Añadir prefijo y/o sufijo si se especifica\n",
    "    if prefix:\n",
    "        binary_df = binary_df.add_prefix(f\"{prefix}_\")\n",
    "    if suffix:\n",
    "        binary_df = binary_df.add_suffix(f\"_{suffix}\")\n",
    "    \n",
    "    # Concatenar las columnas binarias al DataFrame original, asegurando que solo se asigna 1, no se suma\n",
    "    for col in binary_df.columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = np.where((df[col] == 1) | (binary_df[col] == 1), 1, 0)\n",
    "        else:\n",
    "            df[col] = binary_df[col]\n",
    "    \n",
    "    # Eliminar la columna original ya que ha sido expandida\n",
    "    df = df.drop(columns=[column])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def categorizar_hora(hora):\n",
    "    \"\"\"\n",
    "    Categoriza una hora dada en 'mañana', 'tarde' o 'noche'.\n",
    "\n",
    "    Parameters:\n",
    "    - hora: Cadena de tiempo en formato 'HH:MM:SS' o un valor nulo.\n",
    "\n",
    "    Returns:\n",
    "    - Categoría de la parte del día: 'mañana', 'tarde' o 'noche'.\n",
    "    \"\"\"\n",
    "    # Verificar si la hora es nula o no es una cadena\n",
    "    if pd.isna(hora) or not isinstance(hora, str):\n",
    "        return 'h_desconocido'\n",
    "    \n",
    "    # Extraer la hora como entero\n",
    "    try:\n",
    "        hora_int = int(hora.split(':')[0])\n",
    "        if 0 <= hora_int < 12:\n",
    "            return 'mañana'\n",
    "        elif 12 <= hora_int < 18:\n",
    "            return 'tarde'\n",
    "        else:\n",
    "            return 'noche'\n",
    "    except (ValueError, IndexError):\n",
    "        return 'h_desconocido'\n",
    "\n",
    "def convertir_auction_time(df):\n",
    "    \"\"\"\n",
    "    Convierte la columna 'auction_time' de formato timestamp a categorías de parte del día ('mañana', 'tarde', 'noche')\n",
    "    y crea columnas binarias para cada categoría.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame de pandas.\n",
    "\n",
    "    Returns:\n",
    "    - Lista de categorías creadas.\n",
    "    - df: DataFrame con las nuevas columnas binarias añadidas y las columnas originales eliminadas.\n",
    "    \"\"\"\n",
    "    # Convertir 'auction_time' de timestamp a cadena de tiempo 'HH:MM:SS', manejar NaN\n",
    "    df['auction_time'] = pd.to_datetime(df['auction_time'], unit='s', errors='coerce').dt.strftime('%H:%M:%S')\n",
    "    \n",
    "    # Crear una nueva columna con las categorías de parte del día\n",
    "    df['parte_del_dia'] = df['auction_time'].apply(categorizar_hora)\n",
    "    \n",
    "    # Crear columnas binarias para cada parte del día\n",
    "    df['mañana'] = (df['parte_del_dia'] == 'mañana').astype(int)\n",
    "    df['tarde'] = (df['parte_del_dia'] == 'tarde').astype(int)\n",
    "    df['noche'] = (df['parte_del_dia'] == 'noche').astype(int)\n",
    "    df['h_desconocido'] = (df['parte_del_dia'] == 'h_desconocido').astype(int)\n",
    "    \n",
    "    # Eliminar las columnas intermedias si no se necesitan\n",
    "    df.drop(['parte_del_dia', 'auction_time'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_optimized(df):\n",
    "    \"\"\"\n",
    "    Aplica una serie de transformaciones al DataFrame utilizando una función Cython optimizada.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame a procesar.\n",
    "    \n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame procesado.\n",
    "    - categories (set): Conjunto de categorías creadas durante el procesamiento.\n",
    "    \"\"\"\n",
    "    # Definir el número total de pasos para la barra de progreso\n",
    "    total_steps = 5\n",
    "    \n",
    "    # Inicializar la barra de progreso\n",
    "    with tqdm(total=total_steps, desc=\"Procesando DataFrame\", unit=\"paso\") as pbar:\n",
    "        \n",
    "        # 1. Agrupar categorías raras en 'Otro' y reemplazar NaN por 'Desconocidos' usando Cython\n",
    "        # Preparar los datos\n",
    "        columns_to_exclude = ['auction_list_0', 'action_list_1', 'action_list_2', 'auction_time']\n",
    "        categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "        categorical_features = [col for col in categorical_features if col not in columns_to_exclude]\n",
    "        \n",
    "        # Convertir a matriz bidimensional\n",
    "        data_matrix = df[categorical_features].values.tolist()\n",
    "        data_matrix_cython = [list(row) for row in data_matrix]\n",
    "        \n",
    "        # Llamar a la función Cythonizada\n",
    "        df_cython_data = agrupar_categorias_cython(\n",
    "            categorical_features=categorical_features,\n",
    "            columns_to_exclude=columns_to_exclude,\n",
    "            data=data_matrix_cython,\n",
    "            umbral=0                             # Con un umbral de 0, se agrupan solo los NaN\n",
    "        )\n",
    "        \n",
    "        # Reasignar los datos al DataFrame\n",
    "        for i, col in enumerate(categorical_features):\n",
    "            df[col] = [row[i] for row in df_cython_data]\n",
    "        \n",
    "        pbar.update(1)  # Actualizar la barra de progreso\n",
    "        \n",
    "        # 2. Expandir la columna 'auction_list_0' en variables binarias\n",
    "        df = expand_list_dummies_cython(df, 'auction_list_0')\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 3. Expandir la columna 'action_list_1' en variables binarias\n",
    "        df = expand_list_dummies_cython(df, 'action_list_1')\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 4. Expandir la columna 'action_list_2' en variables binarias\n",
    "        df = expand_list_dummies_cython(df, 'action_list_2')\n",
    "        pbar.update(1)\n",
    "        \n",
    "        # 5. Convertir 'auction_time' a categorías de parte del día y crear variables binarias\n",
    "        df = convertir_auction_time(df)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_data_with_dask(df, npartitions=10):\n",
    "    \"\"\"\n",
    "    Procesa un DataFrame utilizando Dask para distribuir el trabajo en varias particiones.\n",
    "    Aplica la función process_optimized a cada partición del DataFrame.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): El DataFrame de pandas a procesar.\n",
    "    - npartitions (int): Número de particiones en las que se dividirá el DataFrame para su procesamiento.\n",
    "    \n",
    "    Retorna:\n",
    "    - final_df (pd.DataFrame): El DataFrame procesado y concatenado.\n",
    "    \"\"\"\n",
    "    # Convertir el DataFrame de pandas a Dask con el número de particiones especificado\n",
    "    dask_df = dd.from_pandas(df, npartitions=npartitions)\n",
    "    \n",
    "    # Aplicar la función process_optimized a cada partición\n",
    "    dask_df = dask_df.map_partitions(lambda df_partition: process_optimized(df_partition))\n",
    "    \n",
    "    # Ejecutar el cálculo distribuido y convertir el resultado a pandas\n",
    "    final_df = dask_df.compute()\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancatenamos datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_21 = pd.read_csv('data/ctr_21.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_20 = pd.read_csv('data/ctr_20.csv')\n",
    "train_data_19 = pd.read_csv('data/ctr_19.csv')\n",
    "train_data_18 = pd.read_csv('data/ctr_18.csv')\n",
    "train_data_17 = pd.read_csv('data/ctr_17.csv')\n",
    "train_data_16 = pd.read_csv('data/ctr_16.csv')\n",
    "train_data_15 = pd.read_csv('data/ctr_15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplementary_datasets = [\n",
    "    ('1', train_data_20),\n",
    "    ('2', train_data_19),\n",
    "    ('3', train_data_18),\n",
    "    ('4', train_data_17),\n",
    "    ('5', train_data_16),\n",
    "    ('6', train_data_15)\n",
    "]\n",
    "\n",
    "train_data = train_data_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nombre, dataset in supplementary_datasets:\n",
    "    print(f\"\\nProcesando dataset {nombre}/{len(supplementary_datasets)}\")\n",
    "    train_data, categorias_desconocidas, categorias_raras = augment_train_data(train_data, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape[0])\n",
    "\n",
    "print(train_data_20.shape[0] + train_data_19.shape[0] + train_data_18.shape[0] + train_data_17.shape[0] + train_data_16.shape[0] + train_data_15.shape[0] + train_data_21.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = agrupar_categorias(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('train_data_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Una vez ya concatenado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_data_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/ctr_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test vs Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En test pero no en train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_exclude = ['auction_list_0', 'action_list_1', 'action_list_2', 'auction_time']\n",
    "categorical_features = train_data.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_features = [col for col in categorical_features if col not in columns_to_exclude]\n",
    "\n",
    "# Crear un diccionario para almacenar las categorías desconocidas por columna\n",
    "categorias_desconocidas = {}\n",
    "\n",
    "total = 0\n",
    "\n",
    "# Iterar a través de cada columna categórica\n",
    "for columna in categorical_features:\n",
    "    # Obtener las categorías únicas en el conjunto de entrenamiento\n",
    "    categorias_train = set(train_data[columna].dropna().unique())\n",
    "    \n",
    "    # Obtener las categorías únicas en el conjunto de prueba\n",
    "    categorias_test = set(test[columna].dropna().unique())\n",
    "    \n",
    "    # Identificar las categorías en test que no están en train\n",
    "    desconocidas = categorias_test - categorias_train\n",
    "    \n",
    "    # Almacenar las categorías desconocidas en el diccionario como una lista\n",
    "    categorias_desconocidas[columna] = list(desconocidas)\n",
    "    if len(desconocidas) > 0:\n",
    "        print(f\"'{columna}' ({len(desconocidas)}): {desconocidas} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos una máscara booleana que será True si la fila tiene una categoría desconocida en cualquier columna\n",
    "mask = pd.Series([False] * len(test))\n",
    "\n",
    "# Iteramos a través de cada columna categórica\n",
    "for columna in categorias_desconocidas:\n",
    "    # Verificamos si el valor en la columna está dentro de las categorías desconocidas\n",
    "    mask = mask | test[columna].isin(categorias_desconocidas[columna])\n",
    "\n",
    "# Contamos el número de filas donde al menos una categoría es desconocida\n",
    "num_filas_desconocidas = mask.sum()\n",
    "\n",
    "print(f\"Cantidad de filas con al menos una categoría desconocida: {num_filas_desconocidas} ({num_filas_desconocidas / len(test) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En train pero no en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_exclude = ['auction_list_0', 'action_list_1', 'action_list_2', 'auction_time']\n",
    "categorical_features = test.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_features = [col for col in categorical_features if col not in columns_to_exclude]\n",
    "\n",
    "# Crear un diccionario para almacenar las categorías desconocidas por columna\n",
    "categorias_conocidas = {}\n",
    "\n",
    "# Iterar a través de cada columna categórica\n",
    "for columna in categorical_features:\n",
    "    # Obtener las categorías únicas en el conjunto de entrenamiento\n",
    "    categorias_test = set(test[columna].dropna().unique())\n",
    "    \n",
    "    # Obtener las categorías únicas en el conjunto de prueba\n",
    "    categorias_train = set(train_data[columna].dropna().unique())\n",
    "    \n",
    "    # Identificar las categorías en test que no están en train\n",
    "    conocidas = categorias_train - categorias_test\n",
    "    \n",
    "    # Almacenar las categorías desconocidas en el diccionario como una lista\n",
    "    categorias_conocidas[columna] = list(conocidas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos una máscara booleana que será True si la fila tiene una categoría desconocida en cualquier columna\n",
    "mask = pd.Series([False] * len(test))\n",
    "\n",
    "# Iteramos a través de cada columna categórica\n",
    "for columna in categorias_conocidas:\n",
    "    # Verificamos si el valor en la columna está dentro de las categorías desconocidas\n",
    "    mask = mask | train_data[columna].isin(categorias_conocidas[columna])\n",
    "\n",
    "# Contamos el número de filas donde al menos una categoría es desconocida\n",
    "num_filas_conocidas = mask.sum()\n",
    "\n",
    "print(f\"Cantidad de filas con al menos una categoría conocida: {num_filas_conocidas} ({num_filas_conocidas / len(train_data) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for columna in categorias_desconocidas:\n",
    "    test[columna] = test[columna].apply(lambda x: 'Otro' if x in categorias_desconocidas[columna] else x)\n",
    "\n",
    "for columna in categorias_conocidas:\n",
    "    train_data[columna] = train_data[columna].apply(lambda x: 'Otro' if x in categorias_conocidas[columna] else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cleaned = process_data_with_dask(train_data, npartitions=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columas IDs y sus apariciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas que son IDs\n",
    "id_columns = [\n",
    "    'action_categorical_0', 'action_categorical_1', 'action_categorical_2',\n",
    "    'action_categorical_3', 'action_categorical_4',\n",
    "    'action_list_1', 'action_list_2',\n",
    "    'auction_categorical_0', 'auction_categorical_1', 'auction_categorical_7',\n",
    "    'auction_categorical_8', 'auction_categorical_9', 'auction_categorical_11',\n",
    "    'creative_categorical_0', 'creative_categorical_5',\n",
    "    'device_id'\n",
    "]\n",
    "\n",
    "list_id_columns = ['action_list_1', 'action_list_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in id_columns:\n",
    "    # Excluir listas de IDs por ahora\n",
    "    if col not in ['action_list_1', 'action_list_2']:\n",
    "        total_ids = train_data[col].nunique()\n",
    "        total_rows = train_data[col].shape[0]\n",
    "        print(f\"Columna '{col}': {total_ids} IDs únicos en {total_rows} filas.\")\n",
    "        \n",
    "        if total_ids == total_rows:\n",
    "            print(f\"  -> Todos los IDs en '{col}' son únicos.\\n\")\n",
    "        else:\n",
    "            duplicados = train_data[col].duplicated().sum()\n",
    "            print(f\"  -> Hay {duplicados} IDs duplicados en '{col}'.\\n\")\n",
    "\n",
    "for col in list_id_columns:\n",
    "    # Expandir las listas de IDs en una sola lista\n",
    "    all_ids = train_data[col].dropna().apply(lambda x: x.split(','))  # Asumiendo que las listas están separadas por comas\n",
    "    all_ids = list(chain.from_iterable(all_ids))\n",
    "    unique_ids = set(all_ids)\n",
    "    total_ids = len(unique_ids)\n",
    "    print(f\"Columna '{col}': {total_ids} IDs únicos en todas las filas.\")\n",
    "    \n",
    "    # Verificar si cada ID es único (no se repite en diferentes filas)\n",
    "    counts = pd.Series(all_ids).value_counts()\n",
    "    duplicados = counts[counts > 1].count()\n",
    "    print(f\"  -> Hay {duplicados} IDs que se repiten en múltiples filas.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y validación con estratificación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data_cleaned.drop(columns='Label'),  # Características\n",
    "    train_data_cleaned['Label'],               # Variable objetivo\n",
    "    test_size=0.2,                              # 20% para validación\n",
    "    stratify=train_data_cleaned['Label'],      # Estratificación basada en la variable objetivo\n",
    "    random_state=random_state                   # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "# Verificar la proporción de clases en el conjunto de entrenamiento y validación\n",
    "print(\"Proporción en el conjunto de entrenamiento:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"Proporción en el conjunto de validación:\")\n",
    "print(y_val.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las columnas categóricas a codificar\n",
    "categorical_features_to_encode = col in X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Crear el Target Encoder para las columnas categóricas\n",
    "target_encoder = ce.TargetEncoder(cols=categorical_features_to_encode)\n",
    "\n",
    "# Crear el imputador\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "model_xgb_te = XGBClassifier(random_state=random_state)\n",
    "\n",
    "pipeline_xgb_te = Pipeline(steps=[\n",
    "    ('target_encoder', target_encoder),  # Paso de Target Encoding\n",
    "    ('imputer', imputer),                # Paso de imputación\n",
    "    ('classifier', model_xgb_te)         # Paso de Xgboost\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDVI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
