{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "random_state = 43992294\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.impute import SimpleImputer\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "import scipy.stats as ss\n",
    "import os\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import ast\n",
    "\n",
    "import warnings\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Cython imports\n",
    "from tools import agrupar_categorias_cython, custom_one_hot_encoder_cython, boolean_features_ohe_cython, agrupar_edades_cython, expand_action_list_0_cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'warnings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwarnings\u001b[49m\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m tqdm\u001b[38;5;241m.\u001b[39mpandas()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Agrega filas que contienen categorías desconocidas o raras (poca frecuencia) al conjunto de entrenamiento\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'warnings' is not defined"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "\n",
    "# Agrega filas que contienen categorías desconocidas o raras (poca frecuencia) al conjunto de entrenamiento\n",
    "def augment_train_data(main_train_df, supplementary_df, umbral_raras=100):\n",
    "    \"\"\"\n",
    "    Agrega filas del dataset suplementario al conjunto de entrenamiento principal\n",
    "    basándose en categorías desconocidas y raras, evitando la duplicación de filas.\n",
    "\n",
    "    Parámetros:\n",
    "    - main_train_df (pd.DataFrame): DataFrame principal de entrenamiento.\n",
    "    - supplementary_df (pd.DataFrame): DataFrame suplementario del cual se extraerán las filas.\n",
    "    - umbral_raras (int): Umbral de frecuencia para considerar una categoría como rara.\n",
    "\n",
    "    Retorna:\n",
    "    - main_train_df (pd.DataFrame): DataFrame de entrenamiento actualizado.\n",
    "    - categorias_desconocidas (dict): Diccionario actualizado de categorías desconocidas.\n",
    "    - categorias_raras (dict): Diccionario actualizado de categorías raras.\n",
    "    \"\"\"\n",
    "    # Definir columnas que no deseas tratar como categóricas\n",
    "    columns_to_exclude = ['auction_list_0', 'action_list_1', 'action_list_2', 'auction_time']\n",
    "\n",
    "    # Identificar columnas categóricas excluyendo las especificadas\n",
    "    categorical_features = main_train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    categorical_features = [col for col in categorical_features if col not in columns_to_exclude]\n",
    "    \n",
    "    # Crear un diccionario para almacenar las categorías desconocidas por columna\n",
    "    categorias_desconocidas = {}\n",
    "    \n",
    "    # Iterar a través de cada columna categórica para identificar categorías desconocidas\n",
    "    for columna in categorical_features:\n",
    "        # Obtener las categorías únicas en el conjunto de entrenamiento\n",
    "        categorias_train = set(main_train_df[columna].dropna().unique())\n",
    "        \n",
    "        # Obtener las categorías únicas en el dataset suplementario\n",
    "        categorias_suplementario = set(supplementary_df[columna].dropna().unique())\n",
    "        \n",
    "        # Identificar las categorías en el dataset suplementario que no están en el entrenamiento\n",
    "        desconocidas = categorias_suplementario - categorias_train\n",
    "        \n",
    "        # Almacenar las categorías desconocidas en el diccionario como una lista\n",
    "        categorias_desconocidas[columna] = list(desconocidas)\n",
    "    \n",
    "    # Inicializar el diccionario para almacenar las categorías raras por columna\n",
    "    categorias_raras = {}\n",
    "    \n",
    "    # Identificar categorías raras en el conjunto de entrenamiento\n",
    "    for columna in categorical_features:\n",
    "        # Contar la frecuencia de cada categoría\n",
    "        frecuencia = main_train_df[columna].value_counts()\n",
    "        \n",
    "        # Identificar categorías que aparecen menos de umbral_raras veces\n",
    "        raras = frecuencia[frecuencia < umbral_raras].index.tolist()\n",
    "        \n",
    "        # Almacenar en el diccionario\n",
    "        categorias_raras[columna] = raras\n",
    "    \n",
    "    # Crear una máscara booleana para filas con categorías desconocidas o raras\n",
    "    mask_desconocidas = pd.Series([False] * len(supplementary_df))\n",
    "    mask_raras = pd.Series([False] * len(supplementary_df))\n",
    "    \n",
    "    for columna in categorical_features:\n",
    "        # Actualizar la máscara para categorías desconocidas\n",
    "        if categorias_desconocidas[columna]:\n",
    "            mask_desconocidas = mask_desconocidas | supplementary_df[columna].isin(categorias_desconocidas[columna])\n",
    "        \n",
    "        # Actualizar la máscara para categorías raras\n",
    "        if categorias_raras[columna]:\n",
    "            mask_raras = mask_raras | supplementary_df[columna].isin(categorias_raras[columna])\n",
    "    \n",
    "    # Combinar ambas máscaras\n",
    "    mask_total = mask_desconocidas | mask_raras\n",
    "    \n",
    "    # Filtrar filas únicas a agregar\n",
    "    filas_a_agregar = supplementary_df[mask_total].drop_duplicates()\n",
    "    \n",
    "    # Mostrar información de agregación\n",
    "    total_agregadas = len(filas_a_agregar)\n",
    "    print(f\"\\nAgregando {total_agregadas} filas del dataset suplementario basadas en categorías desconocidas o raras.\")\n",
    "    \n",
    "    # Agregar las filas al conjunto de entrenamiento\n",
    "    main_train_df = pd.concat([main_train_df, filas_a_agregar], ignore_index=True)\n",
    "    \n",
    "    # Actualizar los diccionarios eliminando las categorías que ya han sido agregadas\n",
    "    for columna in categorical_features:\n",
    "        # Actualizar categorías desconocidas\n",
    "        if categorias_desconocidas[columna]:\n",
    "            categorias_agregadas = filas_a_agregar[columna].unique().tolist()\n",
    "            categorias_desconocidas[columna] = [cat for cat in categorias_desconocidas[columna] if cat not in categorias_agregadas]\n",
    "        \n",
    "        # Actualizar categorías raras\n",
    "        if categorias_raras[columna]:\n",
    "            # Recontar la frecuencia después de agregar\n",
    "            frecuencia = main_train_df[columna].value_counts()\n",
    "            nuevas_raras = frecuencia[frecuencia < umbral_raras].index.tolist()\n",
    "            categorias_raras[columna] = nuevas_raras\n",
    "    \n",
    "    return main_train_df, categorias_desconocidas, categorias_raras\n",
    "\n",
    "# Agrega como columnas binarias las listas de la columna 'auction_list_0', 'action_list_1' y 'action_list_2'\n",
    "def expand_list_dummies_cython(df, column, delimiter='|', prefix=None, suffix=None):\n",
    "    \"\"\"\n",
    "    Expande una columna que contiene listas en múltiples columnas binarias usando un one-hot encoder optimizado con Cython.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame de pandas.\n",
    "    - column (str): Nombre de la columna a expandir.\n",
    "    - delimiter (str): Delimitador a usar en get_dummies (por defecto '|').\n",
    "    - prefix (str, optional): Prefijo para las nuevas columnas binarias.\n",
    "    - suffix (str, optional): Sufijo para las nuevas columnas binarias.\n",
    "    \n",
    "    Returns:\n",
    "    - df_expanded (pd.DataFrame): DataFrame con las nuevas columnas binarias añadidas y la columna original eliminada.\n",
    "    \"\"\"\n",
    "    print(f\"Comenzando la expansión de la columna: '{column}'\")\n",
    "    \n",
    "    # Copiar el DataFrame para evitar modificar el original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Reemplazar NaN por listas vacías\n",
    "    print(f\"Reemplazando NaN en la columna '{column}' por listas vacías.\")\n",
    "    df[column] = df[column].fillna('[]')\n",
    "    \n",
    "    # Definir la función de parsing con impresión de errores\n",
    "    def parse_list(x):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(x)\n",
    "            if isinstance(parsed, list):\n",
    "                # Convertir todos los elementos a strings\n",
    "                return [str(item) for item in parsed]\n",
    "            else:\n",
    "                # Si no es una lista, tratar como un solo elemento\n",
    "                return [str(x)]\n",
    "        except (ValueError, SyntaxError):\n",
    "            # En caso de error al parsear, retornar una lista vacía\n",
    "            return []\n",
    "    \n",
    "    # Aplicar la función de parsing con una barra de progreso\n",
    "    df[column] = df[column].progress_apply(parse_list)\n",
    "    \n",
    "    # Convertir la columna en una lista de listas para pasarla a la función Cythonizada\n",
    "    data_list = df[column].tolist()\n",
    "    \n",
    "    # Llamar a la función optimizada en Cython\n",
    "    unique_categories, binary_matrix = custom_one_hot_encoder_cython(data_list)\n",
    "    print(f\"Codificación completada. {len(unique_categories)} categorías únicas encontradas.\")\n",
    "    \n",
    "    # Crear un DataFrame binario usando la matriz devuelta por Cython\n",
    "    binary_df = pd.DataFrame(binary_matrix, index=df.index, columns=unique_categories)\n",
    "\n",
    "    if prefix:\n",
    "        binary_df = binary_df.add_prefix(f\"{prefix}_\")\n",
    "    if suffix:\n",
    "        binary_df = binary_df.add_suffix(f\"_{suffix}\")\n",
    "    \n",
    "    # Concatenar las columnas binarias al DataFrame original con una barra de progreso\n",
    "    print(\"Concatenando las columnas binarias al DataFrame original.\")\n",
    "    for col in tqdm(binary_df.columns, desc=\"Concatenando columnas binarias\"):\n",
    "        if col in df.columns:\n",
    "            df[col] = np.where((df[col] == 1) | (binary_df[col] == 1), 1, 0)\n",
    "        else:\n",
    "            df[col] = binary_df[col]\n",
    "    \n",
    "    # Eliminar la columna original ya que ha sido expandida\n",
    "    print(f\"Eliminando la columna original '{column}' del DataFrame.\")\n",
    "    df = df.drop(columns=[column])\n",
    "    \n",
    "    print(f\"Expansión de la columna '{column}' completada exitosamente.\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# One-hot encode de columnas booleanas utilizando Cython\n",
    "def boolean_features_ohe(df, columns_to_encode=['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2']):\n",
    "    \"\"\"\n",
    "    Realiza one-hot encoding en columnas booleanas especificadas utilizando una función optimizada con Cython.\n",
    "    Además, muestra el progreso del procesamiento utilizando tqdm y añade comentarios explicativos.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame original que contiene las columnas booleanas a codificar.\n",
    "    - columns_to_encode (list): Lista de nombres de columnas booleanas a codificar.\n",
    "\n",
    "    Retorna:\n",
    "    - df_expanded (pd.DataFrame): DataFrame con las nuevas columnas codificadas añadidas y las columnas booleanas originales eliminadas.\n",
    "    \"\"\"\n",
    "    # Copiar el DataFrame para evitar modificar el original\n",
    "    df = df.copy()\n",
    "    print(\"Inicio del proceso de one-hot encoding para las columnas booleanas especificadas.\")\n",
    "\n",
    "    # Paso 1: Encontrar todos los valores únicos en las columnas a codificar\n",
    "    unique_values_set = set()\n",
    "    print(\"Recopilando valores únicos de las columnas a codificar:\")\n",
    "    for col in tqdm(columns_to_encode, desc=\"Procesando columnas para valores únicos\"):\n",
    "        unique_vals_col = df[col].dropna().unique()\n",
    "        unique_values_set.update(unique_vals_col)\n",
    "    unique_values = sorted(unique_values_set)\n",
    "    print(f\"Valores únicos encontrados: {unique_values}\")\n",
    "\n",
    "    # Paso 2: Convertir las columnas a listas de listas para ser procesadas en Cython\n",
    "    list_data = []\n",
    "    print(\"Convirtiendo las columnas booleanas a listas de listas para Cython:\")\n",
    "    for col in tqdm(columns_to_encode, desc=\"Convertir columnas a listas\"):\n",
    "        column_list = df[col].astype(str).tolist()  # Mantener los valores como strings\n",
    "        list_data.append(column_list)\n",
    "    print(\"Conversión completada.\")\n",
    "\n",
    "    # Paso 3: Procesar los datos con la función optimizada en Cython\n",
    "    print(\"Realizando one-hot encoding utilizando la función optimizada en Cython:\")\n",
    "    ohe_result = boolean_features_ohe_cython(list_data, unique_values)\n",
    "    print(\"One-hot encoding completado.\")\n",
    "\n",
    "    # Paso 4: Convertir el resultado de Cython a un DataFrame, alineando el índice con df\n",
    "    print(\"Creando el DataFrame de columnas codificadas:\")\n",
    "    ohe_df = pd.DataFrame(ohe_result, columns=unique_values, index=df.index)\n",
    "    print(f\"DataFrame de one-hot encoding creado con {len(ohe_df.columns)} columnas y {ohe_df.shape[0]} filas.\")\n",
    "\n",
    "    # Paso 5: Concatenar las nuevas columnas codificadas al DataFrame original\n",
    "    print(\"Concatenando las columnas codificadas al DataFrame original:\")\n",
    "    df_expanded = pd.concat([df, ohe_df], axis=1)\n",
    "    print(f\"Concatenación completada. El DataFrame ahora tiene {df_expanded.shape[1]} columnas y {df_expanded.shape[0]} filas.\")\n",
    "\n",
    "    # Paso 6: Eliminar las columnas booleanas originales del DataFrame\n",
    "    print(\"Eliminando las columnas booleanas originales del DataFrame:\")\n",
    "    df_expanded.drop(columns=columns_to_encode, inplace=True)\n",
    "    print(f\"Columnas eliminadas: {columns_to_encode}\")\n",
    "\n",
    "    print(\"Proceso de one-hot encoding finalizado exitosamente.\\n\")\n",
    "\n",
    "    return df_expanded\n",
    "\n",
    "# Extensión de características temporales (día de la semana, momento del día, etc.) y festividades\n",
    "def time_features_extension(df):\n",
    "    \"\"\"\n",
    "    Procesa las características temporales del DataFrame y agrega nuevas columnas derivadas relacionadas con el tiempo y festividades.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convertir 'auction_time' de timestamp a una fecha legible\n",
    "    df['auction_time'] = pd.to_datetime(df['auction_time'], unit='s')\n",
    "\n",
    "    # Reemplazar NaN en 'timezone_offset' por 0\n",
    "    df['timezone_offset'] = df['timezone_offset'].fillna(0)\n",
    "\n",
    "    # Ajustar la hora según el 'timezone_offset' para obtener la hora local\n",
    "    df['auction_time_local'] = df.apply(\n",
    "        lambda row: row['auction_time'] + pd.DateOffset(hours=row['timezone_offset']), axis=1\n",
    "    )\n",
    "\n",
    "    # Crear la columna 'week_day' (1 para lunes, 7 para domingo)\n",
    "    df['week_day'] = df['auction_time_local'].dt.weekday + 1\n",
    "\n",
    "    # Crear la columna 'moment_of_the_day' (1 para temprano, 2 para tarde, 3 para noche)\n",
    "    df['moment_of_the_day'] = pd.cut(df['auction_time_local'].dt.hour, \n",
    "                                     bins=[0, 12, 18, 24], labels=[1, 2, 3], include_lowest=True, right=False)\n",
    "\n",
    "    # Eliminar las columnas originales 'auction_time', 'timezone_offset' y 'auction_time_local'\n",
    "    df.drop(columns=['auction_time', 'timezone_offset', 'auction_time_local'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Agrupación de edades en rangos numéricos\n",
    "def age_group(df, columna_edad):\n",
    "    \"\"\"\n",
    "    Agrupa las edades en rangos numéricos utilizando Cython para mejorar el rendimiento.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene la columna de edades.\n",
    "    - columna_edad (str): Nombre de la columna que contiene las edades.\n",
    "\n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame con la nueva columna 'age_group' que representa el rango de edad.\n",
    "    \"\"\"\n",
    "    # Convertir la columna de edad a una lista\n",
    "    edades = df[columna_edad].tolist()\n",
    "\n",
    "    # Usar la función Cythonizada para agrupar las edades\n",
    "    df['age_group'] = agrupar_edades_cython(edades)\n",
    "\n",
    "    # Eliminar la columna original de edades\n",
    "    df.drop(columns=[columna_edad], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Agrupo action_list_0 a auction_list_0\n",
    "def expand_action_list_0(df):\n",
    "    \"\"\"\n",
    "    Expande la columna 'action_list_0' en valores únicos y marca con 1 las columnas existentes o las crea si es necesario.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene la columna 'action_list_0' y otras columnas de listas ya expandidas.\n",
    "\n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame actualizado con las columnas de valores únicos de 'action_list_0'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convertir la columna 'action_list_0' y las columnas existentes a listas\n",
    "    action_list_0 = df['action_list_0'].tolist()\n",
    "    existing_columns = df.columns.tolist()\n",
    "    \n",
    "    # Inicializar la matriz actual\n",
    "    current_matrix = df.values.tolist()\n",
    "\n",
    "    # Llamar a la función Cythonizada\n",
    "    updated_matrix = expand_action_list_0_cython(action_list_0, existing_columns, current_matrix)\n",
    "\n",
    "    # Convertir la matriz actualizada de vuelta a un DataFrame\n",
    "    df_updated = pd.DataFrame(updated_matrix, columns=existing_columns)\n",
    "\n",
    "    # Eliminar la columna 'action_list_0'\n",
    "    df_updated.drop(columns=['action_list_0'], inplace=True)\n",
    "\n",
    "    return df_updated\n",
    "\n",
    "# Concateno las categorias de cada nivel\n",
    "def create_level_combination(df):\n",
    "    \"\"\"\n",
    "    Creates a new column 'level_combination' by concatenating the first three characters \n",
    "    of each 'action_categorical' level columns and removes the original level columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame containing the columns:\n",
    "      'action_categorical_0', 'action_categorical_1', 'action_categorical_2', \n",
    "      'action_categorical_3', 'action_categorical_4'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with the new 'level_combination' column and without the original level columns.\n",
    "    \"\"\"\n",
    "    level_columns = [\n",
    "        'action_categorical_0',\n",
    "        'action_categorical_1',\n",
    "        'action_categorical_2',\n",
    "        'action_categorical_3',\n",
    "        'action_categorical_4'\n",
    "    ]\n",
    "    df['level_combination'] = df[level_columns].astype(str).apply(\n",
    "        lambda x: ''.join([s[:3] for s in x]), axis=1\n",
    "    )\n",
    "    df.drop(columns=level_columns, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Heigh x Width a columna\n",
    "def hxw_column(df):\n",
    "    \"\"\"\n",
    "    Crea una nueva columna 'hxw' multiplicando 'creative_height' y 'creative_width'.\n",
    "    Si alguno de los dos tiene un NaN, 'hxw' se establece en 0.\n",
    "    Elimina las columnas originales 'creative_height' y 'creative_width'.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene las columnas 'creative_height' y 'creative_width'.\n",
    "    \n",
    "    Retorna:\n",
    "    - pd.DataFrame: DataFrame con la nueva columna 'hxw' añadida y las columnas originales eliminadas.\n",
    "    \"\"\"\n",
    "    df['hxw'] = df['creative_height'] * df['creative_width']\n",
    "    df.loc[df['creative_height'].isna() | df['creative_width'].isna(), 'hxw'] = 0\n",
    "    df.drop(columns=['creative_height', 'creative_width'], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Gender to number\n",
    "def encode_gender(df):\n",
    "    \"\"\"\n",
    "    Reemplaza los valores de la columna 'gender' de la siguiente manera:\n",
    "    'f' -> 1, 'm' -> 2, 'o' -> 0 y NaN -> -1.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene la columna 'gender'.\n",
    "    \n",
    "    Retorna:\n",
    "    - pd.DataFrame: DataFrame con la columna 'gender' codificada.\n",
    "    \"\"\"\n",
    "    df['gender'] = df['gender'].map({'f': 1, 'm': 2, 'o': 0}).fillna(-1).astype(int)\n",
    "    return df\n",
    "\n",
    "# creative_categorical_11, creative_categorical_9 y creative_categorical_10 a dos columnas\n",
    "def creatives2unique(df):\n",
    "    \"\"\"\n",
    "    Crea o actualiza dos columnas en el DataFrame, una para cada valor único en las columnas\n",
    "    'creative_categorical_11', 'creative_categorical_10', y 'creative_categorical_9'.\n",
    "    Si las columnas ya existen, actualiza los valores a 1 donde ese valor aparece en alguna\n",
    "    de las tres columnas en esa fila.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene las columnas 'creative_categorical_11',\n",
    "                         'creative_categorical_10', y 'creative_categorical_9'.\n",
    "\n",
    "    Retorna:\n",
    "    - pd.DataFrame: DataFrame con las nuevas columnas añadidas o actualizadas.\n",
    "    \"\"\"\n",
    "    unique_values = {'65dcab89', '43c867fd'}\n",
    "    columns_to_check = [\n",
    "        'creative_categorical_11',\n",
    "        'creative_categorical_10',\n",
    "        'creative_categorical_9'\n",
    "    ]\n",
    "\n",
    "    for val in unique_values:\n",
    "        if val in df.columns:\n",
    "            # Si la columna ya existe, actualizamos los valores a 1 donde corresponde\n",
    "            df[val] = df[val] | df[columns_to_check].eq(val).any(axis=1).astype(int)\n",
    "        else:\n",
    "            # Si no existe, creamos la columna con 1 donde corresponde\n",
    "            df[val] = df[columns_to_check].eq(val).any(axis=1).astype(int)\n",
    "\n",
    "    # Eliminar las columnas originales\n",
    "    df.drop(columns=columns_to_check, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Juntar todas las variables categoricas y hacer OHE\n",
    "def process_combineta(df):\n",
    "    \"\"\"\n",
    "    Procesa las columnas proporcionadas en combineta, creando un set con valores únicos,\n",
    "    y generando columnas binarias para cada uno de esos valores. Si la columna ya existe,\n",
    "    actualiza las filas con un 1 donde corresponda. Luego, elimina las columnas originales.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame original.\n",
    "    - combineta_columns (list): Lista de columnas a procesar.\n",
    "\n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame con las columnas binarias añadidas y las columnas originales eliminadas.\n",
    "    \"\"\"\n",
    "    combineta_columns = ['creative_categorical_0', 'creative_categorical_5', 'auction_categorical_0', 'auction_categorical_1', 'auction_categorical_11', 'auction_categorical_7', 'auction_categorical_8', 'auction_categorical_9', 'action_categorical_6', 'action_categorical_7', 'auction_categorical_3', 'auction_categorical_4', 'auction_categorical_5', 'auction_categorical_6', 'auction_categorical_10', 'auction_categorical_12', 'creative_categorical_1', 'creative_categorical_12', 'creative_categorical_2', 'creative_categorical_3', 'creative_categorical_4', 'creative_categorical_6', 'creative_categorical_7', 'creative_categorical_8']\n",
    "\n",
    "    # Unir todas las columnas de combineta en una sola columna de listas\n",
    "    df['combined_combineta'] = df[combineta_columns].astype(str).agg(\n",
    "        lambda x: '[' + ', '.join([f\"'{str(item).strip()}'\" for item in x if item != 'nan']) + ']', axis=1)\n",
    "\n",
    "    # Usar la función expand_list_dummies_cython para descomponer la lista y crear las columnas binarias\n",
    "    df = expand_list_dummies_cython(df, 'combined_combineta')\n",
    "    \n",
    "    df.drop(columns=combineta_columns, inplace=True, errors='raise')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Procesamiento optimizado de un DataFrame\n",
    "def process_optimized(df):\n",
    "    \"\"\"\n",
    "    Aplica una serie de transformaciones al DataFrame utilizando una función Cython optimizada.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame a procesar.\n",
    "    \n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame procesado.\n",
    "    \"\"\"\n",
    "    # Definir el número total de pasos para la barra de progreso\n",
    "    total_steps = 14\n",
    "    \n",
    "    # Inicializar la barra de progreso\n",
    "    with tqdm(total=total_steps, desc=\"Procesando DataFrame\", unit=\"paso\") as pbar:\n",
    "        \n",
    "        print(\"Comenzando el procesamiento optimizado del DataFrame.\")\n",
    "        print(\"Eliminando columnas innecesarias.\")\n",
    "        df = df.drop('action_categorical_5', axis=1)\n",
    "        df = df.drop('auction_categorical_2', axis=1)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Expansión de columnas temporales\")\n",
    "        df = time_features_extension(df)\n",
    "        pbar.update(1)\n",
    "\n",
    "        df = age_group(df, 'auction_age')\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Agrupando columnas de nivel\")\n",
    "        df = create_level_combination(df)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Modificando columnas de genero\")\n",
    "        df = encode_gender(df)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        print(\"Modificando columna de video\")\n",
    "        df['has_video'] = df['has_video'].apply(lambda x: 1 if x == True else 0)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Juntando medidas\")\n",
    "        df = hxw_column(df)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Expansión de columnas booleanas.\")\n",
    "        df = boolean_features_ohe(df)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Creando columnas de creatividad\")\n",
    "        df = creatives2unique(df)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        columns_to_expand = ['auction_list_0', 'action_list_1']\n",
    "\n",
    "        # Bucle para realizar las operaciones\n",
    "        for col in columns_to_expand:\n",
    "            print(f\"Expansión de columnas de listas para {col}.\")\n",
    "            \n",
    "            # Crear la variable 'idx_position' con la última columna antes de la expansión\n",
    "            idx_position = df.columns.get_loc(df.columns[-1])\n",
    "            \n",
    "            # Expansión de la columna\n",
    "            df = expand_list_dummies_cython(df, col)\n",
    "            pbar.update(1)\n",
    "\n",
    "            if col == 'action_list_1':\n",
    "                df = expand_list_dummies_cython(df,'action_list_2')\n",
    "                pbar.update(1)\n",
    "            \n",
    "            if col == 'auction_list_0':\n",
    "                # Bucle para recorrer las columnas que empiezan con 'AND' o 'APL'\n",
    "                if 'AND-APL' not in df.columns:\n",
    "                    df['AND-APL'] = 0  # Inicializar la columna 'AND-APL'\n",
    "\n",
    "                for column in df.columns:\n",
    "                    if column.startswith('AND') or column.startswith('APL'):\n",
    "                        # Poner un 1 en 'AND-APL' si la columna actual tiene un 1 en esa fila\n",
    "                        df['AND-APL'] = df['AND-APL'] | df[column]\n",
    "\n",
    "                # Eliminar todas las columnas que empiezan con 'AND' o 'APL' excepto la columna 'AND-APL'\n",
    "                columns_to_drop = [column for column in df.columns if (column.startswith('AND') or column.startswith('APL')) and column != 'AND-APL']\n",
    "                df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "                print(\"Complementamos con la columna 'action_list_0'\")\n",
    "                df = expand_action_list_0(df)\n",
    "                pbar.update(1)\n",
    "\n",
    "            # Crear la lista de columnas numéricas a partir de la siguiente columna después de 'idx'\n",
    "            categorical_num = df.iloc[:, idx_position:].select_dtypes(include=['number']).columns\n",
    "\n",
    "            # Eliminar columnas numéricas con menos de 1000 valores iguales a 1\n",
    "            for column in categorical_num:\n",
    "                if (df[column] == 1).sum() < 1000 and column != 'AND-APL':\n",
    "                    df.drop(column, axis=1, inplace=True)\n",
    "    \n",
    "        \n",
    "        print(\"Agrupando categorias poco frecuentes\")\n",
    "\n",
    "        categorical_str = df.select_dtypes(include=['object']).columns\n",
    "        categorical_str = categorical_str[categorical_str != 'device_id']\n",
    "        \n",
    "        categorical_num = df.select_dtypes(include=['number']).columns\n",
    "        \n",
    "        # Convertir a matriz bidimensional\n",
    "        data_matrix = df[categorical_str].values.tolist()\n",
    "        data_matrix_cython = [list(row) for row in data_matrix]\n",
    "        \n",
    "        # Llamar a la función Cythonizada\n",
    "        df_cython_data = agrupar_categorias_cython(\n",
    "            categorical_features=categorical_str.tolist(),\n",
    "            data=data_matrix_cython,\n",
    "            umbral=1000  # Umbral de frecuencia para considerar una categoría como rara\n",
    "        )\n",
    "\n",
    "        # Reasignar los datos al DataFrame\n",
    "        for i, col in enumerate(categorical_str):\n",
    "            df[col] = [row[i] for row in df_cython_data]\n",
    "\n",
    "        idx_position = df.columns.get_loc(df.columns[-1])\n",
    "        \n",
    "        df = process_combineta(df)\n",
    "\n",
    "        # Crear la lista de columnas numéricas a partir de la siguiente columna después de 'idx'\n",
    "        categorical_num = df.iloc[:, idx_position:].select_dtypes(include=['number']).columns\n",
    "\n",
    "        # Eliminar columnas numéricas con menos de 1000 valores iguales a 1\n",
    "        for column in categorical_num:\n",
    "            if (df[column] == 1).sum() < 1000:\n",
    "                df.drop(column, axis=1, inplace=True)\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Función para ajustar el tipo de datos de una columna para que Dask tome Nan como valor válido\n",
    "def adjust_dtype(dtype):\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        return 'Int64'\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return 'float64'\n",
    "    elif pd.api.types.is_bool_dtype(dtype):\n",
    "        return 'boolean'\n",
    "    else:\n",
    "        return 'object'\n",
    "    \n",
    "# Procesamiento de datos con Dask\n",
    "def process_data_with_dask(df, npartitions=10, meta_df=None):\n",
    "    \"\"\"\n",
    "    Procesa un DataFrame utilizando Dask para distribuir el trabajo en varias particiones.\n",
    "    Aplica la función process_optimized a cada partición del DataFrame.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): El DataFrame de pandas a procesar.\n",
    "    - npartitions (int): Número de particiones en las que se dividirá el DataFrame para su procesamiento.\n",
    "\n",
    "    Retorna:\n",
    "    - final_df (pd.DataFrame): El DataFrame procesado y concatenado.\n",
    "    \"\"\"\n",
    "    # Convertir el DataFrame de pandas a Dask con el número de particiones especificado\n",
    "    dask_df = dd.from_pandas(df, npartitions=npartitions)\n",
    "    \n",
    "    # Crear el meta DataFrame con tipos ajustados\n",
    "    meta = df.head(0).copy()\n",
    "    for col in meta.columns:\n",
    "        meta[col] = meta[col].astype(adjust_dtype(df[col].dtype))\n",
    "        \n",
    "    # Aplicar la función con map_partitions y especificar el meta\n",
    "    dask_df = dask_df.map_partitions(\n",
    "        lambda df_partition: process_optimized(df_partition),\n",
    "        meta=meta\n",
    "    )\n",
    "\n",
    "    # Ejecutar el cálculo distribuido y convertir el resultado a pandas\n",
    "    final_df = dask_df.compute()\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# Función para calcular la estadística de Cramér's V\n",
    "def cramers_v(confusion_matrix):\n",
    "    \"\"\"\n",
    "    Calcula la estadística de Cramér's V para medir la asociación entre dos variables categóricas.\n",
    "\n",
    "    Parameters:\n",
    "    - confusion_matrix: Matriz de confusión (tabla de contingencia) entre dos variables.\n",
    "\n",
    "    Returns:\n",
    "    - Cramér's V: Valor entre 0 y 1 que indica la fuerza de la asociación.\n",
    "    \"\"\"\n",
    "    # Calcular el estadístico chi-cuadrado\n",
    "    chi2 = chi2_contingency(confusion_matrix, correction=False)[0]\n",
    "    # Número total de observaciones\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    # Obtener el número de filas y columnas de la matriz de confusión\n",
    "    r, k = confusion_matrix.shape\n",
    "    if min(r, k) == 1:\n",
    "        return np.nan  # Evitar dividir por cero\n",
    "    # Calcular Cramér's V\n",
    "    return np.sqrt(chi2 / (n * (min(r, k) - 1)))\n",
    "\n",
    "def handle_none(value):\n",
    "    return 'None' if value is None else str(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_data_combined.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('train_data_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8456028, 52)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_data_combined, _ = train_test_split(\n",
    "    train_data, \n",
    "    train_size=1000000, \n",
    "    random_state=random_state, \n",
    "    stratify=train_data['Label']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb0ab7765814a78883d51e94b74649a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando DataFrame:   0%|          | 0/14 [00:00<?, ?paso/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando el procesamiento optimizado del DataFrame.\n",
      "Eliminando columnas innecesarias.\n",
      "Expansión de columnas temporales\n",
      "Agrupando columnas de nivel\n",
      "Modificando columnas de genero\n",
      "Modificando columna de video\n",
      "Juntando medidas\n",
      "Expansión de columnas booleanas.\n",
      "Inicio del proceso de one-hot encoding para las columnas booleanas especificadas.\n",
      "Recopilando valores únicos de las columnas a codificar:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39ecde9a72c4b40a05f4253bb74b7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando columnas para valores únicos:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores únicos encontrados: ['43c867fd', '47980dda', '65dcab89', '79ceee49']\n",
      "Convirtiendo las columnas booleanas a listas de listas para Cython:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa21c0342ee4cb5bcbc5d484a27c239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Convertir columnas a listas:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversión completada.\n",
      "Realizando one-hot encoding utilizando la función optimizada en Cython:\n",
      "One-hot encoding completado.\n",
      "Creando el DataFrame de columnas codificadas:\n",
      "DataFrame de one-hot encoding creado con 4 columnas y 1000000 filas.\n",
      "Concatenando las columnas codificadas al DataFrame original:\n",
      "Concatenación completada. El DataFrame ahora tiene 49 columnas y 1000000 filas.\n",
      "Eliminando las columnas booleanas originales del DataFrame:\n",
      "Columnas eliminadas: ['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2']\n",
      "Proceso de one-hot encoding finalizado exitosamente.\n",
      "\n",
      "Creando columnas de creatividad\n",
      "Expansión de columnas de listas para auction_list_0.\n",
      "Comenzando la expansión de la columna: 'auction_list_0'\n",
      "Reemplazando NaN en la columna 'auction_list_0' por listas vacías.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33335ce872f4418aa7223d433933c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codificación completada. 274 categorías únicas encontradas.\n",
      "Concatenando las columnas binarias al DataFrame original.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f004a9e5c5b946b4a3202359fba5975b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Concatenando columnas binarias:   0%|          | 0/274 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminando la columna original 'auction_list_0' del DataFrame.\n",
      "Expansión de la columna 'auction_list_0' completada exitosamente.\n",
      "\n",
      "Complementamos con la columna 'action_list_0'\n",
      "Expansión de columnas de listas para action_list_1.\n",
      "Comenzando la expansión de la columna: 'action_list_1'\n",
      "Reemplazando NaN en la columna 'action_list_1' por listas vacías.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91fd388426f40dd9a205b1243921794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codificación completada. 58 categorías únicas encontradas.\n",
      "Concatenando las columnas binarias al DataFrame original.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db38ee04afd444e998ccd0e1e3f1df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Concatenando columnas binarias:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminando la columna original 'action_list_1' del DataFrame.\n",
      "Expansión de la columna 'action_list_1' completada exitosamente.\n",
      "\n",
      "Comenzando la expansión de la columna: 'action_list_2'\n",
      "Reemplazando NaN en la columna 'action_list_2' por listas vacías.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6066adf08775489f9a99f430a53aa502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codificación completada. 146 categorías únicas encontradas.\n",
      "Concatenando las columnas binarias al DataFrame original.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243eb43407164b7b9aea830f89943b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Concatenando columnas binarias:   0%|          | 0/146 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminando la columna original 'action_list_2' del DataFrame.\n",
      "Expansión de la columna 'action_list_2' completada exitosamente.\n",
      "\n",
      "Agrupando categorias poco frecuentes\n",
      "Comenzando la expansión de la columna: 'combined_combineta'\n",
      "Reemplazando NaN en la columna 'combined_combineta' por listas vacías.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7ba121c26e4288824e4453b3c09b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = process_optimized(train_data_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar las columnas numéricas\n",
    "numeric_columns = train_data_combined.select_dtypes(include=['number'])\n",
    "\n",
    "# Verificar que 'Label' esté en las columnas numéricas\n",
    "if 'Label' not in numeric_columns.columns:\n",
    "    train_data_combined['Label'] = train_data_combined['Label'].astype(float)\n",
    "    numeric_columns = train_data_combined.select_dtypes(include=['number'])\n",
    "\n",
    "# Análisis de correlación con 'Label'\n",
    "correlation_with_label = numeric_columns.corr()['Label'].sort_values(key=abs, ascending=False)\n",
    "\n",
    "# Imprimir correlaciones con 'Label'\n",
    "print(\"Correlation of features with 'Label' (sorted by absolute value):\")\n",
    "for feature, corr in correlation_with_label.items():\n",
    "    if feature != 'Label':\n",
    "        print(f\"{feature}: {corr:.4f}\")\n",
    "\n",
    "# Análisis de datos faltantes\n",
    "missing_data = train_data_combined.isnull().sum() / len(train_data_combined) * 100\n",
    "missing_data = missing_data.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nPercentage of Missing Data by Feature:\")\n",
    "for feature, percentage in missing_data.items():\n",
    "    if percentage > 0:  # Solo imprimir características con datos faltantes\n",
    "        print(f\"{feature}: {percentage:.2f}%\")\n",
    "\n",
    "# Visualizar las mayores correlaciones con 'Label'\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_correlations = correlation_with_label.drop('Label').abs().nlargest(15)\n",
    "sns.barplot(x=top_correlations.values, y=top_correlations.index)\n",
    "plt.title('Top 15 Features Correlated with Label')\n",
    "plt.xlabel('Absolute Correlation')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el número de valores faltantes por columna\n",
    "missing_values = train_data_combined.isnull().sum()\n",
    "\n",
    "# Calcular el porcentaje de valores faltantes\n",
    "missing_percent = (missing_values / len(train_data_combined)) * 100\n",
    "\n",
    "# Crear un DataFrame con los resultados\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "\n",
    "# Ordenar el DataFrame por el número de valores faltantes en orden descendente\n",
    "missing_df = missing_df.sort_values(by='Missing Values', ascending=False)\n",
    "\n",
    "# Seleccionar las top 15 columnas con más valores faltantes\n",
    "top15_missing = missing_df.head(15)\n",
    "\n",
    "# Seleccionar las 15 columnas con más valores faltantes\n",
    "top15_cols = top15_missing.index.tolist()\n",
    "\n",
    "# Crear el gráfico de barras\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x=top15_missing['Percentage'], y=top15_missing.index, palette='viridis')\n",
    "\n",
    "# Añadir títulos y etiquetas\n",
    "plt.title('Porcentaje de Valores Faltantes en las Top 15 Columnas', fontsize=16)\n",
    "plt.xlabel('Porcentaje de Valores Faltantes (%)', fontsize=14)\n",
    "plt.ylabel('Características', fontsize=14)\n",
    "\n",
    "# Añadir etiquetas de porcentaje en las barras\n",
    "for index, value in enumerate(top15_missing['Percentage']):\n",
    "    plt.text(value + 0.5, index, f\"{value:.2f}%\", va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('missing_values.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC-AUCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data.drop(columns='Label'),  # Características\n",
    "    train_data['Label'],               # Variable objetivo\n",
    "    test_size=0.2,                             # 10% para validación\n",
    "    stratify=train_data['Label'],      # Estratificación basada en la variable objetivo\n",
    "    random_state=random_state                   # Semilla para reproducibilidad\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar columnas categóricas y numéricas en train_data\n",
    "categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numeric_features = X_train.select_dtypes(include=['number']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesador común para imputación y codificación\n",
    "preprocessor_ohe = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Imputar valores numéricos con la media\n",
    "        ('num', SimpleImputer(strategy='mean'), numeric_features),\n",
    "        \n",
    "        # Imputar valores categóricos con 'Desconocido' y aplicar One-Hot Encoding\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='Desconocido')),\n",
    "            ('onehot', OneHotEncoder(drop='first', sparse_output=True, handle_unknown='ignore'))\n",
    "        ]), categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Ajustar el preprocesador en el conjunto de entrenamiento y transformar ambos conjuntos\n",
    "X_train_ohe = preprocessor_ohe.fit_transform(X_train)\n",
    "X_val_ohe = preprocessor_ohe.transform(X_val)\n",
    "\n",
    "# Obtener los nombres de las columnas después de la transformación\n",
    "ohe_columns = preprocessor_ohe.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "processed_columns_ohe = numeric_features + list(ohe_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el preprocesador con Target Encoding e imputación\n",
    "encoder_te = ce.TargetEncoder(cols=categorical_features)\n",
    "\n",
    "# Ajustar y transformar el conjunto de entrenamiento y prueba\n",
    "X_train_te = encoder_te.fit_transform(X_train, y_train)\n",
    "X_val_te = encoder_te.transform(X_val)\n",
    "\n",
    "# Escalar las características numéricas\n",
    "scaler_te = StandardScaler()\n",
    "X_train_te[numeric_features] = scaler_te.fit_transform(X_train_te[numeric_features])\n",
    "X_val_te[numeric_features] = scaler_te.transform(X_val_te[numeric_features])\n",
    "\n",
    "del X_train, X_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el espacio de búsqueda para XGBoost con OHE\n",
    "space_xgb_ohe = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 10)), \n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]), \n",
    "    'gamma': hp.uniform('gamma', 0, 0.5),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0.5, 2),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 1),  # L1 regularización\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 1),  # L2 regularización\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1),  # Muestreo a nivel de split\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.5, 1),  # Muestreo por nodo\n",
    "    'grow_policy': hp.choice('grow_policy', ['depthwise', 'lossguide']),  # Estrategia de crecimiento\n",
    "    'tree_method': hp.choice('tree_method', ['auto', 'approx', 'hist'])  # Métodos de construcción del árbol\n",
    "}\n",
    "\n",
    "# Definir el espacio de búsqueda para XGBoost con TE\n",
    "space_xgb_te = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 10)), \n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]), \n",
    "    'gamma': hp.uniform('gamma', 0, 0.5),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0.5, 2),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 1),  # L1 regularización\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 1),  # L2 regularización\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1),  # Muestreo a nivel de split\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.5, 1),  # Muestreo por nodo\n",
    "    'grow_policy': hp.choice('grow_policy', ['depthwise', 'lossguide']),  # Estrategia de crecimiento\n",
    "    'tree_method': hp.choice('tree_method', ['auto', 'approx', 'hist'])  # Métodos de construcción del árbol\n",
    "}\n",
    "\n",
    "# Definir el espacio de búsqueda para Random Forest con TE\n",
    "space_rf_te = {\n",
    "    'n_estimators': hp.choice('n_estimators_rf_te', range(100, 1000)),\n",
    "    'max_depth': hp.choice('max_depth_rf_te', range(3, 20)),\n",
    "    'min_samples_split': hp.uniform('min_samples_split_rf_te', 0.1, 1.0),\n",
    "    'min_samples_leaf': hp.uniform('min_samples_leaf_rf_te', 0.1, 0.5),\n",
    "    'bootstrap': hp.choice('bootstrap_rf_te', [True, False]),\n",
    "    'max_features': hp.choice('max_features_rf_te', ['sqrt', 'log2'])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función objetivo para XGBoost con OHE\n",
    "def objective_xgb_ohe(params):\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['min_child_weight'] = int(params['min_child_weight'])\n",
    "    \n",
    "    model = XGBClassifier(\n",
    "        max_depth=params['max_depth'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        n_estimators=params['n_estimators'],\n",
    "        gamma=params['gamma'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        scale_pos_weight=params['scale_pos_weight'],\n",
    "        reg_alpha=params['reg_alpha'],  # L1 regularización\n",
    "        reg_lambda=params['reg_lambda'],  # L2 regularización\n",
    "        colsample_bylevel=params['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "        colsample_bynode=params['colsample_bynode'],  # Muestreo por nodo\n",
    "        grow_policy=params['grow_policy'],  # Estrategia de crecimiento\n",
    "        tree_method=params['tree_method'],  # Método de construcción del árbolobjective='binary:logistic',\n",
    "        random_state=random_state,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='auc',\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train_ohe, \n",
    "        y_train, \n",
    "        eval_set=[(X_val_ohe, y_val)],\n",
    "        early_stopping_rounds=10,\n",
    "        verbose=False\n",
    "    )\n",
    "    preds = model.predict_proba(X_val_ohe)[:,1]\n",
    "    auc = roc_auc_score(y_val, preds)\n",
    "    return {'loss': -auc, 'status': STATUS_OK}\n",
    "\n",
    "# Definir la función objetivo para XGBoost con TE\n",
    "def objective_xgb_te(params):\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['min_child_weight'] = int(params['min_child_weight'])\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        max_depth=params['max_depth'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        n_estimators=params['n_estimators'],\n",
    "        gamma=params['gamma'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        scale_pos_weight=params['scale_pos_weight'],\n",
    "        reg_alpha=params['reg_alpha'],  # L1 regularización\n",
    "        reg_lambda=params['reg_lambda'],  # L2 regularización\n",
    "        colsample_bylevel=params['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "        colsample_bynode=params['colsample_bynode'],  # Muestreo por nodo\n",
    "        grow_policy=params['grow_policy'],  # Estrategia de crecimiento\n",
    "        tree_method=params['tree_method'],  # Método de construcción del árbolobjective='binary:logistic',\n",
    "        random_state=random_state,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='auc',\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train_te, \n",
    "        y_train, \n",
    "        eval_set=[(X_val_te, y_val)],\n",
    "        early_stopping_rounds=10,\n",
    "        verbose=False\n",
    "    )\n",
    "    preds = model.predict_proba(X_val_te)[:,1]\n",
    "    auc = roc_auc_score(y_val, preds)\n",
    "    return {'loss': -auc, 'status': STATUS_OK}\n",
    "\n",
    "# Definir la función objetivo para Random Forest con TE\n",
    "def objective_rf_te(params):\n",
    "    model = RandomForestClassifier(\n",
    "        random_state=random_state,\n",
    "        **params\n",
    "    )\n",
    "    model.fit(\n",
    "        X_train_te, \n",
    "        y_train\n",
    "    )\n",
    "    preds = model.predict_proba(X_val_te)[:,1]\n",
    "    auc = roc_auc_score(y_val, preds)\n",
    "    return {'loss': -auc, 'status': STATUS_OK}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar Trials para cada modelo y encoding\n",
    "trials_xgb_ohe = Trials()\n",
    "trials_rf_ohe = Trials()\n",
    "trials_xgb_te = Trials()\n",
    "trials_rf_te = Trials()\n",
    "\n",
    "# Optimización Hyperopt para XGBoost con OHE\n",
    "best_xgb_ohe = fmin(\n",
    "    fn=objective_xgb_ohe,\n",
    "    space=space_xgb_ohe,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=5,\n",
    "    trials=trials_xgb_ohe,\n",
    "    rstate=np.random.default_rng(random_state)\n",
    ")\n",
    "\n",
    "# Optimización Hyperopt para XGBoost con TE\n",
    "best_xgb_te = fmin(\n",
    "    fn=objective_xgb_te,\n",
    "    space=space_xgb_te,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=5,\n",
    "    trials=trials_xgb_te,\n",
    "    rstate=np.random.default_rng(random_state)\n",
    ")\n",
    "\n",
    "# Optimización Hyperopt para Random Forest con TE\n",
    "best_rf_te = fmin(\n",
    "    fn=objective_rf_te,\n",
    "    space=space_rf_te,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=5,\n",
    "    trials=trials_rf_te,\n",
    "    rstate=np.random.default_rng(random_state)\n",
    ")\n",
    "\n",
    "print(\"Mejores hiperparámetros XGBoost OHE:\", best_xgb_ohe)\n",
    "print(\"Mejores hiperparámetros XGBoost TE:\", best_xgb_te)\n",
    "print(\"Mejores hiperparámetros Random Forest TE:\", best_rf_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar los hiperparámetros obtenidos para XGBoost con OHE\n",
    "tree_method_options = ['auto', 'exact', 'approx', 'hist']\n",
    "grow_policy_options = ['depthwise', 'lossguide']\n",
    "n_estimators_options = [100, 200, 300, 400, 500]\n",
    "\n",
    "# Entrenar el modelo final XGBoost con OHE\n",
    "final_model_xgb_ohe = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    random_state=random_state,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='auc',\n",
    "    n_estimators=n_estimators_options[best_xgb_ohe['n_estimators']],\n",
    "    max_depth=best_xgb_ohe['max_depth'],\n",
    "    learning_rate=best_xgb_ohe['learning_rate'],\n",
    "    subsample=best_xgb_ohe['subsample'],\n",
    "    colsample_bytree=best_xgb_ohe['colsample_bytree'],\n",
    "    min_child_weight=best_xgb_ohe['min_child_weight'],\n",
    "    gamma=best_xgb_ohe['gamma'],\n",
    "    scale_pos_weight=best_xgb_ohe['scale_pos_weight'],\n",
    "    reg_alpha=best_xgb_ohe['reg_alpha'],  # L1 regularización\n",
    "    reg_lambda=best_xgb_ohe['reg_lambda'],  # L2 regularización\n",
    "    colsample_bylevel=best_xgb_ohe['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "    colsample_bynode=best_xgb_ohe['colsample_bynode'],  # Muestreo por nodo\n",
    "    grow_policy=grow_policy_options[best_xgb_ohe['grow_policy']],  # Estrategia de crecimiento\n",
    "    tree_method=tree_method_options[best_xgb_ohe['tree_method']],  # Método de construcción del árbol\n",
    ")\n",
    "\n",
    "final_model_xgb_ohe.fit(\n",
    "    X_train_ohe, \n",
    "    y_train, \n",
    "    eval_set=[(X_val_ohe, y_val)],\n",
    "    early_stopping_rounds=10,\n",
    "    verbose=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar los hiperparámetros obtenidos para XGBoost con TE\n",
    "tree_method_options = ['auto', 'exact', 'approx', 'hist']\n",
    "grow_policy_options = ['depthwise', 'lossguide']\n",
    "n_estimators_options = [100, 200, 300, 400, 500]\n",
    "\n",
    "# Entrenar el modelo final XGBoost con TE\n",
    "final_model_xgb_te = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    random_state=random_state,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='auc',\n",
    "    n_estimators=n_estimators_options[best_xgb_ohe['n_estimators']],\n",
    "    max_depth=best_xgb_ohe['max_depth'],\n",
    "    learning_rate=best_xgb_ohe['learning_rate'],\n",
    "    subsample=best_xgb_ohe['subsample'],\n",
    "    colsample_bytree=best_xgb_ohe['colsample_bytree'],\n",
    "    min_child_weight=best_xgb_ohe['min_child_weight'],\n",
    "    gamma=best_xgb_ohe['gamma'],\n",
    "    scale_pos_weight=best_xgb_ohe['scale_pos_weight'],\n",
    "    reg_alpha=best_xgb_ohe['reg_alpha'],  # L1 regularización\n",
    "    reg_lambda=best_xgb_ohe['reg_lambda'],  # L2 regularización\n",
    "    colsample_bylevel=best_xgb_ohe['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "    colsample_bynode=best_xgb_ohe['colsample_bynode'],  # Muestreo por nodo\n",
    "    grow_policy=grow_policy_options[best_xgb_ohe['grow_policy']],  # Estrategia de crecimiento\n",
    "    tree_method=tree_method_options[best_xgb_ohe['tree_method']],  # Método de construcción del árbol\n",
    ")\n",
    "\n",
    "final_model_xgb_te.fit(\n",
    "    X_train_te, \n",
    "    y_train, \n",
    "    eval_set=[(X_val_te, y_val)],\n",
    "    early_stopping_rounds=10,\n",
    "    verbose=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar los hiperparámetros obtenidos para Random Forest con TE\n",
    "best_params_rf_te = {\n",
    "    'n_estimators': best_rf_te['n_estimators_rf_te'] + 100,\n",
    "    'max_depth': best_rf_te['max_depth_rf_te'] + 3,\n",
    "    'min_samples_split': best_rf_te['min_samples_split_rf_te'],\n",
    "    'min_samples_leaf': best_rf_te['min_samples_leaf_rf_te'],\n",
    "    'bootstrap': [True, False][best_rf_te['bootstrap_rf_te']],\n",
    "    'max_features': ['sqrt', 'log2'][best_rf_te['max_features_rf_te']]\n",
    "}\n",
    "\n",
    "# Entrenar el modelo final Random Forest con TE\n",
    "final_model_rf_te = RandomForestClassifier(\n",
    "    random_state=random_state,\n",
    "    **best_params_rf_te\n",
    ")\n",
    "\n",
    "final_model_rf_te.fit(\n",
    "    X_train_te, \n",
    "    y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular las predicciones y ROC AUC para XGBoost con OHE\n",
    "preds_xgb_ohe = final_model_xgb_ohe.predict_proba(X_val_ohe)[:,1]\n",
    "fpr_xgb_ohe, tpr_xgb_ohe, _ = roc_curve(y_val, preds_xgb_ohe)\n",
    "auc_xgb_ohe = roc_auc_score(y_val, preds_xgb_ohe)\n",
    "\n",
    "# Calcular las predicciones y ROC AUC para XGBoost con TE\n",
    "preds_xgb_te = final_model_xgb_te.predict_proba(X_val_te)[:,1]\n",
    "fpr_xgb_te, tpr_xgb_te, _ = roc_curve(y_val, preds_xgb_te)\n",
    "auc_xgb_te = roc_auc_score(y_val, preds_xgb_te)\n",
    "\n",
    "# Calcular las predicciones y ROC AUC para Random Forest con TE\n",
    "preds_rf_te = final_model_rf_te.predict_proba(X_val_te)[:,1]\n",
    "fpr_rf_te, tpr_rf_te, _ = roc_curve(y_val, preds_rf_te)\n",
    "auc_rf_te = roc_auc_score(y_val, preds_rf_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcional: Guardar el gráfico\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# XGBoost con One-Hot Encoding\n",
    "plt.plot(fpr_xgb_ohe, tpr_xgb_ohe, label=f'XGBoost OHE (AUC = {auc_xgb_ohe:.2f})')\n",
    "\n",
    "# XGBoost con Target Encoding\n",
    "plt.plot(fpr_xgb_te, tpr_xgb_te, label=f'XGBoost TE (AUC = {auc_xgb_te:.2f})')\n",
    "\n",
    "# Random Forest con Target Encoding\n",
    "plt.plot(fpr_rf_te, tpr_rf_te, label=f'Random Forest TE (AUC = {auc_rf_te:.2f})')\n",
    "\n",
    "# Línea diagonal\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "\n",
    "# Configuración del gráfico\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Curvas ROC AUC de Modelos')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.savefig('roc_auc_curves.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar las características (X) y el label (y)\n",
    "X = train_data.drop('Label', axis=1)\n",
    "y = train_data['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las columnas categóricas y numéricas\n",
    "categorical_features_to_encode = X.select_dtypes(include=['object']).columns\n",
    "numeric_features = X.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Preprocesador común para imputación y codificación\n",
    "common_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Imputar valores numéricos con la mediana\n",
    "        ('num', SimpleImputer(strategy='median'), numeric_features),\n",
    "        \n",
    "        # Imputar valores categóricos con 'Desconocido' y aplicar One-Hot Encoding\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='Desconocido')),\n",
    "            ('onehot', OneHotEncoder(drop='first', sparse_output=True, handle_unknown='ignore'))\n",
    "        ]), categorical_features_to_encode)\n",
    "    ],\n",
    "    remainder='drop'  # Excluir columnas no especificadas\n",
    ")\n",
    "\n",
    "# Definir el modelo XGBoost\n",
    "model_xgb = XGBClassifier(\n",
    "    random_state=random_state, \n",
    "    use_label_encoder=False, \n",
    "    eval_metric='auc'\n",
    ")\n",
    "\n",
    "# Crear el pipeline con RFECV y XGBoost\n",
    "# Utilizamos StratifiedKFold para asegurar que se mantengan las proporciones de la clase\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Pipeline para XGBoost con RFECV\n",
    "rfe_xgb = Pipeline([\n",
    "    ('preprocessor', common_preprocessor),\n",
    "    ('selector', RFECV(\n",
    "        estimator=model_xgb,\n",
    "        step=1,\n",
    "        cv=cv,\n",
    "        scoring='roc_auc',\n",
    "        verbose=0,  # Desactiva la salida estándar de RFECV\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Ajustar el modelo con RFECV\n",
    "print(\"Fitting XGBoost RFECV...\")\n",
    "rfe_xgb.named_steps['selector'].fit(X, y)\n",
    "\n",
    "# Obtener el número óptimo de características\n",
    "print('Optimal number of features (XGBoost model): {}'.format(rfe_xgb.named_steps['selector'].n_features_))\n",
    "\n",
    "# Graficar la selección de características\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(rfe_xgb.named_steps['selector'].cv_results_['mean_test_score']) + 1),\n",
    "         rfe_xgb.named_steps['selector'].cv_results_['mean_test_score'])\n",
    "plt.title('Feature Number Selection (XGBoost)')\n",
    "plt.xlabel(\"Number of Features Selected\")\n",
    "plt.ylabel(\"CV Score (ROC AUC)\")\n",
    "plt.show()\n",
    "\n",
    "# Obtener los nombres de las características después del preprocesamiento\n",
    "feature_names = (numeric_features +\n",
    "                 rfe_xgb.named_steps['preprocessor']\n",
    "                 .named_transformers_['cat']\n",
    "                 .named_steps['onehot']\n",
    "                 .get_feature_names_out(categorical_features_to_encode).tolist())\n",
    "\n",
    "# Imprimir las características seleccionadas\n",
    "print(\"\\nFeatures selected by XGBoost:\")\n",
    "selected_features_xgb = [feature_names[i] for i, selected in enumerate(rfe_xgb.named_steps['selector'].support_) if selected]\n",
    "print(selected_features_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y validación con estratificación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data.drop(columns='Label'),  # Características\n",
    "    train_data['Label'],               # Variable objetivo\n",
    "    test_size=0.2,                     # 20% para validación\n",
    "    stratify=train_data['Label'],      # Estratificación basada en la variable objetivo\n",
    "    random_state=random_state          # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "# Asegurarse de que X_train y X_val sean dataframes de pandas\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_val = pd.DataFrame(X_val)\n",
    "\n",
    "# Asegurarse de que y_train y y_val sean series de pandas\n",
    "y_train = pd.Series(y_train)\n",
    "y_val = pd.Series(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar las columnas categóricas y numéricas\n",
    "categorical_columns = X_train.select_dtypes(include=['object']).columns\n",
    "numeric_columns = X_train.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Crear un preprocesador con imputación y One-Hot Encoding\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Imputar valores numéricos faltantes con la media\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),  # Imputar NaNs con la media\n",
    "        ]), numeric_columns),\n",
    "        \n",
    "        # Imputar valores categóricos faltantes con 'Desconocido' y aplicar One-Hot Encoding\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='Desconocido')),  # Imputar NaNs con 'Desconocido'\n",
    "            ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))  # One-Hot Encoding\n",
    "        ]), categorical_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Entrenamiento del modelo XGBoost\n",
    "model_xgb = XGBClassifier(random_state=random_state)\n",
    "pipeline_xgb = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', model_xgb)\n",
    "])\n",
    "pipeline_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los nombres de las características después del One-Hot Encoding\n",
    "encoded_feature_names = pipeline_xgb.named_steps['preprocessor'].transformers_[1][1]['onehot'].get_feature_names_out(categorical_columns)\n",
    "all_feature_names = np.concatenate([numeric_columns, encoded_feature_names])\n",
    "\n",
    "# Crear un DataFrame para las importancias de XGBoost\n",
    "feature_importances_df = pd.DataFrame({\n",
    "    'Feature': all_feature_names,\n",
    "    'XGBoostImportance': pipeline_xgb.named_steps['classifier'].feature_importances_\n",
    "})\n",
    "\n",
    "# Normalizar las importancias\n",
    "feature_importances_df['XGBoostImportanceNorm'] = feature_importances_df['XGBoostImportance'] / feature_importances_df['XGBoostImportance'].sum()\n",
    "\n",
    "# Ordenar por la importancia normalizada\n",
    "feature_importances_df = feature_importances_df.sort_values(by='XGBoostImportanceNorm', ascending=False)\n",
    "\n",
    "# Mostrar las 20 características más importantes\n",
    "print(feature_importances_df.head(20))\n",
    "\n",
    "# Visualización de la comparación con gráfico de barras (top 20 características)\n",
    "plt.figure(figsize=(15, 10))\n",
    "top_20 = feature_importances_df.head(20)\n",
    "indices = np.arange(len(top_20))\n",
    "\n",
    "plt.bar(indices, top_20['XGBoostImportanceNorm'], alpha=0.7)\n",
    "\n",
    "plt.xlabel('Características')\n",
    "plt.ylabel('Importancia Normalizada')\n",
    "plt.title('Top 20 Características según XGBoost')\n",
    "plt.xticks(indices, top_20['Feature'], rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Definir un umbral para la importancia promedio\n",
    "importance_threshold = 0.003  # Ajusta este valor según tus necesidades\n",
    "\n",
    "# Identificar características a mantener\n",
    "features_to_keep = feature_importances_df[feature_importances_df['XGBoostImportanceNorm'] >= importance_threshold]['Feature'].tolist()\n",
    "\n",
    "print(f\"Número de características a mantener: {len(features_to_keep)}\")\n",
    "print(\"Características a eliminar:\")\n",
    "print(set(all_feature_names) - set(features_to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of top features\n",
    "top_features = feature_importances_df['Feature'].head(10).tolist()\n",
    "available_features = [f for f in top_features if f in X_train.columns]\n",
    "X_train[available_features].hist(figsize=(15, 10), bins=50)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prepare data for RFECV\n",
    "X_train_preprocessed = pipeline_xgb.named_steps['preprocessor'].transform(X_train)\n",
    "\n",
    "# Check the shape of the preprocessed data\n",
    "print(f\"Shape of preprocessed data: {X_train_preprocessed.shape}\")\n",
    "\n",
    "# If X_train_preprocessed is a 2D array with only one column, we need to reshape it\n",
    "if X_train_preprocessed.ndim == 2 and X_train_preprocessed.shape[1] == 1:\n",
    "    X_train_preprocessed = X_train_preprocessed.reshape(-1)\n",
    "\n",
    "# Now check if it's a 1D array\n",
    "if X_train_preprocessed.ndim == 1:\n",
    "    X_train_preprocessed = pd.DataFrame(X_train_preprocessed, columns=['preprocessed_feature'])\n",
    "    \n",
    "    # Recursive Feature Elimination with Random Forest\n",
    "    rfe_selector_rf = RFECV(estimator=RandomForestClassifier(random_state=random_state), step=1, cv=5, scoring='roc_auc')\n",
    "    rfe_selector_rf = rfe_selector_rf.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "    # Recursive Feature Elimination with XGBoost\n",
    "    rfe_selector_xgb = RFECV(estimator=XGBClassifier(random_state=random_state), step=1, cv=5, scoring='roc_auc')\n",
    "    rfe_selector_xgb = rfe_selector_xgb.fit(X_train_preprocessed, y_train)\n",
    "\n",
    "    print('Optimal number of features (Random Forest model): {}'.format(rfe_selector_rf.n_features_))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(rfe_selector_rf.grid_scores_) + 1), rfe_selector_rf.grid_scores_)\n",
    "    plt.title('Feature Number Selection (Random Forest)')\n",
    "    plt.xlabel(\"Number of Features Selected\")\n",
    "    plt.ylabel(\"CV Score (ROC AUC)\")\n",
    "    plt.show()\n",
    "\n",
    "    print('Optimal number of features (XGBoost model): {}'.format(rfe_selector_xgb.n_features_))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(rfe_selector_xgb.grid_scores_) + 1), rfe_selector_xgb.grid_scores_)\n",
    "    plt.title('Feature Number Selection (XGBoost)')\n",
    "    plt.xlabel(\"Number of Features Selected\")\n",
    "    plt.ylabel(\"CV Score (ROC AUC)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Preprocessed data is not in the expected format. Please check the preprocessor output.\")\n",
    "    print(f\"Data type: {type(X_train_preprocessed)}\")\n",
    "    print(f\"Shape: {X_train_preprocessed.shape}\")\n",
    "    print(f\"Sample of data: {X_train_preprocessed[:5]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDVI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
