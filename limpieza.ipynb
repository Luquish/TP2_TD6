{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 43992294\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.impute import SimpleImputer\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "import scipy.stats as ss\n",
    "import os\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import ast\n",
    "\n",
    "import warnings\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Cython imports\n",
    "from tools import agrupar_categorias_cython, custom_one_hot_encoder_cython, boolean_features_ohe_cython, agrupar_edades_cython, expand_action_list_0_cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "\n",
    "# Agrega filas que contienen categorías desconocidas o raras (poca frecuencia) al conjunto de entrenamiento\n",
    "def augment_train_data(main_train_df, supplementary_df, umbral_raras=100):\n",
    "    \"\"\"\n",
    "    Agrega filas del dataset suplementario al conjunto de entrenamiento principal\n",
    "    basándose en categorías desconocidas y raras, evitando la duplicación de filas.\n",
    "\n",
    "    Parámetros:\n",
    "    - main_train_df (pd.DataFrame): DataFrame principal de entrenamiento.\n",
    "    - supplementary_df (pd.DataFrame): DataFrame suplementario del cual se extraerán las filas.\n",
    "    - umbral_raras (int): Umbral de frecuencia para considerar una categoría como rara.\n",
    "\n",
    "    Retorna:\n",
    "    - main_train_df (pd.DataFrame): DataFrame de entrenamiento actualizado.\n",
    "    - categorias_desconocidas (dict): Diccionario actualizado de categorías desconocidas.\n",
    "    - categorias_raras (dict): Diccionario actualizado de categorías raras.\n",
    "    \"\"\"\n",
    "    # Definir columnas que no deseas tratar como categóricas\n",
    "    columns_to_exclude = ['auction_list_0', 'action_list_1', 'action_list_2', 'auction_time']\n",
    "\n",
    "    # Identificar columnas categóricas excluyendo las especificadas\n",
    "    categorical_features = main_train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    categorical_features = [col for col in categorical_features if col not in columns_to_exclude]\n",
    "    \n",
    "    # Crear un diccionario para almacenar las categorías desconocidas por columna\n",
    "    categorias_desconocidas = {}\n",
    "    \n",
    "    # Iterar a través de cada columna categórica para identificar categorías desconocidas\n",
    "    for columna in categorical_features:\n",
    "        # Obtener las categorías únicas en el conjunto de entrenamiento\n",
    "        categorias_train = set(main_train_df[columna].dropna().unique())\n",
    "        \n",
    "        # Obtener las categorías únicas en el dataset suplementario\n",
    "        categorias_suplementario = set(supplementary_df[columna].dropna().unique())\n",
    "        \n",
    "        # Identificar las categorías en el dataset suplementario que no están en el entrenamiento\n",
    "        desconocidas = categorias_suplementario - categorias_train\n",
    "        \n",
    "        # Almacenar las categorías desconocidas en el diccionario como una lista\n",
    "        categorias_desconocidas[columna] = list(desconocidas)\n",
    "    \n",
    "    # Inicializar el diccionario para almacenar las categorías raras por columna\n",
    "    categorias_raras = {}\n",
    "    \n",
    "    # Identificar categorías raras en el conjunto de entrenamiento\n",
    "    for columna in categorical_features:\n",
    "        # Contar la frecuencia de cada categoría\n",
    "        frecuencia = main_train_df[columna].value_counts()\n",
    "        \n",
    "        # Identificar categorías que aparecen menos de umbral_raras veces\n",
    "        raras = frecuencia[frecuencia < umbral_raras].index.tolist()\n",
    "        \n",
    "        # Almacenar en el diccionario\n",
    "        categorias_raras[columna] = raras\n",
    "    \n",
    "    # Crear una máscara booleana para filas con categorías desconocidas o raras\n",
    "    mask_desconocidas = pd.Series([False] * len(supplementary_df))\n",
    "    mask_raras = pd.Series([False] * len(supplementary_df))\n",
    "    \n",
    "    for columna in categorical_features:\n",
    "        # Actualizar la máscara para categorías desconocidas\n",
    "        if categorias_desconocidas[columna]:\n",
    "            mask_desconocidas = mask_desconocidas | supplementary_df[columna].isin(categorias_desconocidas[columna])\n",
    "        \n",
    "        # Actualizar la máscara para categorías raras\n",
    "        if categorias_raras[columna]:\n",
    "            mask_raras = mask_raras | supplementary_df[columna].isin(categorias_raras[columna])\n",
    "    \n",
    "    # Combinar ambas máscaras\n",
    "    mask_total = mask_desconocidas | mask_raras\n",
    "    \n",
    "    # Filtrar filas únicas a agregar\n",
    "    filas_a_agregar = supplementary_df[mask_total].drop_duplicates()\n",
    "    \n",
    "    # Mostrar información de agregación\n",
    "    total_agregadas = len(filas_a_agregar)\n",
    "    print(f\"\\nAgregando {total_agregadas} filas del dataset suplementario basadas en categorías desconocidas o raras.\")\n",
    "    \n",
    "    # Agregar las filas al conjunto de entrenamiento\n",
    "    main_train_df = pd.concat([main_train_df, filas_a_agregar], ignore_index=True)\n",
    "    \n",
    "    # Actualizar los diccionarios eliminando las categorías que ya han sido agregadas\n",
    "    for columna in categorical_features:\n",
    "        # Actualizar categorías desconocidas\n",
    "        if categorias_desconocidas[columna]:\n",
    "            categorias_agregadas = filas_a_agregar[columna].unique().tolist()\n",
    "            categorias_desconocidas[columna] = [cat for cat in categorias_desconocidas[columna] if cat not in categorias_agregadas]\n",
    "        \n",
    "        # Actualizar categorías raras\n",
    "        if categorias_raras[columna]:\n",
    "            # Recontar la frecuencia después de agregar\n",
    "            frecuencia = main_train_df[columna].value_counts()\n",
    "            nuevas_raras = frecuencia[frecuencia < umbral_raras].index.tolist()\n",
    "            categorias_raras[columna] = nuevas_raras\n",
    "    \n",
    "    return main_train_df, categorias_desconocidas, categorias_raras\n",
    "\n",
    "# Agrega como columnas binarias las listas de la columna 'auction_list_0', 'action_list_1' y 'action_list_2'\n",
    "def expand_list_dummies_cython(df, column, delimiter='|', prefix=None, suffix=None):\n",
    "    \"\"\"\n",
    "    Expande una columna que contiene listas en múltiples columnas binarias usando un one-hot encoder optimizado con Cython.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame de pandas.\n",
    "    - column (str): Nombre de la columna a expandir.\n",
    "    - delimiter (str): Delimitador a usar en get_dummies (por defecto '|').\n",
    "    - prefix (str, optional): Prefijo para las nuevas columnas binarias.\n",
    "    - suffix (str, optional): Sufijo para las nuevas columnas binarias.\n",
    "    \n",
    "    Returns:\n",
    "    - df_expanded (pd.DataFrame): DataFrame con las nuevas columnas binarias añadidas y la columna original eliminada.\n",
    "    \"\"\"\n",
    "    print(f\"Comenzando la expansión de la columna: '{column}'\")\n",
    "    \n",
    "    # Copiar el DataFrame para evitar modificar el original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Reemplazar NaN por listas vacías\n",
    "    print(f\"Reemplazando NaN en la columna '{column}' por listas vacías.\")\n",
    "    df[column] = df[column].fillna('[]')\n",
    "    \n",
    "    # Definir la función de parsing con impresión de errores\n",
    "    def parse_list(x):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(x)\n",
    "            if isinstance(parsed, list):\n",
    "                # Convertir todos los elementos a strings\n",
    "                return [str(item) for item in parsed]\n",
    "            else:\n",
    "                # Si no es una lista, tratar como un solo elemento\n",
    "                return [str(x)]\n",
    "        except (ValueError, SyntaxError):\n",
    "            # En caso de error al parsear, retornar una lista vacía\n",
    "            return []\n",
    "    \n",
    "    # Aplicar la función de parsing con una barra de progreso\n",
    "    df[column] = df[column].progress_apply(parse_list)\n",
    "    \n",
    "    # Convertir la columna en una lista de listas para pasarla a la función Cythonizada\n",
    "    data_list = df[column].tolist()\n",
    "    \n",
    "    # Llamar a la función optimizada en Cython\n",
    "    unique_categories, binary_matrix = custom_one_hot_encoder_cython(data_list)\n",
    "    print(f\"Codificación completada. {len(unique_categories)} categorías únicas encontradas.\")\n",
    "    \n",
    "    # Crear un DataFrame binario usando la matriz devuelta por Cython\n",
    "    binary_df = pd.DataFrame(binary_matrix, index=df.index, columns=unique_categories)\n",
    "\n",
    "    if prefix:\n",
    "        binary_df = binary_df.add_prefix(f\"{prefix}_\")\n",
    "    if suffix:\n",
    "        binary_df = binary_df.add_suffix(f\"_{suffix}\")\n",
    "    \n",
    "    # Concatenar las columnas binarias al DataFrame original con una barra de progreso\n",
    "    print(\"Concatenando las columnas binarias al DataFrame original.\")\n",
    "    for col in tqdm(binary_df.columns, desc=\"Concatenando columnas binarias\"):\n",
    "        if col in df.columns:\n",
    "            df[col] = np.where((df[col] == 1) | (binary_df[col] == 1), 1, 0)\n",
    "        else:\n",
    "            df[col] = binary_df[col]\n",
    "    \n",
    "    # Eliminar la columna original ya que ha sido expandida\n",
    "    print(f\"Eliminando la columna original '{column}' del DataFrame.\")\n",
    "    df = df.drop(columns=[column])\n",
    "    \n",
    "    print(f\"Expansión de la columna '{column}' completada exitosamente.\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# One-hot encode de columnas booleanas utilizando Cython\n",
    "def boolean_features_ohe(df, columns_to_encode=['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2']):\n",
    "    \"\"\"\n",
    "    Realiza one-hot encoding en columnas booleanas especificadas utilizando una función optimizada con Cython.\n",
    "    Además, muestra el progreso del procesamiento utilizando tqdm y añade comentarios explicativos.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame original que contiene las columnas booleanas a codificar.\n",
    "    - columns_to_encode (list): Lista de nombres de columnas booleanas a codificar.\n",
    "\n",
    "    Retorna:\n",
    "    - df_expanded (pd.DataFrame): DataFrame con las nuevas columnas codificadas añadidas y las columnas booleanas originales eliminadas.\n",
    "    \"\"\"\n",
    "    # Copiar el DataFrame para evitar modificar el original\n",
    "    df = df.copy()\n",
    "    print(\"Inicio del proceso de one-hot encoding para las columnas booleanas especificadas.\")\n",
    "\n",
    "    # Paso 1: Encontrar todos los valores únicos en las columnas a codificar\n",
    "    unique_values_set = set()\n",
    "    print(\"Recopilando valores únicos de las columnas a codificar:\")\n",
    "    for col in tqdm(columns_to_encode, desc=\"Procesando columnas para valores únicos\"):\n",
    "        unique_vals_col = df[col].dropna().unique()\n",
    "        unique_values_set.update(unique_vals_col)\n",
    "    unique_values = sorted(unique_values_set)\n",
    "    print(f\"Valores únicos encontrados: {unique_values}\")\n",
    "\n",
    "    # Paso 2: Convertir las columnas a listas de listas para ser procesadas en Cython\n",
    "    list_data = []\n",
    "    print(\"Convirtiendo las columnas booleanas a listas de listas para Cython:\")\n",
    "    for col in tqdm(columns_to_encode, desc=\"Convertir columnas a listas\"):\n",
    "        column_list = df[col].astype(str).tolist()  # Mantener los valores como strings\n",
    "        list_data.append(column_list)\n",
    "    print(\"Conversión completada.\")\n",
    "\n",
    "    # Paso 3: Procesar los datos con la función optimizada en Cython\n",
    "    print(\"Realizando one-hot encoding utilizando la función optimizada en Cython:\")\n",
    "    ohe_result = boolean_features_ohe_cython(list_data, unique_values)\n",
    "    print(\"One-hot encoding completado.\")\n",
    "\n",
    "    # Paso 4: Convertir el resultado de Cython a un DataFrame, alineando el índice con df\n",
    "    print(\"Creando el DataFrame de columnas codificadas:\")\n",
    "    ohe_df = pd.DataFrame(ohe_result, columns=unique_values, index=df.index)\n",
    "    print(f\"DataFrame de one-hot encoding creado con {len(ohe_df.columns)} columnas y {ohe_df.shape[0]} filas.\")\n",
    "\n",
    "    # Paso 5: Concatenar las nuevas columnas codificadas al DataFrame original\n",
    "    print(\"Concatenando las columnas codificadas al DataFrame original:\")\n",
    "    df_expanded = pd.concat([df, ohe_df], axis=1)\n",
    "    print(f\"Concatenación completada. El DataFrame ahora tiene {df_expanded.shape[1]} columnas y {df_expanded.shape[0]} filas.\")\n",
    "\n",
    "    # Paso 6: Eliminar las columnas booleanas originales del DataFrame\n",
    "    print(\"Eliminando las columnas booleanas originales del DataFrame:\")\n",
    "    df_expanded.drop(columns=columns_to_encode, inplace=True)\n",
    "    print(f\"Columnas eliminadas: {columns_to_encode}\")\n",
    "\n",
    "    print(\"Proceso de one-hot encoding finalizado exitosamente.\\n\")\n",
    "\n",
    "    return df_expanded\n",
    "\n",
    "# Extensión de características temporales (día de la semana, momento del día, etc.) y festividades\n",
    "def time_features_extension(df):\n",
    "    \"\"\"\n",
    "    Procesa las características temporales del DataFrame y agrega nuevas columnas derivadas relacionadas con el tiempo y festividades.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convertir 'auction_time' de timestamp a una fecha legible\n",
    "    df['auction_time'] = pd.to_datetime(df['auction_time'], unit='s')\n",
    "\n",
    "    # Reemplazar NaN en 'timezone_offset' por 0\n",
    "    df['timezone_offset'] = df['timezone_offset'].fillna(0)\n",
    "\n",
    "    # Ajustar la hora según el 'timezone_offset' para obtener la hora local\n",
    "    df['auction_time_local'] = df.apply(\n",
    "        lambda row: row['auction_time'] + pd.DateOffset(hours=row['timezone_offset']), axis=1\n",
    "    )\n",
    "\n",
    "    # Crear la columna 'week_day' (1 para lunes, 7 para domingo)\n",
    "    df['week_day'] = df['auction_time_local'].dt.weekday + 1\n",
    "\n",
    "    # Crear la columna 'moment_of_the_day' (1 para temprano, 2 para tarde, 3 para noche)\n",
    "    df['moment_of_the_day'] = pd.cut(df['auction_time_local'].dt.hour, \n",
    "                                     bins=[0, 12, 18, 24], labels=[1, 2, 3], include_lowest=True, right=False)\n",
    "\n",
    "    # Eliminar las columnas originales 'auction_time', 'timezone_offset' y 'auction_time_local'\n",
    "    df.drop(columns=['auction_time', 'timezone_offset', 'auction_time_local'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Agrupación de edades en rangos numéricos\n",
    "def age_group(df, columna_edad):\n",
    "    \"\"\"\n",
    "    Agrupa las edades en rangos numéricos utilizando Cython para mejorar el rendimiento.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene la columna de edades.\n",
    "    - columna_edad (str): Nombre de la columna que contiene las edades.\n",
    "\n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame con la nueva columna 'age_group' que representa el rango de edad.\n",
    "    \"\"\"\n",
    "    # Convertir la columna de edad a una lista\n",
    "    edades = df[columna_edad].tolist()\n",
    "\n",
    "    # Usar la función Cythonizada para agrupar las edades\n",
    "    df['age_group'] = agrupar_edades_cython(edades)\n",
    "\n",
    "    # Eliminar la columna original de edades\n",
    "    df.drop(columns=[columna_edad], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Agrupo action_list_0 a auction_list_0\n",
    "def expand_action_list_0(df):\n",
    "    \"\"\"\n",
    "    Expande la columna 'action_list_0' en valores únicos y marca con 1 las columnas existentes o las crea si es necesario.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene la columna 'action_list_0' y otras columnas de listas ya expandidas.\n",
    "\n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame actualizado con las columnas de valores únicos de 'action_list_0'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convertir la columna 'action_list_0' y las columnas existentes a listas\n",
    "    action_list_0 = df['action_list_0'].tolist()\n",
    "    existing_columns = df.columns.tolist()\n",
    "    \n",
    "    # Inicializar la matriz actual\n",
    "    current_matrix = df.values.tolist()\n",
    "\n",
    "    # Llamar a la función Cythonizada\n",
    "    updated_matrix = expand_action_list_0_cython(action_list_0, existing_columns, current_matrix)\n",
    "\n",
    "    # Convertir la matriz actualizada de vuelta a un DataFrame\n",
    "    df_updated = pd.DataFrame(updated_matrix, columns=existing_columns)\n",
    "\n",
    "    # Eliminar la columna 'action_list_0'\n",
    "    df_updated.drop(columns=['action_list_0'], inplace=True)\n",
    "\n",
    "    return df_updated\n",
    "\n",
    "# Concateno las categorias de cada nivel\n",
    "def create_level_combination(df):\n",
    "    \"\"\"\n",
    "    Creates a new column 'level_combination' by concatenating the first three characters \n",
    "    of each 'action_categorical' level columns and removes the original level columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame containing the columns:\n",
    "      'action_categorical_0', 'action_categorical_1', 'action_categorical_2', \n",
    "      'action_categorical_3', 'action_categorical_4'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with the new 'level_combination' column and without the original level columns.\n",
    "    \"\"\"\n",
    "    level_columns = [\n",
    "        'action_categorical_0',\n",
    "        'action_categorical_1',\n",
    "        'action_categorical_2',\n",
    "        'action_categorical_3',\n",
    "        'action_categorical_4'\n",
    "    ]\n",
    "    df['level_combination'] = df[level_columns].astype(str).apply(\n",
    "        lambda x: ''.join([s[:3] for s in x]), axis=1\n",
    "    )\n",
    "    df.drop(columns=level_columns, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Heigh x Width a columna\n",
    "def hxw_column(df):\n",
    "    \"\"\"\n",
    "    Crea una nueva columna 'hxw' multiplicando 'creative_height' y 'creative_width'.\n",
    "    Si alguno de los dos tiene un NaN, 'hxw' se establece en 0.\n",
    "    Elimina las columnas originales 'creative_height' y 'creative_width'.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene las columnas 'creative_height' y 'creative_width'.\n",
    "    \n",
    "    Retorna:\n",
    "    - pd.DataFrame: DataFrame con la nueva columna 'hxw' añadida y las columnas originales eliminadas.\n",
    "    \"\"\"\n",
    "    df['hxw'] = df['creative_height'] * df['creative_width']\n",
    "    df.loc[df['creative_height'].isna() | df['creative_width'].isna(), 'hxw'] = 0\n",
    "    df.drop(columns=['creative_height', 'creative_width'], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Gender to number\n",
    "def encode_gender(df):\n",
    "    \"\"\"\n",
    "    Reemplaza los valores de la columna 'gender' de la siguiente manera:\n",
    "    'f' -> 1, 'm' -> 2, 'o' -> 0 y NaN -> -1.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene la columna 'gender'.\n",
    "    \n",
    "    Retorna:\n",
    "    - pd.DataFrame: DataFrame con la columna 'gender' codificada.\n",
    "    \"\"\"\n",
    "    df['gender'] = df['gender'].map({'f': 1, 'm': 2, 'o': 0}).fillna(-1).astype(int)\n",
    "    return df\n",
    "\n",
    "# creative_categorical_11, creative_categorical_9 y creative_categorical_10 a dos columnas\n",
    "def creatives2unique(df):\n",
    "    \"\"\"\n",
    "    Crea o actualiza dos columnas en el DataFrame, una para cada valor único en las columnas\n",
    "    'creative_categorical_11', 'creative_categorical_10', y 'creative_categorical_9'.\n",
    "    Si las columnas ya existen, actualiza los valores a 1 donde ese valor aparece en alguna\n",
    "    de las tres columnas en esa fila.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene las columnas 'creative_categorical_11',\n",
    "                         'creative_categorical_10', y 'creative_categorical_9'.\n",
    "\n",
    "    Retorna:\n",
    "    - pd.DataFrame: DataFrame con las nuevas columnas añadidas o actualizadas.\n",
    "    \"\"\"\n",
    "    unique_values = {'65dcab89', '43c867fd'}\n",
    "    columns_to_check = [\n",
    "        'creative_categorical_11',\n",
    "        'creative_categorical_10',\n",
    "        'creative_categorical_9'\n",
    "    ]\n",
    "\n",
    "    for val in unique_values:\n",
    "        if val in df.columns:\n",
    "            # Si la columna ya existe, actualizamos los valores a 1 donde corresponde\n",
    "            df[val] = df[val] | df[columns_to_check].eq(val).any(axis=1).astype(int)\n",
    "        else:\n",
    "            # Si no existe, creamos la columna con 1 donde corresponde\n",
    "            df[val] = df[columns_to_check].eq(val).any(axis=1).astype(int)\n",
    "\n",
    "    # Eliminar las columnas originales\n",
    "    df.drop(columns=columns_to_check, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Juntar todas las variables categoricas y hacer OHE\n",
    "def process_combineta(df):\n",
    "    \"\"\"\n",
    "    Procesa las columnas proporcionadas en combineta, creando un set con valores únicos,\n",
    "    y generando columnas binarias para cada uno de esos valores. Si la columna ya existe,\n",
    "    actualiza las filas con un 1 donde corresponda. Luego, elimina las columnas originales.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame original.\n",
    "    - combineta_columns (list): Lista de columnas a procesar.\n",
    "\n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame con las columnas binarias añadidas y las columnas originales eliminadas.\n",
    "    \"\"\"\n",
    "    combineta_columns = ['creative_categorical_0', 'creative_categorical_5', 'auction_categorical_0', 'auction_categorical_1', 'auction_categorical_11', 'auction_categorical_7', 'auction_categorical_8', 'auction_categorical_9', 'action_categorical_6', 'action_categorical_7', 'auction_categorical_3', 'auction_categorical_4', 'auction_categorical_5', 'auction_categorical_6', 'auction_categorical_10', 'auction_categorical_12', 'creative_categorical_1', 'creative_categorical_12', 'creative_categorical_2', 'creative_categorical_3', 'creative_categorical_4', 'creative_categorical_6', 'creative_categorical_7', 'creative_categorical_8', 'device_id', 'device_id_type', 'level_combination']\n",
    "\n",
    "    # Unir todas las columnas de combineta en una sola columna de listas\n",
    "    df['combined_combineta'] = df[combineta_columns].astype(str).agg(\n",
    "        lambda x: '[' + ', '.join([f\"'{str(item).strip()}'\" for item in x if item != 'nan']) + ']', axis=1)\n",
    "\n",
    "    # Usar la función expand_list_dummies_cython para descomponer la lista y crear las columnas binarias\n",
    "    df = expand_list_dummies_cython(df, 'combined_combineta')\n",
    "    \n",
    "    df.drop(columns=combineta_columns, inplace=True, errors='raise')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Procesamiento optimizado de un DataFrame\n",
    "def process_optimized(df):\n",
    "    \"\"\"\n",
    "    Aplica una serie de transformaciones al DataFrame utilizando una función Cython optimizada.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame a procesar.\n",
    "    \n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame procesado.\n",
    "    \"\"\"\n",
    "    # Definir el número total de pasos para la barra de progreso\n",
    "    total_steps = 14\n",
    "    \n",
    "    # Inicializar la barra de progreso\n",
    "    with tqdm(total=total_steps, desc=\"Procesando DataFrame\", unit=\"paso\") as pbar:\n",
    "        \n",
    "        print(\"Comenzando el procesamiento optimizado del DataFrame.\")\n",
    "        print(\"Eliminando columnas innecesarias.\")\n",
    "        df = df.drop('action_categorical_5', axis=1)\n",
    "        df = df.drop('auction_categorical_2', axis=1)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Expansión de columnas temporales\")\n",
    "        df = time_features_extension(df)\n",
    "        pbar.update(1)\n",
    "\n",
    "        df = age_group(df, 'auction_age')\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Agrupando columnas de nivel\")\n",
    "        df = create_level_combination(df)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Modificando columnas de genero\")\n",
    "        df = encode_gender(df)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        print(\"Modificando columna de video\")\n",
    "        df['has_video'] = df['has_video'].apply(lambda x: 1 if x == True else 0)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Juntando medidas\")\n",
    "        df = hxw_column(df)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Expansión de columnas booleanas.\")\n",
    "        df = boolean_features_ohe(df)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Creando columnas de creatividad\")\n",
    "        df = creatives2unique(df)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        columns_to_expand = ['auction_list_0', 'action_list_1']\n",
    "\n",
    "        # Bucle para realizar las operaciones\n",
    "        for col in columns_to_expand:\n",
    "            print(f\"Expansión de columnas de listas para {col}.\")\n",
    "            \n",
    "            # Crear la variable 'idx_position' con la última columna antes de la expansión\n",
    "            idx_position = df.columns.get_loc(df.columns[-1])\n",
    "            \n",
    "            # Expansión de la columna\n",
    "            df = expand_list_dummies_cython(df, col)\n",
    "            pbar.update(1)\n",
    "\n",
    "            if col == 'action_list_1':\n",
    "                df = expand_list_dummies_cython(df,'action_list_2')\n",
    "                pbar.update(1)\n",
    "            \n",
    "            if col == 'auction_list_0':\n",
    "                # Bucle para recorrer las columnas que empiezan con 'AND' o 'APL'\n",
    "                if 'AND-APL' not in df.columns:\n",
    "                    df['AND-APL'] = 0  # Inicializar la columna 'AND-APL'\n",
    "\n",
    "                for column in df.columns:\n",
    "                    if column.startswith('AND') or column.startswith('APL'):\n",
    "                        # Poner un 1 en 'AND-APL' si la columna actual tiene un 1 en esa fila\n",
    "                        df['AND-APL'] = df['AND-APL'] | df[column]\n",
    "\n",
    "                # Eliminar todas las columnas que empiezan con 'AND' o 'APL' excepto la columna 'AND-APL'\n",
    "                columns_to_drop = [column for column in df.columns if (column.startswith('AND') or column.startswith('APL')) and column != 'AND-APL']\n",
    "                df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "                print(\"Complementamos con la columna 'action_list_0'\")\n",
    "                df = expand_action_list_0(df)\n",
    "                pbar.update(1)\n",
    "\n",
    "            # Crear la lista de columnas numéricas a partir de la siguiente columna después de 'idx'\n",
    "            categorical_num = df.iloc[:, idx_position:].select_dtypes(include=['number']).columns\n",
    "\n",
    "            # Eliminar columnas numéricas con menos de 1000 valores iguales a 1\n",
    "            for column in categorical_num:\n",
    "                if (df[column] == 1).sum() < 1000 and column != 'AND-APL':\n",
    "                    df.drop(column, axis=1, inplace=True)\n",
    "    \n",
    "        \n",
    "        print(\"Agrupando categorias poco frecuentes\")\n",
    "\n",
    "        categorical_str = df.select_dtypes(include=['object']).columns\n",
    "        categorical_str = categorical_str[categorical_str != 'device_id']\n",
    "        \n",
    "        categorical_num = df.select_dtypes(include=['number']).columns\n",
    "        \n",
    "        # Convertir a matriz bidimensional\n",
    "        data_matrix = df[categorical_str].values.tolist()\n",
    "        data_matrix_cython = [list(row) for row in data_matrix]\n",
    "        \n",
    "        # Llamar a la función Cythonizada\n",
    "        df_cython_data = agrupar_categorias_cython(\n",
    "            categorical_features=categorical_str.tolist(),\n",
    "            data=data_matrix_cython,\n",
    "            umbral=1000  # Umbral de frecuencia para considerar una categoría como rara\n",
    "        )\n",
    "\n",
    "        # Reasignar los datos al DataFrame\n",
    "        for i, col in enumerate(categorical_str):\n",
    "            df[col] = [row[i] for row in df_cython_data]\n",
    "\n",
    "        idx_position = df.columns.get_loc(df.columns[-1])\n",
    "        \n",
    "        df = process_combineta(df)\n",
    "\n",
    "        # Crear la lista de columnas numéricas a partir de la siguiente columna después de 'idx'\n",
    "        categorical_num = df.iloc[:, idx_position:].select_dtypes(include=['number']).columns\n",
    "\n",
    "        # Eliminar columnas numéricas con menos de 1000 valores iguales a 1\n",
    "        for column in categorical_num:\n",
    "            if (df[column] == 1).sum() < 1000:\n",
    "                df.drop(column, axis=1, inplace=True)\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Función para ajustar el tipo de datos de una columna para que Dask tome Nan como valor válido\n",
    "def adjust_dtype(dtype):\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        return 'Int64'\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return 'float64'\n",
    "    elif pd.api.types.is_bool_dtype(dtype):\n",
    "        return 'boolean'\n",
    "    else:\n",
    "        return 'object'\n",
    "    \n",
    "# Procesamiento de datos con Dask\n",
    "def process_data_with_dask(df, npartitions=10, meta_df=None):\n",
    "    \"\"\"\n",
    "    Procesa un DataFrame utilizando Dask para distribuir el trabajo en varias particiones.\n",
    "    Aplica la función process_optimized a cada partición del DataFrame.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): El DataFrame de pandas a procesar.\n",
    "    - npartitions (int): Número de particiones en las que se dividirá el DataFrame para su procesamiento.\n",
    "\n",
    "    Retorna:\n",
    "    - final_df (pd.DataFrame): El DataFrame procesado y concatenado.\n",
    "    \"\"\"\n",
    "    # Convertir el DataFrame de pandas a Dask con el número de particiones especificado\n",
    "    dask_df = dd.from_pandas(df, npartitions=npartitions)\n",
    "    \n",
    "    # Crear el meta DataFrame con tipos ajustados\n",
    "    meta = df.head(0).copy()\n",
    "    for col in meta.columns:\n",
    "        meta[col] = meta[col].astype(adjust_dtype(df[col].dtype))\n",
    "        \n",
    "    # Aplicar la función con map_partitions y especificar el meta\n",
    "    dask_df = dask_df.map_partitions(\n",
    "        lambda df_partition: process_optimized(df_partition),\n",
    "        meta=meta\n",
    "    )\n",
    "\n",
    "    # Ejecutar el cálculo distribuido y convertir el resultado a pandas\n",
    "    final_df = dask_df.compute()\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# Función para calcular la estadística de Cramér's V\n",
    "def cramers_v(confusion_matrix):\n",
    "    \"\"\"\n",
    "    Calcula la estadística de Cramér's V para medir la asociación entre dos variables categóricas.\n",
    "\n",
    "    Parameters:\n",
    "    - confusion_matrix: Matriz de confusión (tabla de contingencia) entre dos variables.\n",
    "\n",
    "    Returns:\n",
    "    - Cramér's V: Valor entre 0 y 1 que indica la fuerza de la asociación.\n",
    "    \"\"\"\n",
    "    # Calcular el estadístico chi-cuadrado\n",
    "    chi2 = chi2_contingency(confusion_matrix, correction=False)[0]\n",
    "    # Número total de observaciones\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    # Obtener el número de filas y columnas de la matriz de confusión\n",
    "    r, k = confusion_matrix.shape\n",
    "    if min(r, k) == 1:\n",
    "        return np.nan  # Evitar dividir por cero\n",
    "    # Calcular Cramér's V\n",
    "    return np.sqrt(chi2 / (n * (min(r, k) - 1)))\n",
    "\n",
    "def handle_none(value):\n",
    "    return 'None' if value is None else str(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenamos datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_21 = pd.read_csv('data/ctr_21.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_20 = pd.read_csv('data/ctr_20.csv')\n",
    "train_data_19 = pd.read_csv('data/ctr_19.csv')\n",
    "train_data_18 = pd.read_csv('data/ctr_18.csv')\n",
    "train_data_17 = pd.read_csv('data/ctr_17.csv')\n",
    "train_data_16 = pd.read_csv('data/ctr_16.csv')\n",
    "train_data_15 = pd.read_csv('data/ctr_15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplementary_datasets = [\n",
    "    ('1', train_data_20),\n",
    "    ('2', train_data_19),\n",
    "    ('3', train_data_18),\n",
    "    ('4', train_data_17),\n",
    "    ('5', train_data_16),\n",
    "    ('6', train_data_15)\n",
    "]\n",
    "\n",
    "train_data = train_data_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nombre, dataset in supplementary_datasets:\n",
    "    print(f\"\\nProcesando dataset {nombre}/{len(supplementary_datasets)}\")\n",
    "    train_data, categorias_desconocidas, categorias_raras = augment_train_data(train_data, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape[0])\n",
    "\n",
    "print(train_data_20.shape[0] + train_data_19.shape[0] + train_data_18.shape[0] + train_data_17.shape[0] + train_data_16.shape[0] + train_data_15.shape[0] + train_data_21.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('train_data_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingenieria de atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_data_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/ctr_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorias numericas\n",
    "numeric_columns = train_data.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Categorias categóricas\n",
    "categorical_features = train_data.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribucion de 'Label' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir la cantidad de filas del dataset combinado\n",
    "print(f\"Cantidad de filas en el dataset combinado: {train_data.shape[0]}\")\n",
    "print(f\"Cantidad de columnas en el dataset combinado: {train_data.shape[1]}\")\n",
    "\n",
    "# Ver porcentaje de clics vs no clics en la columna Label\n",
    "label_counts = train_data['Label'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPorcentaje de clics (1) y no clics (0):\")\n",
    "print(label_counts)\n",
    "\n",
    "# Cantidad de clics (1) y no clics (0)\n",
    "label_counts_abs = train_data['Label'].value_counts()\n",
    "print(\"\\nCantidad de clics (1) y no clics (0):\")\n",
    "print(label_counts_abs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlaciones con 'Label'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis with 'Label'\n",
    "numeric_data = train_data[numeric_columns]\n",
    "correlation_with_label = numeric_data.corr()['Label'].sort_values(key=abs, ascending=False)\n",
    "\n",
    "# Print correlations with 'Label'\n",
    "print(\"Correlation of features with 'Label' (sorted by absolute value):\")\n",
    "for feature, corr in correlation_with_label.items():\n",
    "    if feature != 'Label':\n",
    "        print(f\"{feature}: {corr:.4f}\")\n",
    "\n",
    "# Visualize top correlations with 'Label'\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_correlations = correlation_with_label.drop('Label').abs()\n",
    "sns.barplot(x=top_correlations.values, y=top_correlations.index)\n",
    "plt.title('Numerical Features Correlated with Label')\n",
    "plt.xlabel('Absolute Correlation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cramer's V for each categorical feature with 'Label'\n",
    "cramer_v_results = {}\n",
    "for col in categorical_features:\n",
    "    confusion_matrix = pd.crosstab(train_data[col], train_data['Label'])\n",
    "    cramer_v_results[col] = cramers_v(confusion_matrix)\n",
    "\n",
    "# Sort results\n",
    "cramer_v_results = dict(sorted(cramer_v_results.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Visualize top Cramer's V results\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = dict(list(cramer_v_results.items())[:20])\n",
    "sns.barplot(x=list(top_features.values()), y=list(top_features.keys()))\n",
    "plt.title(\"Top 20 Categorical Features by Cramer's V with Label\")\n",
    "plt.xlabel(\"Cramer's V\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print results\n",
    "print(\"Cramer's V for categorical features with 'Label':\")\n",
    "for feature, v in top_features.items():\n",
    "    print(f\"{feature}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de datos faltantes (sin cambios)\n",
    "missing_data = train_data.isnull().sum() / len(train_data) * 100\n",
    "missing_data = missing_data[missing_data > 10].sort_values(ascending=False)  # Filtrar las columnas con más del 10% de datos faltantes\n",
    "\n",
    "# Visualizar datos faltantes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=missing_data.values, y=missing_data.index)\n",
    "plt.title('Percentage of Missing Data by Numerical Feature (>10%)')\n",
    "plt.xlabel('Percentage Missing')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"\\nPercentage of Missing Data by Feature (>10% missing):\")\n",
    "for feature, percentage in missing_data.items():\n",
    "    print(f\"{feature}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame de ejemplo\n",
    "data = {\n",
    "    'auction_boolean_0': ['47980dda', '43c867fd', None, '79ceee49', '47980dda', 'unknown'],\n",
    "    'auction_boolean_1': ['79ceee49', None, '79ceee49', None, '1', '0'],\n",
    "    'auction_boolean_2': ['65dcab89', None, '43c867fd', '65dcab89', '65dcab89', '43c867fd'],\n",
    "    'Label': [0, 1, 0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "# Crear el DataFrame\n",
    "train_data_combined = pd.DataFrame(data)\n",
    "\n",
    "# Seleccionar solo las columnas relevantes\n",
    "df = train_data_combined[['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2', 'Label']]\n",
    "\n",
    "print(\"DataFrame Original:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Convertir el DataFrame a una lista de listas\n",
    "data_list = df.values.tolist()\n",
    "\n",
    "# Definir columnas categóricas y columnas a excluir\n",
    "categorical_features = ['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2']\n",
    "columns_to_exclude = []  # No se excluye ninguna en este ejemplo\n",
    "\n",
    "# Aplicar la función de agrupación de categorías en Cython\n",
    "data_processed = agrupar_categorias_cython(categorical_features, data_list, umbral=2)\n",
    "\n",
    "# Convertir la lista de listas de nuevo a DataFrame para visualizar\n",
    "df_processed = pd.DataFrame(data_processed, columns=['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2', 'Label'])\n",
    "\n",
    "print(\"DataFrame después de Agrupar Categorías con Cython:\")\n",
    "print(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valores unicos y su frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train_data.columns:\n",
    "    if train_data[column].value_counts() < 1000:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario para almacenar los valores únicos de cada columna\n",
    "unique_values = {col: set(train_data[col].dropna().unique()) for col in categorical_features}\n",
    "\n",
    "# Diccionario para almacenar las columnas que tienen valores comunes\n",
    "common_columns = {}\n",
    "\n",
    "# Comparar las columnas entre sí para ver qué valores comparten\n",
    "for i in range(len(categorical_features)):\n",
    "    for j in range(i + 1, len(categorical_features)):\n",
    "        col1 = categorical_features[i]\n",
    "        col2 = categorical_features[j]\n",
    "        \n",
    "        # Ver los valores que se repiten entre las dos columnas\n",
    "        common_values = unique_values[col1].intersection(unique_values[col2])\n",
    "        \n",
    "        if common_values:\n",
    "            # Almacenar las columnas con valores comunes\n",
    "            if (col1, col2) not in common_columns:\n",
    "                common_columns[(col1, col2)] = common_values\n",
    "\n",
    "# Ver las columnas que tienen valores comunes y sus unique values\n",
    "for col_pair, common_vals in common_columns.items():\n",
    "    col1, col2 = col_pair\n",
    "    print(f\"\\nValores comunes entre {col1} y {col2}: {common_vals}\")\n",
    "    print(f\"Valores únicos de {col1}: {len(unique_values[col1])}\")\n",
    "    print(f\"Valores únicos de {col2}: {len(unique_values[col2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### action_categorical_5 (motivos para eliminarlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_to_check = '6bc0e29c'\n",
    "filtered_rows = test[test['action_categorical_5'] == value_to_check]\n",
    "\n",
    "# Verificar si el valor aparece en otras columnas categóricas\n",
    "for col in categorical_features:\n",
    "    if col != 'action_categorical_5':  # Evitar verificar la columna original\n",
    "        matching_rows = filtered_rows[filtered_rows[col] == value_to_check]\n",
    "        if not matching_rows.empty:\n",
    "            print(f\"El valor '{value_to_check}' también aparece en la columna '{col}'\")\n",
    "\n",
    "# Ver filas donde el valor aparece solo en action_categorical_5\n",
    "only_in_action_5 = filtered_rows[~filtered_rows[categorical_features].isin([value_to_check]).any(axis=1)]\n",
    "print(f\"Filas donde el valor '{value_to_check}' solo aparece en 'action_categorical_5':\")\n",
    "print(only_in_action_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_to_check = '79ceee49'\n",
    "filtered_rows = test[test['action_categorical_5'] == value_to_check]\n",
    "\n",
    "# Verificar si el valor aparece en otras columnas categóricas\n",
    "for col in categorical_features:\n",
    "    if col != 'action_categorical_5':  # Evitar verificar la columna original\n",
    "        matching_rows = filtered_rows[filtered_rows[col] == value_to_check]\n",
    "        if not matching_rows.empty:\n",
    "            print(f\"El valor '{value_to_check}' también aparece en la columna '{col}'\")\n",
    "\n",
    "# Ver filas donde el valor aparece solo en action_categorical_5\n",
    "only_in_action_5 = filtered_rows[~filtered_rows[categorical_features].isin([value_to_check]).any(axis=1)]\n",
    "print(f\"Filas donde el valor '{value_to_check}' solo aparece en 'action_categorical_5':\")\n",
    "print(only_in_action_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### auction_categorical_2 (motivos para eliminarlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show unique values of auction_categorical_2\n",
    "print(\"Unique values of 'auction_categorical_2':\")\n",
    "print(train_data['auction_categorical_2'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Levels features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_features = ['action_categorical_0', 'action_categorical_1', 'action_categorical_2', 'action_categorical_3', 'action_categorical_4']\n",
    "\n",
    "print(f\"Valores únicos y su frecuencia:\")\n",
    "for column in train_data.columns:\n",
    "    if column in level_features:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle para contar los NaNs en cada columna\n",
    "for column in level_features:\n",
    "    num_nans = train_data[column].isna().sum()\n",
    "    print(f\"Columna {column} tiene {num_nans} valores NaN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una columna con las combinaciones únicas de las 5 columnas pero tomando solo las dos primeras letras de cada valor\n",
    "truncated_cols = train_data[level_features].applymap(lambda x: x[:3])\n",
    "\n",
    "# Concatenar las columnas truncadas\n",
    "train_data['combination'] = truncated_cols.apply(lambda row: ''.join(row.values), axis=1)\n",
    "\n",
    "# Obtener las combinaciones únicas y sus frecuencias\n",
    "combination_counts = train_data['combination'].value_counts().reset_index()\n",
    "combination_counts.columns = ['Combination', 'Frequency']\n",
    "\n",
    "# Mostrar las primeras 10 combinaciones más comunes\n",
    "print(combination_counts.head(25))\n",
    "print(f\"De: {len(combination_counts)} combinaciones únicas.\")\n",
    "\n",
    "# Graficar las combinaciones más frecuentes\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_combinations = combination_counts.head(25)  # Mostrar las 25 combinaciones más comunes\n",
    "sns.barplot(x='Frequency', y='Combination', data=top_combinations)\n",
    "plt.title(\"Top 25 Combinations of Action Categorical Columns out of {} Unique Combinations\".format(len(combination_counts)))\n",
    "plt.xlabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinaciones menos comunes (< 10000 veces)\n",
    "print(combination_counts[combination_counts['Frequency'] <= 1])\n",
    "\n",
    "# Suma de todas las frecuencias de las combinaciones menos comunes\n",
    "print(f\"Suma de las frecuencias de las combinaciones menos comunes: {combination_counts[combination_counts['Frequency'] <= 1]['Frequency'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels_df = create_level_combination(train_data)\n",
    "\n",
    "levels_df['level_combination']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boolean Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_features = ['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2']\n",
    "\n",
    "for column in train_data.columns:\n",
    "    if column in boolean_features:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle para contar los NaNs en cada columna\n",
    "for column in boolean_features:\n",
    "    num_nans = train_data[column].isna().sum()\n",
    "    print(f\"Columna {column} tiene {num_nans} valores NaN.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una funcion para resolverl el problema de los valores booleanos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame de ejemplo\n",
    "data = {\n",
    "    'auction_boolean_0': ['47980dda', '43c867fd', None, '79ceee49'],\n",
    "    'auction_boolean_1': ['79ceee49', None, '79ceee49', None],\n",
    "    'auction_boolean_2': ['65dcab89', None, '43c867fd', '65dcab89'],\n",
    "    'Label': [0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "# Crear el DataFrame\n",
    "train_data = pd.DataFrame(data)\n",
    "\n",
    "# Imprimir el DataFrame original\n",
    "print(\"DataFrame Original:\")\n",
    "print(train_data)\n",
    "\n",
    "# Aplicar la función personalizada de one-hot encoding\n",
    "train_data_encoded = boolean_features_ohe(train_data)\n",
    "\n",
    "# Imprimir el DataFrame resultante\n",
    "print(\"\\nDataFrame después del One-Hot Encoding personalizado:\")\n",
    "print(train_data_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_features = ['auction_time', 'auction_age', 'timezone_offset']\n",
    "\n",
    "for column in train_data.columns:\n",
    "    if column in time_features:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle para contar los NaNs en cada columna\n",
    "for column in time_features:\n",
    "    num_nans = train_data[column].isna().sum()\n",
    "    print(f\"Columna {column} tiene {num_nans} valores NaN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegúrate de tener la columna 'auction_age' en tu train\n",
    "if 'auction_age' in train_data.columns:\n",
    "    # Obtener los valores únicos y su frecuencia\n",
    "    unique_ages = train_data['auction_age'].value_counts().sort_index()\n",
    "\n",
    "    # Imprimir cada edad y su frecuencia\n",
    "    for age, frequency in unique_ages.items():\n",
    "        print(f\"Edad: {age}, Frecuencia: {frequency}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funcion para crear atributos temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_features_extension\n",
    "\n",
    "data = {\n",
    "    'auction_time': [\n",
    "        1545676800,  # 24 de Diciembre de 2018, 22:00:00\n",
    "        1483228800,  # 31 de Diciembre de 2016, 23:00:00\n",
    "        1288396800,  # 30 de Octubre de 2010, 10:00:00\n",
    "        1412899200,  # 10 de Octubre de 2014, 15:00:00\n",
    "        1483574400   # 5 de Enero de 2017, 09:00:00\n",
    "    ],\n",
    "    'timezone_offset': [1, -2, 3, 0, 5]  # Diferentes zonas horarias\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = time_features_extension(df)\n",
    "\n",
    "# Imprimir el DataFrame resultante\n",
    "print(\"\\nDataFrame después de procesar el tiempo de subasta:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age_group\n",
    "\n",
    "data = {\n",
    "    'auction_age': [\n",
    "        -1, 15, 25, 35, 50, 65, 105, 80, 18, 99, 0, 579\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = age_group(df, 'auction_age')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quiero tomar las filas que sean de tipo label 1 y que me digan si esta cerca de una festividad o no con mi train_data\n",
    "\n",
    "# Crea un dataframe solo con las filas que tienen Label 1 a partir de train_data\n",
    "\n",
    "df = train_data[train_data['Label'] == 1]\n",
    "\n",
    "# Aplica la función time_features_extension al dataframe\n",
    "\n",
    "df = time_features_extension(df)\n",
    "\n",
    "# Imprimir las frecuencias de los valores que toman las columnas que se crearon con time_features_extension (week_day time_of_month moment_of_the_day  close_to_festivity)\n",
    "\n",
    "for column in ['week_day', 'time_of_month', 'moment_of_the_day']:\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos y su frecuencia:\")\n",
    "    print(df[column].value_counts())\n",
    "    print(\"-\" * 50)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica la función time_features_extension al dataframe\n",
    "\n",
    "test = time_features_extension(test)\n",
    "\n",
    "# Imprimir las frecuencias de los valores que toman las columnas que se crearon con time_features_extension (week_day time_of_month moment_of_the_day  close_to_festivity)\n",
    "\n",
    "for column in ['week_day', 'time_of_month', 'moment_of_the_day']:\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos y su frecuencia:\")\n",
    "    print(test[column].value_counts())\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_features = ['auction_list_0', 'action_list_1', 'action_list_2', 'action_list_0']\n",
    "\n",
    "for column in train_data.columns:\n",
    "    if column in list_features:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle para contar los NaNs en cada columna\n",
    "for column in list_features:\n",
    "    num_nans = train_data[column].isna().sum()\n",
    "    print(f\"Columna {column} tiene {num_nans} valores NaN.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funcion para hacer columnas de listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame de prueba más pequeño\n",
    "data = {\n",
    "    'auction_list_0': ['[\"IAB3\",\"utilities\", \"IAB22-2\"]', '[\"IAB19\",\"IAB4-5\", \"IAB8-9\"]', None, '[\"IAB3\",\"utilities\", \"IAB22-2\"]'],\n",
    "    'action_list_1': ['[-6779]', '[-6824, -6823]', None, '[-6824, -6823]' ],\n",
    "    'action_list_2': ['[6871, -6543, -6544]', '[-2560, -5902]', '[-6779]', None],\n",
    "    'action_list_0': ['IAB8-9', 'IAB22-2', 'IAB9-30', 'IAB9-30']\n",
    "}\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print('Antes de utilizar la función expand_list_dummies_cython:')\n",
    "print(df)\n",
    "\n",
    "# Aplicar la función expand_list_dummies_cython en 'auction_list_0', 'action_list_1' y 'action_list_2'\n",
    "# Aquí se asume que ya has definido y aplicado `expand_list_dummies_cython`\n",
    "df = expand_list_dummies_cython(df, 'auction_list_0')\n",
    "df = expand_list_dummies_cython(df, 'action_list_1')\n",
    "df = expand_list_dummies_cython(df, 'action_list_2')\n",
    "\n",
    "print('Antes de utilizar la función expand_action_list_0:')\n",
    "print(df)\n",
    "\n",
    "print()\n",
    "\n",
    "print('Después de utilizar la función expand_action_list_0:')\n",
    "df = expand_action_list_0(df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pixels features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels_features = ['creative_height', 'creative_width']\n",
    "\n",
    "for column in train_data.columns:\n",
    "    if column in pixels_features:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle para contar los NaNs en cada columna\n",
    "for column in pixels_features:\n",
    "    num_nans = train_data[column].isna().sum()\n",
    "    print(f\"Columna {column} tiene {num_nans} valores NaN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar las filas donde ambas columnas tienen valores NaN\n",
    "nans_both_columns = train_data[pixels_features].isna().all(axis=1).sum()\n",
    "\n",
    "print(f\"El número de filas donde ambas columnas tienen valores NaN es: {nans_both_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volver a filtrar las filas donde ambas columnas son NaN\n",
    "filtered_rows = train_data[train_data['creative_height'].isna() & train_data['creative_width'].isna()]\n",
    "\n",
    "# Calcular el porcentaje de cada valor de 'Label' en las filas donde ambas columnas tienen valores NaN\n",
    "label_counts = filtered_rows['Label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Mostrar los resultados\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar las filas donde tanto creative_height como creative_width son NaN\n",
    "subset_nan = train_data[train_data['creative_height'].isna() & train_data['creative_width'].isna()]\n",
    "\n",
    "# Recorrer todas las columnas y mostrar particularidades en las 1,304,272 filas\n",
    "for column in train_data.columns:\n",
    "    unique_values = subset_nan[column].nunique()\n",
    "    num_nans = subset_nan[column].isna().sum()\n",
    "    \n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos: {unique_values}\")\n",
    "    print(f\"Cantidad de NaNs: {num_nans}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores unicos de la variable categorica auction_categorical_2\n",
    "unique_values = train_data['auction_categorical_2'].unique()\n",
    "\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creative features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = [\n",
    "    'creative_categorical_11',\n",
    "    'creative_categorical_10',\n",
    "    'creative_categorical_9'\n",
    "]\n",
    "unique_vals = {'65dcab89', '43c867fd'}\n",
    "\n",
    "mask = train_data[columns_to_check].apply(lambda row: set(row).issubset(unique_vals) and len(set(row)) == 1, axis=1)\n",
    "\n",
    "train_data[mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No podemos eliminar las columnas pero si podemos crear dos nuevas que tomen los nombres de los dos valores unicos de estas columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auction categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_id  = [\"auction_categorical_0\", \"auction_categorical_1\", \"auction_categorical_7\", \"auction_categorical_8\", \"auction_categorical_9\", \"auction_categorical_11\"]\n",
    "\n",
    "auction_categorical_variable = [\"auction_categorical_2\", \"auction_categorical_3\", \"auction_categorical_4\", \"auction_categorical_5\", \"auction_categorical_6\", \"auction_categorical_10\", \"auction_categorical_12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train_data.columns:\n",
    "    if column in entity_id:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train_data.columns:\n",
    "    if column in auction_categorical_variable:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variables = [\n",
    "    'action_categorical_6', 'action_categorical_7', 'auction_categorical_3', 'auction_categorical_4', 'auction_categorical_5', 'auction_categorical_6', 'auction_categorical_10', 'auction_categorical_12', 'creative_categorical_1', 'creative_categorical_12', 'creative_categorical_2', 'creative_categorical_3', 'creative_categorical_4', 'creative_categorical_6', 'creative_categorical_7', 'creative_categorical_8'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train_data.columns:\n",
    "    if column in categorical_variables:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = {col: set(train_data[col].dropna().unique()) for col in categorical_variables}\n",
    "\n",
    "common_columns = {}\n",
    "\n",
    "for i in range(len(categorical_variables)):\n",
    "    for j in range(i + 1, len(categorical_variables)):\n",
    "        col1 = categorical_variables[i]\n",
    "        col2 = categorical_variables[j]\n",
    "        \n",
    "        common_values = unique_values[col1].intersection(unique_values[col2])\n",
    "        \n",
    "        if common_values:\n",
    "            if (col1, col2) not in common_columns:\n",
    "                common_columns[(col1, col2)] = common_values\n",
    "\n",
    "for col_pair, common_vals in common_columns.items():\n",
    "    col1, col2 = col_pair\n",
    "    print(f\"\\nValores comunes entre {col1} y {col2}: {common_vals}\")\n",
    "    print(f\"Valores únicos de {col1}: {len(unique_values[col1])}\")\n",
    "    print(f\"Valores únicos de {col2}: {len(unique_values[col2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entitiy IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_id = ['auction_categorical_0', 'auction_categorical_1', 'auction_categorical_11', 'auction_categorical_7', 'auction_categorical_8', 'auction_categorical_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train_data.columns:\n",
    "    if column in entity_id:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = {col: set(train_data[col].dropna().unique()) for col in entity_id}\n",
    "\n",
    "common_columns = {}\n",
    "\n",
    "for i in range(len(entity_id)):\n",
    "    for j in range(i + 1, len(entity_id)):\n",
    "        col1 = entity_id[i]\n",
    "        col2 = entity_id[j]\n",
    "        \n",
    "        common_values = unique_values[col1].intersection(unique_values[col2])\n",
    "        \n",
    "        if common_values:\n",
    "            if (col1, col2) not in common_columns:\n",
    "                common_columns[(col1, col2)] = common_values\n",
    "\n",
    "for col_pair, common_vals in common_columns.items():\n",
    "    col1, col2 = col_pair\n",
    "    print(f\"\\nValores comunes entre {col1} y {col2}: {common_vals}\")\n",
    "    print(f\"Valores únicos de {col1}: {len(unique_values[col1])}\")\n",
    "    print(f\"Valores únicos de {col2}: {len(unique_values[col2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_columns = ['creative_categorical_0', 'creative_categorical_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train_data.columns:\n",
    "    if column in business_columns:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many unquie values are between business_columns, and how many are common\n",
    "\n",
    "unique_values = {col: set(train_data[col].dropna().unique()) for col in business_columns}\n",
    "\n",
    "common_columns = {}\n",
    "\n",
    "for i in range(len(business_columns)):\n",
    "    for j in range(i + 1, len(business_columns)):\n",
    "        col1 = business_columns[i]\n",
    "        col2 = business_columns[j]\n",
    "        \n",
    "        common_values = unique_values[col1].intersection(unique_values[col2])\n",
    "        \n",
    "        if common_values:\n",
    "            if (col1, col2) not in common_columns:\n",
    "                common_columns[(col1, col2)] = common_values\n",
    "\n",
    "for col_pair, common_vals in common_columns.items():\n",
    "    col1, col2 = col_pair\n",
    "    print(f\"\\nValores comunes entre {col1} y {col2}: {common_vals}\")\n",
    "    print(f\"Valores únicos de {col1}: {len(unique_values[col1])}\")\n",
    "    print(f\"Valores únicos de {col2}: {len(unique_values[col2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_combined, _ = train_test_split(\n",
    "    train_data, \n",
    "    train_size=200000, \n",
    "    random_state=random_state, \n",
    "    stratify=train_data['Label']\n",
    ")\n",
    "\n",
    "# Imprimir la cantidad de filas del dataset combinado\n",
    "print(f\"Cantidad de filas en el dataset combinado: {train_data_combined.shape[0]}\")\n",
    "print(f\"Cantidad de columnas en el dataset combinado: {train_data_combined.shape[1]}\")\n",
    "\n",
    "# Ver porcentaje de clics vs no clics en la columna Label\n",
    "label_counts = train_data_combined['Label'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPorcentaje de clics (1) y no clics (0):\")\n",
    "print(label_counts)\n",
    "\n",
    "# Cantidad de clics (1) y no clics (0)\n",
    "label_counts_abs = train_data_combined['Label'].value_counts()\n",
    "print(\"\\nCantidad de clics (1) y no clics (0):\")\n",
    "print(label_counts_abs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y validación con estratificación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data_combined.drop(columns='Label'),  # Características\n",
    "    train_data_combined['Label'],               # Variable objetivo\n",
    "    test_size=0.01,                              # 10% para validación\n",
    "    stratify=train_data_combined['Label'],      # Estratificación basada en la variable objetivo\n",
    "    random_state=random_state                   # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "# Verificar la proporción de clases en el conjunto de entrenamiento y validación\n",
    "print(\"Proporción en el conjunto de entrenamiento:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"Proporción en el conjunto de validación:\")\n",
    "print(y_val.value_counts(normalize=True))\n",
    "\n",
    "del train_data_combined, train_data, y_val, X_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = process_optimized(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in X_train.columns:\n",
    "    # Verifica si la suma de las frecuencias de los valores únicos es menor a 1000\n",
    "    if (X_train[column] == 1).sum() < 1000:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(X_train[column].value_counts())\n",
    "        print(\"-\" * 50)  # Separador entre columnas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/ctr_test.csv')\n",
    "\n",
    "# X_test = process_data_with_dask(test_data, npartitions=10)\n",
    "\n",
    "X_test = process_optimized(test_data)\n",
    "\n",
    "# Excluir 'id' de las columnas de X_test para el reordenamiento\n",
    "common_columns = [col for col in X_test.columns if col != 'id']\n",
    "\n",
    "# Asegúrate de que las columnas en X_train coincidan con las de X_test (sin 'id')\n",
    "missing_cols = set(common_columns) - set(X_train.columns)\n",
    "for col in missing_cols:\n",
    "    X_train[col] = 0\n",
    "\n",
    "# Eliminar columnas extra en X_train que no están en X_test (sin contar 'id')\n",
    "extra_cols = set(X_train.columns) - set(common_columns)\n",
    "X_train.drop(columns=extra_cols, inplace=True)\n",
    "\n",
    "# Reordenar X_train para que tenga el mismo orden de columnas que X_test (sin 'id')\n",
    "X_train = X_train[common_columns]\n",
    "\n",
    "# Verificar el número de columnas en X_train\n",
    "print(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las columnas categóricas y numéricas\n",
    "categorical_features_to_encode = X_train.select_dtypes(include=['object']).columns\n",
    "numeric_features = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Preprocesador común para imputación y codificación\n",
    "common_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Imputar valores numéricos con la media\n",
    "        ('num', SimpleImputer(strategy='mean'), numeric_features),\n",
    "        \n",
    "        # Imputar valores categóricos con 'Desconocido' y aplicar One-Hot Encoding\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='Desconocido')),\n",
    "            ('onehot', OneHotEncoder(drop='first', sparse_output=True, handle_unknown='ignore'))\n",
    "        ]), categorical_features_to_encode)\n",
    "    ],\n",
    "    remainder='drop'  # Excluir columnas no especificadas\n",
    ")\n",
    "\n",
    "# Definir el modelo XGBoost\n",
    "model_xgb = XGBClassifier(\n",
    "    random_state=random_state, \n",
    "    use_label_encoder=False, \n",
    "    eval_metric='auc'\n",
    ")\n",
    "\n",
    "# Pipeline para XGBoost (mantiene matrices dispersas)\n",
    "pipeline_xgb = Pipeline(steps=[\n",
    "    ('preprocessor', common_preprocessor),\n",
    "    ('classifier', model_xgb),\n",
    "])\n",
    "\n",
    "model_ada = AdaBoostClassifier(\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# Pipeline para AdaBoost\n",
    "pipeline_ada = Pipeline(steps=[\n",
    "    ('preprocessor', common_preprocessor),\n",
    "    ('classifier', model_ada),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperot XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el espacio de búsqueda para los hiperparámetros de XGBoost\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 10)), \n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]), \n",
    "    'gamma': hp.uniform('gamma', 0, 0.5),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0.5, 2),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 1),  # L1 regularización\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 1),  # L2 regularización\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1),  # Muestreo a nivel de split\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.5, 1),  # Muestreo por nodo\n",
    "    'grow_policy': hp.choice('grow_policy', ['depthwise', 'lossguide']),  # Estrategia de crecimiento\n",
    "    'tree_method': hp.choice('tree_method', ['auto', 'approx', 'hist'])  # Métodos de construcción del árbol\n",
    "}\n",
    "\n",
    "# Función objetivo para Hyperopt utilizando un conjunto de validación fijo\n",
    "def objective_xgb(params):\n",
    "    \n",
    "    # Asegurar que los parámetros sean del tipo correcto\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['min_child_weight'] = int(params['min_child_weight'])\n",
    "    \n",
    "    # Definir el modelo con los hiperparámetros actuales\n",
    "    model_xgb = XGBClassifier(\n",
    "        max_depth=params['max_depth'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        n_estimators=params['n_estimators'],\n",
    "        gamma=params['gamma'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        scale_pos_weight=params['scale_pos_weight'],\n",
    "        reg_alpha=params['reg_alpha'],  # L1 regularización\n",
    "        reg_lambda=params['reg_lambda'],  # L2 regularización\n",
    "        colsample_bylevel=params['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "        colsample_bynode=params['colsample_bynode'],  # Muestreo por nodo\n",
    "        grow_policy=params['grow_policy'],  # Estrategia de crecimiento\n",
    "        tree_method=params['tree_method'],  # Método de construcción del árbol\n",
    "        use_label_encoder=False,  # Evitar advertencias en versiones más recientes de XGBoost\n",
    "        eval_metric='auc',\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Pipeline para XGBoost (mantiene matrices dispersas)\n",
    "    pipeline_xgb = Pipeline(steps=[\n",
    "        ('preprocessor', common_preprocessor),\n",
    "        ('classifier', model_xgb),\n",
    "    ])\n",
    "    \n",
    "    # Entrenar el modelo en el conjunto de entrenamiento\n",
    "    pipeline_xgb.fit(X_train, y_train)\n",
    "    \n",
    "    # Predecir las probabilidades en el conjunto de validación\n",
    "    y_pred_proba = pipeline_xgb.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Calcular el AUC en el conjunto de validación\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    # Opcional: imprimir los hiperparámetros y el AUC actual\n",
    "    print(f\"Hiperparámetros: {params}, AUC: {auc:.4f}\")\n",
    "    \n",
    "    # Retornar el valor a minimizar (1 - AUC)\n",
    "    return {'loss': 1 - auc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la optimización\n",
    "trials = Trials()\n",
    "\n",
    "best_xgb = fmin(\n",
    "    fn=objective_xgb,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=1,\n",
    "    trials=trials,\n",
    "    rstate=np.random.default_rng(random_state)  # Asegurar reproducibilidad\n",
    ")\n",
    "\n",
    "# No es necesario volver a mapear los hiperparámetros aquí, ya se hizo dentro de la función objetivo\n",
    "print(\"Mejores hiperparámetros para XGBoost:\")\n",
    "print(best_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PRUEBA CON TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices\n",
    "tree_method_options = ['auto', 'exact', 'approx', 'hist']\n",
    "grow_policy_options = ['depthwise', 'lossguide']\n",
    "n_estimators_options = [100, 200, 300, 400, 500]\n",
    "\n",
    "# Reconstruir el modelo de XGBoost con los mejores hiperparámetros\n",
    "best_model_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=n_estimators_options[best_xgb['n_estimators']],\n",
    "    max_depth=best_xgb['max_depth'],\n",
    "    learning_rate=best_xgb['learning_rate'],\n",
    "    subsample=best_xgb['subsample'],\n",
    "    colsample_bytree=best_xgb['colsample_bytree'],\n",
    "    min_child_weight=best_xgb['min_child_weight'],\n",
    "    gamma=best_xgb['gamma'],\n",
    "    scale_pos_weight=best_xgb['scale_pos_weight'],\n",
    "    reg_alpha=best_xgb['reg_alpha'],  # L1 regularización\n",
    "    reg_lambda=best_xgb['reg_lambda'],  # L2 regularización\n",
    "    colsample_bylevel=best_xgb['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "    colsample_bynode=best_xgb['colsample_bynode'],  # Muestreo por nodo\n",
    "    grow_policy=grow_policy_options[best_xgb['grow_policy']],  # Estrategia de crecimiento\n",
    "    tree_method=tree_method_options[best_xgb['tree_method']],  # Método de construcción del árbol\n",
    "    random_state=random_state,\n",
    "    use_label_encoder=False,  # Evitar advertencias en versiones más recientes de XGBoost\n",
    "    eval_metric=\"auc\"  # Establecer AUC como métrica de evaluación\n",
    ")\n",
    "\n",
    "# Crear un nuevo pipeline reutilizando el preprocesador original y el mejor modelo\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', common_preprocessor),\n",
    "    ('classifier', model_xgb),\n",
    "])\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "xgb_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir en el conjunto de testeo\n",
    "y_preds_xgb_test = xgb_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_xgb = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_xgb_test})\n",
    "submission_df_xgb[\"id\"] = submission_df_xgb[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros\n",
    "file_name_xgb = (\n",
    "    f\"xgboost_preds_ne_{best_xgb['n_estimators']}_\"\n",
    "    f\"md_{handle_none(best_xgb['max_depth'])}_\"\n",
    "    f\"lr_{round(best_xgb['learning_rate'], 2)}_\"\n",
    "    f\"ss_{round(best_xgb['subsample'], 2)}_\"\n",
    "    f\"csb_{round(best_xgb['colsample_bytree'], 2)}_\"\n",
    "    f\"cb_level_{round(best_xgb['colsample_bylevel'], 2)}_\"\n",
    "    f\"cb_node_{round(best_xgb['colsample_bynode'], 2)}_\"\n",
    "    f\"mcw_{round(best_xgb['min_child_weight'], 2)}_\"\n",
    "    f\"gamma_{round(best_xgb['gamma'], 2)}_\"\n",
    "    f\"ra_{round(best_xgb['reg_alpha'], 2)}_\"\n",
    "    f\"rl_{round(best_xgb['reg_lambda'], 2)}_\"\n",
    "    f\"spw_{round(best_xgb['scale_pos_weight'], 2)}_\"\n",
    "    f\"gp_{grow_policy_options[best_xgb['grow_policy']]}_\"\n",
    "    f\"tm_{tree_method_options[best_xgb['tree_method']]}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_xgb.to_csv(os.path.join(\"submits\", file_name_xgb), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_xgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperot AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_ada = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 50, 500, 10),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(1.0)),\n",
    "    'algorithm': hp.choice('algorithm', ['SAMME', 'SAMME.R']),\n",
    "    \n",
    "    # Hiperparámetros del estimador base (DecisionTreeClassifier)\n",
    "    'base_max_depth': hp.quniform('base_max_depth', 1, 15, 1),\n",
    "    'base_min_samples_split': hp.quniform('base_min_samples_split', 2, 20, 1),\n",
    "    'base_min_samples_leaf': hp.quniform('base_min_samples_leaf', 1, 20, 1),\n",
    "    'base_max_features': hp.choice('base_max_features', ['auto', 'sqrt', 'log2', None])\n",
    "}\n",
    "\n",
    "def objective_ada(params):\n",
    "    # Convertir hiperparámetros a enteros si es necesario\n",
    "    n_estimators = int(params['n_estimators'])\n",
    "    base_max_depth = int(params['base_max_depth'])\n",
    "    base_min_samples_split = int(params['base_min_samples_split'])\n",
    "    base_min_samples_leaf = int(params['base_min_samples_leaf'])\n",
    "    \n",
    "    # Seleccionar el algoritmo\n",
    "    algorithm = params['algorithm']\n",
    "    \n",
    "    # Manejar 'base_max_features' que puede ser None\n",
    "    base_max_features = params['base_max_features']\n",
    "    if base_max_features == 'auto':\n",
    "        base_max_features = 'sqrt'\n",
    "    elif base_max_features == 'log2':\n",
    "        base_max_features = 'log2'\n",
    "    elif base_max_features is None:\n",
    "        base_max_features = None\n",
    "    else:\n",
    "        base_max_features = base_max_features  # 'sqrt' ya está manejado\n",
    "    \n",
    "    # Definir el estimador base\n",
    "    base_estimator = DecisionTreeClassifier(\n",
    "        max_depth=base_max_depth,\n",
    "        min_samples_split=base_min_samples_split,\n",
    "        min_samples_leaf=base_min_samples_leaf,\n",
    "        max_features=base_max_features,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Definir el clasificador AdaBoost\n",
    "    clf = AdaBoostClassifier(\n",
    "        base_estimator=base_estimator,\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=params['learning_rate'],\n",
    "        algorithm=algorithm,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Definir el pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', common_preprocessor),\n",
    "        ('classifier', clf),\n",
    "    ])\n",
    "    \n",
    "    # Definir la validación cruzada\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Evaluar el modelo usando cross_val_score\n",
    "    auc = cross_val_score(pipeline, X_train, y_train, cv=skf, scoring='roc_auc').mean()\n",
    "    \n",
    "    # Opcional: imprimir los hiperparámetros y el AUC actual\n",
    "    print(f\"Hiperparámetros: {params}, AUC: {auc:.4f}\")\n",
    "    \n",
    "    # Retornar el valor a minimizar (1 - AUC)\n",
    "    return {'loss': 1 - auc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials_ada = Trials()\n",
    "\n",
    "# Ejecutar la optimización\n",
    "best_ada = fmin(\n",
    "    fn=objective_ada,\n",
    "    space=space_ada,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=50,\n",
    "    trials=trials_ada,\n",
    "    rstate=np.random.RandomState(random_state)  # Asegurar reproducibilidad\n",
    ")\n",
    "\n",
    "# Procesar los hiperparámetros seleccionados\n",
    "best_ada['n_estimators'] = int(best_ada['n_estimators'])\n",
    "best_ada['base_max_depth'] = int(best_ada['base_max_depth'])\n",
    "best_ada['base_min_samples_split'] = int(best_ada['base_min_samples_split'])\n",
    "best_ada['base_min_samples_leaf'] = int(best_ada['base_min_samples_leaf'])\n",
    "best_ada['algorithm'] = ['SAMME', 'SAMME.R'][best_ada['algorithm']]\n",
    "best_ada['base_max_features'] = ['auto', 'sqrt', 'log2', None][best_ada['base_max_features']]\n",
    "\n",
    "print(\"Mejores hiperparámetros para AdaBoost:\")\n",
    "print(best_ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruir el modelo de AdaBoost con los mejores hiperparámetros\n",
    "best_base_estimator_ada = DecisionTreeClassifier(\n",
    "    max_depth=best_ada['base_max_depth'],\n",
    "    min_samples_split=best_ada['base_min_samples_split'],\n",
    "    min_samples_leaf=best_ada['base_min_samples_leaf'],\n",
    "    max_features=best_ada['base_max_features'],\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "best_clf_ada = AdaBoostClassifier(\n",
    "    base_estimator=best_base_estimator_ada,\n",
    "    n_estimators=best_ada['n_estimators'],\n",
    "    learning_rate=best_ada['learning_rate'],\n",
    "    algorithm=best_ada['algorithm'],\n",
    "    random_state=random_state\n",
    ")\n",
    "\n",
    "# Crear un nuevo pipeline reutilizando el preprocesador original y el mejor modelo\n",
    "ada_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', common_preprocessor),\n",
    "    ('classifier', best_clf_ada),\n",
    "])\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "ada_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir en el conjunto de testeo\n",
    "y_preds_ada_test = ada_pipeline.predict(X_test)\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_ada = pd.DataFrame({\n",
    "    \"id\": test.index,  # Asegúrate de que 'test' tiene una columna 'id'; ajusta si es necesario\n",
    "    \"Label\": y_preds_ada_test\n",
    "})\n",
    "\n",
    "# Asegurar que la columna 'id' es de tipo entero\n",
    "submission_df_ada[\"id\"] = submission_df_ada[\"id\"].astype(int)\n",
    "\n",
    "# Función para manejar valores None en nombres de archivos\n",
    "def handle_none(value):\n",
    "    return 'none' if value is None else value\n",
    "\n",
    "# Opciones para los parámetros que pueden tener opciones específicas\n",
    "algorithm_options_ada = {'SAMME': 'samm', 'SAMME.R': 'sammr'}\n",
    "max_features_options_ada = {'auto': 'auto', 'sqrt': 'sqrt', 'log2': 'log2', None: 'none'}\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros\n",
    "file_name_ada = (\n",
    "    f\"adaboost_preds_ne_{best_ada['n_estimators']}_\"\n",
    "    f\"lr_{round(best_ada['learning_rate'], 4)}_\"\n",
    "    f\"alg_{algorithm_options_ada[best_ada['algorithm']]}_\"\n",
    "    f\"md_{best_ada['base_max_depth']}_\"\n",
    "    f\"mss_{best_ada['base_min_samples_split']}_\"\n",
    "    f\"msl_{best_ada['base_min_samples_leaf']}_\"\n",
    "    f\"mf_{max_features_options_ada[best_ada['base_max_features']]}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_ada.to_csv(os.path.join(\"submits\", file_name_ada), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_ada}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDVI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
