{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "random_state = 43992294\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.impute import SimpleImputer\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "import scipy.stats as ss\n",
    "import os\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import ast\n",
    "\n",
    "import warnings\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Cython imports\n",
    "from tools import agrupar_categorias_cython, custom_one_hot_encoder_cython, boolean_features_ohe_cython, agrupar_edades_cython, expand_action_list_0_cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "\n",
    "# Agrega filas que contienen categorías desconocidas o raras (poca frecuencia) al conjunto de entrenamiento\n",
    "def augment_train_data(main_train_df, supplementary_df, umbral_raras=100):\n",
    "    \"\"\"\n",
    "    Agrega filas del dataset suplementario al conjunto de entrenamiento principal\n",
    "    basándose en categorías desconocidas y raras, evitando la duplicación de filas.\n",
    "\n",
    "    Parámetros:\n",
    "    - main_train_df (pd.DataFrame): DataFrame principal de entrenamiento.\n",
    "    - supplementary_df (pd.DataFrame): DataFrame suplementario del cual se extraerán las filas.\n",
    "    - umbral_raras (int): Umbral de frecuencia para considerar una categoría como rara.\n",
    "\n",
    "    Retorna:\n",
    "    - main_train_df (pd.DataFrame): DataFrame de entrenamiento actualizado.\n",
    "    - categorias_desconocidas (dict): Diccionario actualizado de categorías desconocidas.\n",
    "    - categorias_raras (dict): Diccionario actualizado de categorías raras.\n",
    "    \"\"\"\n",
    "    # Definir columnas que no deseas tratar como categóricas\n",
    "    columns_to_exclude = ['auction_list_0', 'action_list_1', 'action_list_2', 'auction_time']\n",
    "\n",
    "    # Identificar columnas categóricas excluyendo las especificadas\n",
    "    categorical_features = main_train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    categorical_features = [col for col in categorical_features if col not in columns_to_exclude]\n",
    "    \n",
    "    # Crear un diccionario para almacenar las categorías desconocidas por columna\n",
    "    categorias_desconocidas = {}\n",
    "    \n",
    "    # Iterar a través de cada columna categórica para identificar categorías desconocidas\n",
    "    for columna in categorical_features:\n",
    "        # Obtener las categorías únicas en el conjunto de entrenamiento\n",
    "        categorias_train = set(main_train_df[columna].dropna().unique())\n",
    "        \n",
    "        # Obtener las categorías únicas en el dataset suplementario\n",
    "        categorias_suplementario = set(supplementary_df[columna].dropna().unique())\n",
    "        \n",
    "        # Identificar las categorías en el dataset suplementario que no están en el entrenamiento\n",
    "        desconocidas = categorias_suplementario - categorias_train\n",
    "        \n",
    "        # Almacenar las categorías desconocidas en el diccionario como una lista\n",
    "        categorias_desconocidas[columna] = list(desconocidas)\n",
    "    \n",
    "    # Inicializar el diccionario para almacenar las categorías raras por columna\n",
    "    categorias_raras = {}\n",
    "    \n",
    "    # Identificar categorías raras en el conjunto de entrenamiento\n",
    "    for columna in categorical_features:\n",
    "        # Contar la frecuencia de cada categoría\n",
    "        frecuencia = main_train_df[columna].value_counts()\n",
    "        \n",
    "        # Identificar categorías que aparecen menos de umbral_raras veces\n",
    "        raras = frecuencia[frecuencia < umbral_raras].index.tolist()\n",
    "        \n",
    "        # Almacenar en el diccionario\n",
    "        categorias_raras[columna] = raras\n",
    "    \n",
    "    # Crear una máscara booleana para filas con categorías desconocidas o raras\n",
    "    mask_desconocidas = pd.Series([False] * len(supplementary_df))\n",
    "    mask_raras = pd.Series([False] * len(supplementary_df))\n",
    "    \n",
    "    for columna in categorical_features:\n",
    "        # Actualizar la máscara para categorías desconocidas\n",
    "        if categorias_desconocidas[columna]:\n",
    "            mask_desconocidas = mask_desconocidas | supplementary_df[columna].isin(categorias_desconocidas[columna])\n",
    "        \n",
    "        # Actualizar la máscara para categorías raras\n",
    "        if categorias_raras[columna]:\n",
    "            mask_raras = mask_raras | supplementary_df[columna].isin(categorias_raras[columna])\n",
    "    \n",
    "    # Combinar ambas máscaras\n",
    "    mask_total = mask_desconocidas | mask_raras\n",
    "    \n",
    "    # Filtrar filas únicas a agregar\n",
    "    filas_a_agregar = supplementary_df[mask_total].drop_duplicates()\n",
    "    \n",
    "    # Mostrar información de agregación\n",
    "    total_agregadas = len(filas_a_agregar)\n",
    "    print(f\"\\nAgregando {total_agregadas} filas del dataset suplementario basadas en categorías desconocidas o raras.\")\n",
    "    \n",
    "    # Agregar las filas al conjunto de entrenamiento\n",
    "    main_train_df = pd.concat([main_train_df, filas_a_agregar], ignore_index=True)\n",
    "    \n",
    "    # Actualizar los diccionarios eliminando las categorías que ya han sido agregadas\n",
    "    for columna in categorical_features:\n",
    "        # Actualizar categorías desconocidas\n",
    "        if categorias_desconocidas[columna]:\n",
    "            categorias_agregadas = filas_a_agregar[columna].unique().tolist()\n",
    "            categorias_desconocidas[columna] = [cat for cat in categorias_desconocidas[columna] if cat not in categorias_agregadas]\n",
    "        \n",
    "        # Actualizar categorías raras\n",
    "        if categorias_raras[columna]:\n",
    "            # Recontar la frecuencia después de agregar\n",
    "            frecuencia = main_train_df[columna].value_counts()\n",
    "            nuevas_raras = frecuencia[frecuencia < umbral_raras].index.tolist()\n",
    "            categorias_raras[columna] = nuevas_raras\n",
    "    \n",
    "    return main_train_df, categorias_desconocidas, categorias_raras\n",
    "\n",
    "# Agrega como columnas binarias las listas de la columna 'auction_list_0', 'action_list_1' y 'action_list_2'\n",
    "def expand_list_dummies_cython(df, column, delimiter='|', prefix=None, suffix=None):\n",
    "    \"\"\"\n",
    "    Expande una columna que contiene listas en múltiples columnas binarias usando un one-hot encoder optimizado con Cython.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame de pandas.\n",
    "    - column (str): Nombre de la columna a expandir.\n",
    "    - delimiter (str): Delimitador a usar en get_dummies (por defecto '|').\n",
    "    - prefix (str, optional): Prefijo para las nuevas columnas binarias.\n",
    "    - suffix (str, optional): Sufijo para las nuevas columnas binarias.\n",
    "    \n",
    "    Returns:\n",
    "    - df_expanded (pd.DataFrame): DataFrame con las nuevas columnas binarias añadidas y la columna original eliminada.\n",
    "    \"\"\"\n",
    "    print(f\"Comenzando la expansión de la columna: '{column}'\")\n",
    "    \n",
    "    # Copiar el DataFrame para evitar modificar el original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Reemplazar NaN por listas vacías\n",
    "    print(f\"Reemplazando NaN en la columna '{column}' por listas vacías.\")\n",
    "    df[column] = df[column].fillna('[]')\n",
    "    \n",
    "    # Definir la función de parsing con impresión de errores\n",
    "    def parse_list(x):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(x)\n",
    "            if isinstance(parsed, list):\n",
    "                # Convertir todos los elementos a strings\n",
    "                return [str(item) for item in parsed]\n",
    "            else:\n",
    "                # Si no es una lista, tratar como un solo elemento\n",
    "                return [str(x)]\n",
    "        except (ValueError, SyntaxError):\n",
    "            # En caso de error al parsear, retornar una lista vacía\n",
    "            return []\n",
    "    \n",
    "    # Aplicar la función de parsing con una barra de progreso\n",
    "    df[column] = df[column].progress_apply(parse_list)\n",
    "    \n",
    "    # Convertir la columna en una lista de listas para pasarla a la función Cythonizada\n",
    "    data_list = df[column].tolist()\n",
    "    \n",
    "    # Llamar a la función optimizada en Cython\n",
    "    unique_categories, binary_matrix = custom_one_hot_encoder_cython(data_list)\n",
    "    print(f\"Codificación completada. {len(unique_categories)} categorías únicas encontradas.\")\n",
    "    \n",
    "    # Crear un DataFrame binario usando la matriz devuelta por Cython\n",
    "    binary_df = pd.DataFrame(binary_matrix, index=df.index, columns=unique_categories)\n",
    "\n",
    "    if prefix:\n",
    "        binary_df = binary_df.add_prefix(f\"{prefix}_\")\n",
    "    if suffix:\n",
    "        binary_df = binary_df.add_suffix(f\"_{suffix}\")\n",
    "    \n",
    "    # Concatenar las columnas binarias al DataFrame original con una barra de progreso\n",
    "    print(\"Concatenando las columnas binarias al DataFrame original.\")\n",
    "    for col in tqdm(binary_df.columns, desc=\"Concatenando columnas binarias\"):\n",
    "        if col in df.columns:\n",
    "            df[col] = np.where((df[col] == 1) | (binary_df[col] == 1), 1, 0)\n",
    "        else:\n",
    "            df[col] = binary_df[col]\n",
    "    \n",
    "    # Eliminar la columna original ya que ha sido expandida\n",
    "    print(f\"Eliminando la columna original '{column}' del DataFrame.\")\n",
    "    df = df.drop(columns=[column])\n",
    "    \n",
    "    print(f\"Expansión de la columna '{column}' completada exitosamente.\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# One-hot encode de columnas booleanas utilizando Cython\n",
    "def boolean_features_ohe(df, columns_to_encode=['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2']):\n",
    "    \"\"\"\n",
    "    Realiza one-hot encoding en columnas booleanas especificadas utilizando una función optimizada con Cython.\n",
    "    Además, muestra el progreso del procesamiento utilizando tqdm y añade comentarios explicativos.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame original que contiene las columnas booleanas a codificar.\n",
    "    - columns_to_encode (list): Lista de nombres de columnas booleanas a codificar.\n",
    "\n",
    "    Retorna:\n",
    "    - df_expanded (pd.DataFrame): DataFrame con las nuevas columnas codificadas añadidas y las columnas booleanas originales eliminadas.\n",
    "    \"\"\"\n",
    "    # Copiar el DataFrame para evitar modificar el original\n",
    "    df = df.copy()\n",
    "    print(\"Inicio del proceso de one-hot encoding para las columnas booleanas especificadas.\")\n",
    "\n",
    "    # Paso 1: Encontrar todos los valores únicos en las columnas a codificar\n",
    "    unique_values_set = set()\n",
    "    print(\"Recopilando valores únicos de las columnas a codificar:\")\n",
    "    for col in tqdm(columns_to_encode, desc=\"Procesando columnas para valores únicos\"):\n",
    "        unique_vals_col = df[col].dropna().unique()\n",
    "        unique_values_set.update(unique_vals_col)\n",
    "    unique_values = sorted(unique_values_set)\n",
    "    print(f\"Valores únicos encontrados: {unique_values}\")\n",
    "\n",
    "    # Paso 2: Convertir las columnas a listas de listas para ser procesadas en Cython\n",
    "    list_data = []\n",
    "    print(\"Convirtiendo las columnas booleanas a listas de listas para Cython:\")\n",
    "    for col in tqdm(columns_to_encode, desc=\"Convertir columnas a listas\"):\n",
    "        column_list = df[col].astype(str).tolist()  # Mantener los valores como strings\n",
    "        list_data.append(column_list)\n",
    "    print(\"Conversión completada.\")\n",
    "\n",
    "    # Paso 3: Procesar los datos con la función optimizada en Cython\n",
    "    print(\"Realizando one-hot encoding utilizando la función optimizada en Cython:\")\n",
    "    ohe_result = boolean_features_ohe_cython(list_data, unique_values)\n",
    "    print(\"One-hot encoding completado.\")\n",
    "\n",
    "    # Paso 4: Convertir el resultado de Cython a un DataFrame, alineando el índice con df\n",
    "    print(\"Creando el DataFrame de columnas codificadas:\")\n",
    "    ohe_df = pd.DataFrame(ohe_result, columns=unique_values, index=df.index)\n",
    "    print(f\"DataFrame de one-hot encoding creado con {len(ohe_df.columns)} columnas y {ohe_df.shape[0]} filas.\")\n",
    "\n",
    "    # Paso 5: Concatenar las nuevas columnas codificadas al DataFrame original\n",
    "    print(\"Concatenando las columnas codificadas al DataFrame original:\")\n",
    "    df_expanded = pd.concat([df, ohe_df], axis=1)\n",
    "    print(f\"Concatenación completada. El DataFrame ahora tiene {df_expanded.shape[1]} columnas y {df_expanded.shape[0]} filas.\")\n",
    "\n",
    "    # Paso 6: Eliminar las columnas booleanas originales del DataFrame\n",
    "    print(\"Eliminando las columnas booleanas originales del DataFrame:\")\n",
    "    df_expanded.drop(columns=columns_to_encode, inplace=True)\n",
    "    print(f\"Columnas eliminadas: {columns_to_encode}\")\n",
    "\n",
    "    print(\"Proceso de one-hot encoding finalizado exitosamente.\\n\")\n",
    "\n",
    "    return df_expanded\n",
    "\n",
    "# Extensión de características temporales (día de la semana, momento del día, etc.) y festividades\n",
    "def time_features_extension(df):\n",
    "    \"\"\"\n",
    "    Procesa las características temporales del DataFrame y agrega nuevas columnas derivadas relacionadas con el tiempo y festividades.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convertir 'auction_time' de timestamp a una fecha legible\n",
    "    df['auction_time'] = pd.to_datetime(df['auction_time'], unit='s')\n",
    "\n",
    "    # Reemplazar NaN en 'timezone_offset' por 0\n",
    "    df['timezone_offset'] = df['timezone_offset'].fillna(0)\n",
    "\n",
    "    # Ajustar la hora según el 'timezone_offset' para obtener la hora local\n",
    "    df['auction_time_local'] = df.apply(\n",
    "        lambda row: row['auction_time'] + pd.DateOffset(hours=row['timezone_offset']), axis=1\n",
    "    )\n",
    "\n",
    "    # Crear la columna 'week_day' (1 para lunes, 7 para domingo)\n",
    "    df['week_day'] = df['auction_time_local'].dt.weekday + 1\n",
    "\n",
    "    # Crear la columna 'moment_of_the_day' (1 para temprano, 2 para tarde, 3 para noche)\n",
    "    df['moment_of_the_day'] = pd.cut(df['auction_time_local'].dt.hour, \n",
    "                                     bins=[0, 12, 18, 24], labels=[1, 2, 3], include_lowest=True, right=False)\n",
    "\n",
    "    # Eliminar las columnas originales 'auction_time', 'timezone_offset' y 'auction_time_local'\n",
    "    df.drop(columns=['auction_time', 'timezone_offset', 'auction_time_local'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Agrupación de edades en rangos numéricos\n",
    "def age_group(df, columna_edad):\n",
    "    \"\"\"\n",
    "    Agrupa las edades en rangos numéricos utilizando Cython para mejorar el rendimiento.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene la columna de edades.\n",
    "    - columna_edad (str): Nombre de la columna que contiene las edades.\n",
    "\n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame con la nueva columna 'age_group' que representa el rango de edad.\n",
    "    \"\"\"\n",
    "    # Convertir la columna de edad a una lista\n",
    "    edades = df[columna_edad].tolist()\n",
    "\n",
    "    # Usar la función Cythonizada para agrupar las edades\n",
    "    df['age_group'] = agrupar_edades_cython(edades)\n",
    "\n",
    "    # Eliminar la columna original de edades\n",
    "    df.drop(columns=[columna_edad], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Agrupo action_list_0 a auction_list_0\n",
    "def expand_action_list_0(df):\n",
    "    \"\"\"\n",
    "    Expande la columna 'action_list_0' en valores únicos y marca con 1 las columnas existentes o las crea si es necesario.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene la columna 'action_list_0' y otras columnas de listas ya expandidas.\n",
    "\n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame actualizado con las columnas de valores únicos de 'action_list_0'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convertir la columna 'action_list_0' y las columnas existentes a listas\n",
    "    action_list_0 = df['action_list_0'].tolist()\n",
    "    existing_columns = df.columns.tolist()\n",
    "    \n",
    "    # Inicializar la matriz actual\n",
    "    current_matrix = df.values.tolist()\n",
    "\n",
    "    # Llamar a la función Cythonizada\n",
    "    updated_matrix = expand_action_list_0_cython(action_list_0, existing_columns, current_matrix)\n",
    "\n",
    "    # Convertir la matriz actualizada de vuelta a un DataFrame\n",
    "    df_updated = pd.DataFrame(updated_matrix, columns=existing_columns)\n",
    "\n",
    "    # Eliminar la columna 'action_list_0'\n",
    "    df_updated.drop(columns=['action_list_0'], inplace=True)\n",
    "\n",
    "    return df_updated\n",
    "\n",
    "# Concateno las categorias de cada nivel\n",
    "def create_level_combination(df):\n",
    "    \"\"\"\n",
    "    Creates a new column 'level_combination' by concatenating the first three characters \n",
    "    of each 'action_categorical' level columns and removes the original level columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame containing the columns:\n",
    "      'action_categorical_0', 'action_categorical_1', 'action_categorical_2', \n",
    "      'action_categorical_3', 'action_categorical_4'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with the new 'level_combination' column and without the original level columns.\n",
    "    \"\"\"\n",
    "    level_columns = [\n",
    "        'action_categorical_0',\n",
    "        'action_categorical_1',\n",
    "        'action_categorical_2',\n",
    "        'action_categorical_3',\n",
    "        'action_categorical_4'\n",
    "    ]\n",
    "    df['level_combination'] = df[level_columns].astype(str).apply(\n",
    "        lambda x: ''.join([s[:3] for s in x]), axis=1\n",
    "    )\n",
    "    df.drop(columns=level_columns, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Heigh x Width a columna\n",
    "def hxw_column(df):\n",
    "    \"\"\"\n",
    "    Crea una nueva columna 'hxw' multiplicando 'creative_height' y 'creative_width'.\n",
    "    Si alguno de los dos tiene un NaN, 'hxw' se establece en 0.\n",
    "    Elimina las columnas originales 'creative_height' y 'creative_width'.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene las columnas 'creative_height' y 'creative_width'.\n",
    "    \n",
    "    Retorna:\n",
    "    - pd.DataFrame: DataFrame con la nueva columna 'hxw' añadida y las columnas originales eliminadas.\n",
    "    \"\"\"\n",
    "    df['hxw'] = df['creative_height'] * df['creative_width']\n",
    "    df.loc[df['creative_height'].isna() | df['creative_width'].isna(), 'hxw'] = 0\n",
    "    df.drop(columns=['creative_height', 'creative_width'], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Gender to number\n",
    "def encode_gender(df):\n",
    "    \"\"\"\n",
    "    Reemplaza los valores de la columna 'gender' de la siguiente manera:\n",
    "    'f' -> 1, 'm' -> 2, 'o' -> 0 y NaN -> -1.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene la columna 'gender'.\n",
    "    \n",
    "    Retorna:\n",
    "    - pd.DataFrame: DataFrame con la columna 'gender' codificada.\n",
    "    \"\"\"\n",
    "    df['gender'] = df['gender'].map({'f': 1, 'm': 2, 'o': 0}).fillna(-1).astype(int)\n",
    "    return df\n",
    "\n",
    "# creative_categorical_11, creative_categorical_9 y creative_categorical_10 a dos columnas\n",
    "def creatives2unique(df):\n",
    "    \"\"\"\n",
    "    Crea o actualiza dos columnas en el DataFrame, una para cada valor único en las columnas\n",
    "    'creative_categorical_11', 'creative_categorical_10', y 'creative_categorical_9'.\n",
    "    Si las columnas ya existen, actualiza los valores a 1 donde ese valor aparece en alguna\n",
    "    de las tres columnas en esa fila.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene las columnas 'creative_categorical_11',\n",
    "                         'creative_categorical_10', y 'creative_categorical_9'.\n",
    "\n",
    "    Retorna:\n",
    "    - pd.DataFrame: DataFrame con las nuevas columnas añadidas o actualizadas.\n",
    "    \"\"\"\n",
    "    unique_values = {'65dcab89', '43c867fd'}\n",
    "    columns_to_check = [\n",
    "        'creative_categorical_11',\n",
    "        'creative_categorical_10',\n",
    "        'creative_categorical_9'\n",
    "    ]\n",
    "\n",
    "    for val in unique_values:\n",
    "        if val in df.columns:\n",
    "            # Si la columna ya existe, actualizamos los valores a 1 donde corresponde\n",
    "            df[val] = df[val] | df[columns_to_check].eq(val).any(axis=1).astype(int)\n",
    "        else:\n",
    "            # Si no existe, creamos la columna con 1 donde corresponde\n",
    "            df[val] = df[columns_to_check].eq(val).any(axis=1).astype(int)\n",
    "\n",
    "    # Eliminar las columnas originales\n",
    "    df.drop(columns=columns_to_check, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Juntar todas las variables categoricas y hacer OHE\n",
    "def process_combineta(df):\n",
    "    \"\"\"\n",
    "    Procesa las columnas proporcionadas en combineta, creando un set con valores únicos,\n",
    "    y generando columnas binarias para cada uno de esos valores. Si la columna ya existe,\n",
    "    actualiza las filas con un 1 donde corresponda. Luego, elimina las columnas originales.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame original.\n",
    "    - combineta_columns (list): Lista de columnas a procesar.\n",
    "\n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame con las columnas binarias añadidas y las columnas originales eliminadas.\n",
    "    \"\"\"\n",
    "    combineta_columns = ['creative_categorical_0', 'creative_categorical_5', 'auction_categorical_0', 'auction_categorical_1', 'auction_categorical_11', 'auction_categorical_7', 'auction_categorical_8', 'auction_categorical_9', 'action_categorical_6', 'action_categorical_7', 'auction_categorical_3', 'auction_categorical_4', 'auction_categorical_5', 'auction_categorical_6', 'auction_categorical_10', 'auction_categorical_12', 'creative_categorical_1', 'creative_categorical_12', 'creative_categorical_2', 'creative_categorical_3', 'creative_categorical_4', 'creative_categorical_6', 'creative_categorical_7', 'creative_categorical_8']\n",
    "\n",
    "    # Unir todas las columnas de combineta en una sola columna de listas\n",
    "    df['combined_combineta'] = df[combineta_columns].astype(str).agg(\n",
    "        lambda x: '[' + ', '.join([f\"'{str(item).strip()}'\" for item in x if item != 'nan']) + ']', axis=1)\n",
    "\n",
    "    # Usar la función expand_list_dummies_cython para descomponer la lista y crear las columnas binarias\n",
    "    df = expand_list_dummies_cython(df, 'combined_combineta')\n",
    "    \n",
    "    df.drop(columns=combineta_columns, inplace=True, errors='raise')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Procesamiento optimizado de un DataFrame\n",
    "def process_optimized(df):\n",
    "    \"\"\"\n",
    "    Aplica una serie de transformaciones al DataFrame utilizando una función Cython optimizada.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame a procesar.\n",
    "    \n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame procesado.\n",
    "    \"\"\"\n",
    "    # Definir el número total de pasos para la barra de progreso\n",
    "    total_steps = 14\n",
    "    \n",
    "    # Inicializar la barra de progreso\n",
    "    with tqdm(total=total_steps, desc=\"Procesando DataFrame\", unit=\"paso\") as pbar:\n",
    "        \n",
    "        print(\"Comenzando el procesamiento optimizado del DataFrame.\")\n",
    "        print(\"Eliminando columnas innecesarias.\")\n",
    "        df = df.drop('action_categorical_5', axis=1)\n",
    "        df = df.drop('auction_categorical_2', axis=1)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Expansión de columnas temporales\")\n",
    "        df = time_features_extension(df)\n",
    "        pbar.update(1)\n",
    "\n",
    "        df = age_group(df, 'auction_age')\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Agrupando columnas de nivel\")\n",
    "        df = create_level_combination(df)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Modificando columnas de genero\")\n",
    "        df = encode_gender(df)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        print(\"Modificando columna de video\")\n",
    "        df['has_video'] = df['has_video'].apply(lambda x: 1 if x == True else 0)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Juntando medidas\")\n",
    "        df = hxw_column(df)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Expansión de columnas booleanas.\")\n",
    "        df = boolean_features_ohe(df)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Creando columnas de creatividad\")\n",
    "        df = creatives2unique(df)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        columns_to_expand = ['auction_list_0', 'action_list_1']\n",
    "\n",
    "        # Bucle para realizar las operaciones\n",
    "        for col in columns_to_expand:\n",
    "            print(f\"Expansión de columnas de listas para {col}.\")\n",
    "            \n",
    "            # Crear la variable 'idx_position' con la última columna antes de la expansión\n",
    "            idx_position = df.columns.get_loc(df.columns[-1])\n",
    "            \n",
    "            # Expansión de la columna\n",
    "            df = expand_list_dummies_cython(df, col)\n",
    "            pbar.update(1)\n",
    "\n",
    "            if col == 'action_list_1':\n",
    "                df = expand_list_dummies_cython(df,'action_list_2')\n",
    "                pbar.update(1)\n",
    "            \n",
    "            if col == 'auction_list_0':\n",
    "                # Bucle para recorrer las columnas que empiezan con 'AND' o 'APL'\n",
    "                if 'AND-APL' not in df.columns:\n",
    "                    df['AND-APL'] = 0  # Inicializar la columna 'AND-APL'\n",
    "\n",
    "                for column in df.columns:\n",
    "                    if column.startswith('AND') or column.startswith('APL'):\n",
    "                        # Poner un 1 en 'AND-APL' si la columna actual tiene un 1 en esa fila\n",
    "                        df['AND-APL'] = df['AND-APL'] | df[column]\n",
    "\n",
    "                # Eliminar todas las columnas que empiezan con 'AND' o 'APL' excepto la columna 'AND-APL'\n",
    "                columns_to_drop = [column for column in df.columns if (column.startswith('AND') or column.startswith('APL')) and column != 'AND-APL']\n",
    "                df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "                print(\"Complementamos con la columna 'action_list_0'\")\n",
    "                df = expand_action_list_0(df)\n",
    "                pbar.update(1)\n",
    "\n",
    "            # Crear la lista de columnas numéricas a partir de la siguiente columna después de 'idx'\n",
    "            categorical_num = df.iloc[:, idx_position:].select_dtypes(include=['number']).columns\n",
    "\n",
    "            # Eliminar columnas numéricas con menos de 1000 valores iguales a 1\n",
    "            for column in categorical_num:\n",
    "                if (df[column] == 1).sum() < 1000 and column != 'AND-APL':\n",
    "                    df.drop(column, axis=1, inplace=True)\n",
    "    \n",
    "        \n",
    "        print(\"Agrupando categorias poco frecuentes\")\n",
    "\n",
    "        categorical_str = df.select_dtypes(include=['object']).columns\n",
    "        categorical_str = categorical_str[categorical_str != 'device_id']\n",
    "        \n",
    "        categorical_num = df.select_dtypes(include=['number']).columns\n",
    "        \n",
    "        # Convertir a matriz bidimensional\n",
    "        data_matrix = df[categorical_str].values.tolist()\n",
    "        data_matrix_cython = [list(row) for row in data_matrix]\n",
    "        \n",
    "        # Llamar a la función Cythonizada\n",
    "        df_cython_data = agrupar_categorias_cython(\n",
    "            categorical_features=categorical_str.tolist(),\n",
    "            data=data_matrix_cython,\n",
    "            umbral=1000  # Umbral de frecuencia para considerar una categoría como rara\n",
    "        )\n",
    "\n",
    "        # Reasignar los datos al DataFrame\n",
    "        for i, col in enumerate(categorical_str):\n",
    "            df[col] = [row[i] for row in df_cython_data]\n",
    "\n",
    "        idx_position = df.columns.get_loc(df.columns[-1])\n",
    "        \n",
    "        df = process_combineta(df)\n",
    "\n",
    "        # Crear la lista de columnas numéricas a partir de la siguiente columna después de 'idx'\n",
    "        categorical_num = df.iloc[:, idx_position:].select_dtypes(include=['number']).columns\n",
    "\n",
    "        # Eliminar columnas numéricas con menos de 1000 valores iguales a 1\n",
    "        for column in categorical_num:\n",
    "            if (df[column] == 1).sum() < 1000:\n",
    "                df.drop(column, axis=1, inplace=True)\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Función para ajustar el tipo de datos de una columna para que Dask tome Nan como valor válido\n",
    "def adjust_dtype(dtype):\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        return 'Int64'\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return 'float64'\n",
    "    elif pd.api.types.is_bool_dtype(dtype):\n",
    "        return 'boolean'\n",
    "    else:\n",
    "        return 'object'\n",
    "    \n",
    "# Procesamiento de datos con Dask\n",
    "def process_data_with_dask(df, npartitions=10, meta_df=None):\n",
    "    \"\"\"\n",
    "    Procesa un DataFrame utilizando Dask para distribuir el trabajo en varias particiones.\n",
    "    Aplica la función process_optimized a cada partición del DataFrame.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): El DataFrame de pandas a procesar.\n",
    "    - npartitions (int): Número de particiones en las que se dividirá el DataFrame para su procesamiento.\n",
    "\n",
    "    Retorna:\n",
    "    - final_df (pd.DataFrame): El DataFrame procesado y concatenado.\n",
    "    \"\"\"\n",
    "    # Convertir el DataFrame de pandas a Dask con el número de particiones especificado\n",
    "    dask_df = dd.from_pandas(df, npartitions=npartitions)\n",
    "    \n",
    "    # Crear el meta DataFrame con tipos ajustados\n",
    "    meta = df.head(0).copy()\n",
    "    for col in meta.columns:\n",
    "        meta[col] = meta[col].astype(adjust_dtype(df[col].dtype))\n",
    "        \n",
    "    # Aplicar la función con map_partitions y especificar el meta\n",
    "    dask_df = dask_df.map_partitions(\n",
    "        lambda df_partition: process_optimized(df_partition),\n",
    "        meta=meta\n",
    "    )\n",
    "\n",
    "    # Ejecutar el cálculo distribuido y convertir el resultado a pandas\n",
    "    final_df = dask_df.compute()\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# Función para calcular la estadística de Cramér's V\n",
    "def cramers_v(confusion_matrix):\n",
    "    \"\"\"\n",
    "    Calcula la estadística de Cramér's V para medir la asociación entre dos variables categóricas.\n",
    "\n",
    "    Parameters:\n",
    "    - confusion_matrix: Matriz de confusión (tabla de contingencia) entre dos variables.\n",
    "\n",
    "    Returns:\n",
    "    - Cramér's V: Valor entre 0 y 1 que indica la fuerza de la asociación.\n",
    "    \"\"\"\n",
    "    # Calcular el estadístico chi-cuadrado\n",
    "    chi2 = chi2_contingency(confusion_matrix, correction=False)[0]\n",
    "    # Número total de observaciones\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    # Obtener el número de filas y columnas de la matriz de confusión\n",
    "    r, k = confusion_matrix.shape\n",
    "    if min(r, k) == 1:\n",
    "        return np.nan  # Evitar dividir por cero\n",
    "    # Calcular Cramér's V\n",
    "    return np.sqrt(chi2 / (n * (min(r, k) - 1)))\n",
    "\n",
    "def handle_none(value):\n",
    "    return 'None' if value is None else str(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenamos datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_data_21 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/ctr_21.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m train_data_20 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/ctr_20.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m train_data_19 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/ctr_19.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m train_data_18 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/ctr_18.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1723\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1723\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1725\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/TDVI/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:579\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:668\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:879\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:2058\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "train_data_21 = pd.read_csv('data/ctr_21.csv')\n",
    "train_data_20 = pd.read_csv('data/ctr_20.csv')\n",
    "train_data_19 = pd.read_csv('data/ctr_19.csv')\n",
    "train_data_18 = pd.read_csv('data/ctr_18.csv')\n",
    "train_data_17 = pd.read_csv('data/ctr_17.csv')\n",
    "train_data_16 = pd.read_csv('data/ctr_16.csv')\n",
    "train_data_15 = pd.read_csv('data/ctr_15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplementary_datasets = [\n",
    "    ('1', train_data_20),\n",
    "    ('2', train_data_19),\n",
    "    ('3', train_data_18),\n",
    "    ('4', train_data_17),\n",
    "    ('5', train_data_16),\n",
    "    ('6', train_data_15)\n",
    "]\n",
    "\n",
    "train_data = train_data_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nombre, dataset in supplementary_datasets:\n",
    "    print(f\"\\nProcesando dataset {nombre}/{len(supplementary_datasets)}\")\n",
    "    train_data, categorias_desconocidas, categorias_raras = augment_train_data(train_data, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape[0])\n",
    "\n",
    "print(train_data_20.shape[0] + train_data_19.shape[0] + train_data_18.shape[0] + train_data_17.shape[0] + train_data_16.shape[0] + train_data_15.shape[0] + train_data_21.shape[0])\n",
    "\n",
    "del train_data_20, train_data_19, train_data_18, train_data_17, train_data_16, train_data_15, train_data_21\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('train_data_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargamos dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_data_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8456028\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape[0])\n",
    "print(train_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, _ = train_test_split(\n",
    "    train_data, \n",
    "    train_size=0.05, \n",
    "    random_state=random_state, \n",
    "    stratify=train_data['Label']\n",
    ")\n",
    "\n",
    "test_data = pd.read_csv('data/ctr_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas de train_data: ['Label', 'auction_bidfloor', 'device_id', 'device_id_type', 'gender', 'has_video', 'week_day', 'moment_of_the_day', 'age_group', 'level_combination', 'hxw', '43c867fd', '47980dda', '65dcab89', '79ceee49', 'IAB-5', 'IAB1', 'IAB1-1', 'IAB1-6', 'IAB12', 'IAB14', 'IAB14-1', 'IAB14-3', 'IAB15', 'IAB15-10', 'IAB17', 'IAB18', 'IAB19', 'IAB19-19', 'IAB20', 'IAB20-6', 'IAB22', 'IAB24', 'IAB3', 'IAB5', 'IAB7', 'IAB9', 'IAB9-23', 'IAB9-30', 'IAB9-5', 'books', 'business', 'entertainment', 'games', 'healthcare_and_fitness', 'lifestyle', 'music', 'news', 'photography', 'productivity', 'reference', 'social_networking', 'sports', 'travel', 'utilities', 'weather', 'AND-APL', 'IAB22-2', 'IAB8-9', '-2606', '-5559', '-5577', '-5578', '-5579', '-5736', '-6118', '-6119', '-6451', '-6454', '-6613', '-6614', '-6615', '-6616', '-6617', '-6618', '-6779', '-6780', '-6823', '-6824', '-6871', '-6874', '-6902', '-6903', '-7033', '-7190', '-7194', '-7195', '-7199', '5577', '5578', '5579', '6118', '6119', '6451', '6454', '6614', '6615', '6616', '6617', '6618', '6780', '6823', '6824', '6871', '6874', '6902', '6903', '7190', '7194', '7195', '7199', '-2560', '-5469', '-5470', '-5471', '-5560', '-5576', '-5603', '-5604', '-5605', '-5613', '-5902', '-6125', '-6217', '-6218', '-6219', '-6220', '-6223', '-6224', '-6226', '-6309', '-6543', '-6544', '-6547', '-6548', '-6620', '-6621', '-6770', '-6774', '-6775', '-6800', '-6820', '-6822', '-6825', '-6848', '-6849', '-6850', '-6865', '-6866', '-6867', '-6875', '-6876', '-6904', '-6905', '-6929', '-6938', '-6946', '-7111', '-7112', '-7126', '-7127', '-7143', '-7196', '-7263', '-7264', '-7265', '2560', '5469', '5470', '5471', '5560', '5576', '5603', '5604', '5605', '5613', '6125', '6217', '6218', '6226', '6309', '6543', '6544', '6547', '6548', '6620', '6621', '6770', '6774', '6775', '6800', '6820', '6822', '6825', '6849', '6850', '6866', '6867', '6875', '6876', '6904', '6905', '6929', '6938', '6946', '7126', '7143', '7196', '7263', '000a42f5', '010c4d7f', '01566d09', '02245f43', '034e01d7', '03e3f351', '047da47e', '04cae3c5', '04d2ed63', '05a781fa', '05d2e7fd', '05e210ae', '07c8199e', '08950fee', '08e7c6ae', '095cc02c', '09dd0e8d', '0a196880', '0a5fea98', '0bcbc0b8', '0d389cf9', '0d660d9a', '0db77ae8', '0e2046fb', '0e28039c', '0e83d9f5', '0eaceca2', '0ef62fd9', '0f40c06b', '0f493a39', '0fa26b70', '0fe6706c', '103fa18f', '105a7850', '10c46747', '11105871', '11a9f48f', '13e1c4f4', '140bab3d', '150d94b7', '15520e05', '15f36bd8', '16005afa', '161edb22', '173ce000', '186b549a', '196e9b2e', '198c733d', '199871e3', '19998e8d', '19b3a8af', '1af546ea', '1b1239b8', '1b26c761', '1ba824f9', '1bd7f5c2', '1be29569', '1c5b8730', '1c95d1e5', '1d2fa540', '1d8d143d', '1e5ce4e9', '1ea53035', '1f411474', '217a75f8', '21c0db7e', '21c52ab4', '21e00c7d', '220eba28', '22546969', '22960cfa', '22e0670e', '22e4f44c', '2344f8ca', '239a435c', '24377591', '246cce9b', '24c5c9b3', '25343e0e', '25776807', '26112e76', '26b4ffc3', '26cbe8ca', '273f62c3', '28712cad', '28c10de5', '2906514a', '29b77816', '29d06c7b', '2a9c5636', '2b22dd6f', '2b235cde', '2b60b31f', '2b751709', '2d04d29d', '2d142ca9', '2d3a2bdc', '2daf33f8', '2e87db1e', '2eefcb56', '2f74fc92', '2f8a3392', '3103cd5a', '310dd70a', '3117d02e', '3124f99c', '316aa491', '31b31f57', '32362bb2', '335efc2f', '3396bc83', '3474d33c', '34a5b5e1', '356a814d', '35fcfcbb', '373200a1', '379a8766', '3a76072f', '3aabd428', '3b26dcb0', '3c4be72e', '3d496348', '3d8d64ea', '3df9c1bd', '3e4b21a0', '3fb01dc9', '406ad3e7', '40807ca1', '40ceda44', '428c8b8a', '431302a4', '434922cf', '451daa93', '459c074c', '45a5c44a', '460e1f8d', '46de1f27', '473bbfa4', '4763a186', '49428151', '49b3ff76', '4a8ff226', '4ba9f278', '4c38e1d7', '4cb70e24', '4d10eedf', '4f17add5', '4f7786a1', '50921258', '5098dc9a', '50e522c2', '51499fde', '5174be50', '518e5033', '518f127b', '5271393e', '533e962f', '534e1302', '543a32c7', '546044ce', '5631509c', '566b54c2', '575789c9', '585c60a6', '59638795', '598b63e9', '5a1b9b0e', '5a6ad423', '5a8c5850', '5b030ee9', '5de48e0f', '5df0789c', '5e6c4fa2', '5f68c5e6', '61559b0f', '62ae5ddb', '62c54706', '62d6343e', '64bc05f5', '652c4598', '654a0207', '65537bf4', '65cc5f44', '66f31a18', '67555e9a', '6856f984', '6942a91d', '6942f6f1', '6988d295', '6a275605', '6a3c2b88', '6acc6de3', '6b5d903f', '6d3267a4', '6d4f0cb8', '6e069bec', '6f5a8fcb', '6fd21cfe', '6fda6ba6', '714a9147', '71ecc2e3', '72541507', '7359bd0d', '735ae96a', '7392abc4', '73de9646', '75bcbeb3', '76d66943', '77201adc', '777f2dce', '77aec44a', '782c2fd0', '79896a66', '799809fe', '79ac9638', '79c57c57', '7a1dc896', '7acb895b', '7b12ea74', '7b40e398', '7b7a6b4c', '7c119b88', '7c1e45fe', '7c34e4e9', '7d7776ec', '7daeddab', '7db9dc73', '7e952598', '7f1dcf83', '7f4d3c81', '7f776def', '80253976', '80819d2d', '80d0eca9', '810586d7', '81cf1a0e', '82429792', '8293f7bc', '837792d1', '83dae26d', '85301835', '856f1b75', '857b03df', '857e02b0', '85878e10', '85ff3683', '864f5111', '8663f240', '868a1c2d', '86ad615f', '87f54743', '88a3d2fd', '8963d136', '897b7228', '898b7415', '89a1e033', '89b2cd84', '8aca96c3', '8ba07f8b', '8bc11a1e', '8e3a0297', '8e9f619f', '8f6f5f41', '9041f1b3', '90557a5e', '9062e8e1', '922325e3', '92a99bcb', '93546afe', '94bcceb7', '94e43af2', '955b4015', '95e5cf4b', '9663198a', '9685087a', '96b109e9', '96bfa073', '96fe5027', '973025d7', '974b0af5', '976e2631', '977c2300', '97e9b40f', '97fd48df', '98cb5201', '9914b65d', '99dae3dc', '99df4af8', '9a225691', '9c3324c7', '9ddf0de9', '9e55df10', '9efce19a', '9fc861ea', 'Otro', 'a14abbcb', 'a167e52e', 'a1a5ced2', 'a25af161', 'a52dbf3d', 'a59cb230', 'a5b257c7', 'a5c30fc9', 'a649d6fa', 'a6d930ab', 'a75afb39', 'a7e34545', 'a803a2e6', 'a8514219', 'aa146d77', 'aa454a3e', 'ababfacb', 'adf97351', 'ae9d3bc0', 'b00371d3', 'b04e4d07', 'b08b0036', 'b108b520', 'b139ea74', 'b402150e', 'b42d829e', 'b4e520dd', 'b50305c8', 'b5162e4d', 'b5734977', 'b6473078', 'b6910b48', 'b78504d4', 'b8d511a1', 'b91470d8', 'b98125c8', 'b9c727d7', 'bc2ea7c7', 'bd2e3ef9', 'bea5c21c', 'c041644a', 'c20b2db7', 'c5abc8fa', 'c5de5449', 'c628d5b8', 'c6382961', 'c785677e', 'c99696ad', 'ca48f685', 'cbfed1f4', 'cc2472bb', 'cc8c473b', 'cd44ba3f', 'cdc6c248', 'ce04f021', 'd08710db', 'd11f018a', 'd299e829', 'd2fee5de', 'd3a7d8ba', 'd3c7863f', 'd40926f7', 'd42bf4a2', 'd4eea9a2', 'd6496210', 'd66cdf02', 'd75eba3c', 'd7a49a2e', 'd81fe3e2', 'd8d27d3b', 'd9157eec', 'd97f9242', 'd99d56d5', 'd9d53fe0', 'da6404fd', 'db64b047', 'dbacf9af', 'dc06e8a9', 'dd370958', 'df9868ac', 'e10064eb', 'e10525ac', 'e11ff413', 'e2538fca', 'e361d5bc', 'e36715bf', 'e3883dd9', 'e4abb93a', 'e5892738', 'e5dc05a8', 'e609fa5a', 'e70caf72', 'e7c67c20', 'e7e5ef8c', 'e995c285', 'e9cc8788', 'e9e0850e', 'ea1be7a8', 'ea1e3add', 'ea238f7b', 'ea39abac', 'ea5bfdf9', 'ea79857a', 'ea8a821b', 'ea97353f', 'eacc48c8', 'eb878a56', 'ec38fdce', 'ec679451', 'ec8edf0b', 'ed14c0c3', 'edc46daf', 'edfccd70', 'ee2cf71d', 'ee36ba80', 'ee79c73f', 'eea07a2a', 'eec28017', 'ef20d227', 'ef64698f', 'f0369c26', 'f0504648', 'f079a0fd', 'f1120521', 'f1a4229a', 'f1b6690d', 'f2400ab2', 'f28636b3', 'f3b258d1', 'f451977a', 'f45760f0', 'f46a6d51', 'f588215f', 'f58db417', 'f65ed99c', 'f690df59', 'f69afee2', 'f71b16a6', 'f7755cb3', 'f87110e6', 'f8867747', 'f8a1840a', 'f936a787', 'f9c77d37', 'f9c9a471', 'f9ddb3c2', 'f9e98c82', 'fa1a6c06', 'fae6eae7', 'fccb6ea5', 'fcf406ac', 'fd953d8d', 'fdc88ebb', 'fe3a6e2a', 'fe864de5', 'fe917183', 'fed5fd5c', 'fedeee13', 'ff1e860d', 'ff25306d', 'ff29e19d', 'ff858f87', 'ffb0cc91']\n",
      "Columnas de test_data: ['action_categorical_0', 'action_categorical_1', 'action_categorical_2', 'action_categorical_3', 'action_categorical_4', 'action_categorical_5', 'action_categorical_6', 'action_categorical_7', 'action_list_0', 'action_list_1', 'action_list_2', 'auction_age', 'auction_bidfloor', 'auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2', 'auction_categorical_0', 'auction_categorical_1', 'auction_categorical_10', 'auction_categorical_11', 'auction_categorical_12', 'auction_categorical_2', 'auction_categorical_3', 'auction_categorical_4', 'auction_categorical_5', 'auction_categorical_6', 'auction_categorical_7', 'auction_categorical_8', 'auction_categorical_9', 'auction_list_0', 'auction_time', 'creative_categorical_0', 'creative_categorical_1', 'creative_categorical_10', 'creative_categorical_11', 'creative_categorical_12', 'creative_categorical_2', 'creative_categorical_3', 'creative_categorical_4', 'creative_categorical_5', 'creative_categorical_6', 'creative_categorical_7', 'creative_categorical_8', 'creative_categorical_9', 'creative_height', 'creative_width', 'device_id', 'device_id_type', 'gender', 'has_video', 'timezone_offset', 'id']\n"
     ]
    }
   ],
   "source": [
    "# Verificar las columnas\n",
    "print(\"Columnas de train_data:\", train_data.columns.tolist())\n",
    "print(\"Columnas de test_data:\", test_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Características categóricas: ['action_categorical_0', 'action_categorical_1', 'action_categorical_2', 'action_categorical_3', 'action_categorical_4', 'action_categorical_5', 'action_categorical_6', 'action_categorical_7', 'action_list_0', 'action_list_1', 'action_list_2', 'auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2', 'auction_categorical_0', 'auction_categorical_1', 'auction_categorical_10', 'auction_categorical_11', 'auction_categorical_12', 'auction_categorical_2', 'auction_categorical_3', 'auction_categorical_4', 'auction_categorical_5', 'auction_categorical_6', 'auction_categorical_7', 'auction_categorical_8', 'auction_categorical_9', 'auction_list_0', 'creative_categorical_0', 'creative_categorical_1', 'creative_categorical_10', 'creative_categorical_11', 'creative_categorical_12', 'creative_categorical_2', 'creative_categorical_3', 'creative_categorical_4', 'creative_categorical_5', 'creative_categorical_6', 'creative_categorical_7', 'creative_categorical_8', 'creative_categorical_9', 'device_id', 'device_id_type', 'gender']\n",
      "Características numéricas: ['auction_age', 'auction_bidfloor', 'auction_time', 'creative_height', 'creative_width', 'timezone_offset']\n"
     ]
    }
   ],
   "source": [
    "# Identificar columnas categóricas y numéricas en train_data\n",
    "categorical_features = train_data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numeric_features = train_data.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Excluir columnas que no se usarán para preprocesamiento\n",
    "# Asumiendo que 'Label' está en train_data y 'id' en test_data\n",
    "if 'Label' in numeric_features:\n",
    "    numeric_features.remove('Label')\n",
    "if 'id' in test_data.columns:\n",
    "    test_data = test_data.drop('id', axis=1)\n",
    "\n",
    "print(\"Características categóricas:\", categorical_features)\n",
    "print(\"Características numéricas:\", numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el preprocesador\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Imputar valores numéricos con la media y escalar\n",
    "        ('num', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='mean'))\n",
    "        ]), numeric_features),\n",
    "        \n",
    "        # Imputar valores categóricos con 'Desconocido' y aplicar One-Hot Encoding\n",
    "        ('cat', Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='Desconocido')),\n",
    "            ('onehot', OneHotEncoder(drop='first', sparse=False, handle_unknown='ignore'))\n",
    "        ]), categorical_features)\n",
    "    ],\n",
    "    remainder='drop'  # Excluir columnas no especificadas\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separar las etiquetas del conjunto de entrenamiento\n",
    "y_train = train_data['Label']\n",
    "X_train = train_data.drop(['Label'], axis=1)\n",
    "\n",
    "del train_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar el preprocesador en el conjunto de entrenamiento y transformar ambos conjuntos\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "del X_train\n",
    "gc.collect()\n",
    "\n",
    "X_test_processed = preprocessor.transform(test_data)\n",
    "\n",
    "del test_data\n",
    "gc.collect()\n",
    "\n",
    "# Convertir a DataFrame para facilitar la manipulación\n",
    "# Obtener los nombres de las columnas después de la transformación\n",
    "ohe_columns = preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names_out(categorical_features)\n",
    "processed_columns = numeric_features + list(ohe_columns)\n",
    "\n",
    "X_train_processed = pd.DataFrame(X_train_processed, columns=processed_columns)\n",
    "X_test_processed = pd.DataFrame(X_test_processed, columns=processed_columns)\n",
    "\n",
    "print(\"X_train_processed Shape:\", X_train_processed.shape)\n",
    "print(\"X_test_processed Shape:\", X_test_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "# Definir el número de vecinos\n",
    "k = 5  # Puedes ajustar este valor según tus necesidades\n",
    "\n",
    "# Inicializar y ajustar el modelo NearestNeighbors en el conjunto de entrenamiento procesado\n",
    "nbrs = NearestNeighbors(n_neighbors=k, metric='cosine')\n",
    "nbrs.fit(X_train_processed)\n",
    "\n",
    "# Encontrar los índices de los vecinos más cercanos para cada fila de prueba\n",
    "distances, indices = nbrs.kneighbors(X_test_processed)\n",
    "\n",
    "# Mostrar una muestra de los índices y distancias\n",
    "print(\"Índices de Vecinos Más Cercanos (muestra):\\n\", indices[:5])\n",
    "print(\"Distancias de Vecinos Más Cercanos (muestra):\\n\", distances[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_data_combined.csv')\n",
    "\n",
    "# Obtener los índices únicos de las filas de entrenamiento que son similares a alguna de las filas de prueba\n",
    "similar_train_indices = np.unique(indices.flatten())\n",
    "\n",
    "# Seleccionar las filas similares del conjunto de entrenamiento original\n",
    "similar_train_data = train_data.iloc[similar_train_indices].copy()\n",
    "\n",
    "# Mostrar la forma del conjunto de datos similar\n",
    "print(f\"Filas similares encontradas: {similar_train_data.shape[0]}\")\n",
    "\n",
    "# Mostrar las primeras filas seleccionadas\n",
    "print(similar_train_data.head())\n",
    "\n",
    "del train_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(similar_train_data.shape[1])\n",
    "print(similar_train_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_combined = similar_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y validación con estratificación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data_combined.drop(columns='Label'),  # Características\n",
    "    train_data_combined['Label'],               # Variable objetivo\n",
    "    test_size=0.01,                              # 10% para validación\n",
    "    stratify=train_data_combined['Label'],      # Estratificación basada en la variable objetivo\n",
    "    random_state=random_state                   # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "# Verificar la proporción de clases en el conjunto de entrenamiento y validación\n",
    "print(\"Proporción en el conjunto de entrenamiento:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"Proporción en el conjunto de validación:\")\n",
    "print(y_val.value_counts(normalize=True))\n",
    "\n",
    "del train_data_combined, y_val, X_val, similar_train_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = process_optimized(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/ctr_test.csv')\n",
    "\n",
    "# X_test = process_data_with_dask(test_data, npartitions=10)\n",
    "\n",
    "X_test = X_test_processed\n",
    "\n",
    "del X_test_processed\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excluir 'id' de las columnas de X_test para el reordenamiento\n",
    "common_columns = [col for col in X_test.columns if col != 'id']\n",
    "\n",
    "# Asegúrate de que las columnas en X_train coincidan con las de X_test (sin 'id')\n",
    "missing_cols = set(common_columns) - set(X_train.columns)\n",
    "for col in missing_cols:\n",
    "    X_train[col] = 0\n",
    "\n",
    "# Eliminar columnas extra en X_train que no están en X_test (sin contar 'id')\n",
    "extra_cols = set(X_train.columns) - set(common_columns)\n",
    "X_train.drop(columns=extra_cols, inplace=True)\n",
    "\n",
    "# Reordenar X_train para que tenga el mismo orden de columnas que X_test (sin 'id')\n",
    "X_train = X_train[common_columns]\n",
    "\n",
    "# Verificar el número de columnas en X_train\n",
    "print(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las columnas categóricas y numéricas\n",
    "categorical_features_to_encode = X_train.select_dtypes(include=['object']).columns\n",
    "numeric_features = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Preprocesador común para imputación y codificación\n",
    "common_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Imputar valores numéricos con la media\n",
    "        ('num', SimpleImputer(strategy='mean'), numeric_features),\n",
    "        \n",
    "        # Imputar valores categóricos con 'Desconocido' y aplicar One-Hot Encoding\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='Desconocido')),\n",
    "            ('onehot', OneHotEncoder(drop='first', sparse_output=True, handle_unknown='ignore'))\n",
    "        ]), categorical_features_to_encode)\n",
    "    ],\n",
    "    remainder='drop'  # Excluir columnas no especificadas\n",
    ")\n",
    "\n",
    "# Definir el modelo XGBoost\n",
    "model_xgb = XGBClassifier(\n",
    "    random_state=random_state, \n",
    "    use_label_encoder=False, \n",
    "    eval_metric='auc'\n",
    ")\n",
    "\n",
    "# Pipeline para XGBoost (mantiene matrices dispersas)\n",
    "pipeline_xgb = Pipeline(steps=[\n",
    "    ('preprocessor', common_preprocessor),\n",
    "    ('classifier', model_xgb),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperot XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el espacio de búsqueda para los hiperparámetros de XGBoost\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 10)), \n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]), \n",
    "    'gamma': hp.uniform('gamma', 0, 0.5),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0.5, 2),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 1),  # L1 regularización\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 1),  # L2 regularización\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1),  # Muestreo a nivel de split\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.5, 1),  # Muestreo por nodo\n",
    "    'grow_policy': hp.choice('grow_policy', ['depthwise', 'lossguide']),  # Estrategia de crecimiento\n",
    "    'tree_method': hp.choice('tree_method', ['auto', 'approx', 'hist'])  # Métodos de construcción del árbol\n",
    "}\n",
    "\n",
    "# Función objetivo para Hyperopt utilizando validación cruzada\n",
    "def objective_xgb(params):\n",
    "    \n",
    "    # Asegurar que los parámetros sean del tipo correcto\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['min_child_weight'] = int(params['min_child_weight'])\n",
    "    \n",
    "    # Definir el modelo con los hiperparámetros actuales\n",
    "    model_xgb = XGBClassifier(\n",
    "        max_depth=params['max_depth'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        n_estimators=params['n_estimators'],\n",
    "        gamma=params['gamma'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        scale_pos_weight=params['scale_pos_weight'],\n",
    "        reg_alpha=params['reg_alpha'],  # L1 regularización\n",
    "        reg_lambda=params['reg_lambda'],  # L2 regularización\n",
    "        colsample_bylevel=params['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "        colsample_bynode=params['colsample_bynode'],  # Muestreo por nodo\n",
    "        grow_policy=params['grow_policy'],  # Estrategia de crecimiento\n",
    "        tree_method=params['tree_method'],  # Método de construcción del árbol\n",
    "        use_label_encoder=False,  # Evitar advertencias en versiones más recientes de XGBoost\n",
    "        eval_metric='auc',\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Pipeline para XGBoost (mantiene matrices dispersas)\n",
    "    pipeline_xgb = Pipeline(steps=[\n",
    "        ('preprocessor', common_preprocessor),\n",
    "        ('classifier', model_xgb),\n",
    "    ])\n",
    "    \n",
    "    # Definir la validación cruzada\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Calcular el AUC utilizando validación cruzada\n",
    "    auc = cross_val_score(pipeline_xgb, X_train, y_train, cv=cv, scoring='roc_auc').mean()\n",
    "    \n",
    "    # Opcional: imprimir los hiperparámetros y el AUC actual\n",
    "    print(f\"Hiperparámetros: {params}, AUC: {auc:.4f}\")\n",
    "    \n",
    "    # Retornar el valor a minimizar (1 - AUC)\n",
    "    return {'loss': 1 - auc, 'status': STATUS_OK}\n",
    "\n",
    "# Ejecutar la optimización\n",
    "trials = Trials()\n",
    "\n",
    "best_xgb = fmin(\n",
    "    fn=objective_xgb,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=1,\n",
    "    trials=trials,\n",
    "    rstate=np.random.default_rng(random_state)  # Asegurar reproducibilidad\n",
    ")\n",
    "\n",
    "# No es necesario volver a mapear los hiperparámetros aquí, ya se hizo dentro de la función objetivo\n",
    "print(\"Mejores hiperparámetros para XGBoost:\")\n",
    "print(best_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PRUEBA CON TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices\n",
    "tree_method_options = ['auto', 'exact', 'approx', 'hist']\n",
    "grow_policy_options = ['depthwise', 'lossguide']\n",
    "n_estimators_options = [100, 200, 300, 400, 500]\n",
    "\n",
    "# Reconstruir el modelo de XGBoost con los mejores hiperparámetros\n",
    "best_model_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=n_estimators_options[best_xgb['n_estimators']],\n",
    "    max_depth=best_xgb['max_depth'],\n",
    "    learning_rate=best_xgb['learning_rate'],\n",
    "    subsample=best_xgb['subsample'],\n",
    "    colsample_bytree=best_xgb['colsample_bytree'],\n",
    "    min_child_weight=best_xgb['min_child_weight'],\n",
    "    gamma=best_xgb['gamma'],\n",
    "    scale_pos_weight=best_xgb['scale_pos_weight'],\n",
    "    reg_alpha=best_xgb['reg_alpha'],  # L1 regularización\n",
    "    reg_lambda=best_xgb['reg_lambda'],  # L2 regularización\n",
    "    colsample_bylevel=best_xgb['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "    colsample_bynode=best_xgb['colsample_bynode'],  # Muestreo por nodo\n",
    "    grow_policy=grow_policy_options[best_xgb['grow_policy']],  # Estrategia de crecimiento\n",
    "    tree_method=tree_method_options[best_xgb['tree_method']],  # Método de construcción del árbol\n",
    "    random_state=random_state,\n",
    "    use_label_encoder=False,  # Evitar advertencias en versiones más recientes de XGBoost\n",
    "    eval_metric=\"auc\"  # Establecer AUC como métrica de evaluación\n",
    ")\n",
    "\n",
    "# Crear un nuevo pipeline reutilizando el preprocesador original y el mejor modelo\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', common_preprocessor),\n",
    "    ('classifier', model_xgb),\n",
    "])\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "xgb_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir en el conjunto de testeo\n",
    "y_preds_xgb_test = xgb_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_xgb = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_xgb_test})\n",
    "submission_df_xgb[\"id\"] = submission_df_xgb[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros\n",
    "file_name_xgb = (\n",
    "    f\"xgboost_preds_ne_{best_xgb['n_estimators']}_\"\n",
    "    f\"md_{handle_none(best_xgb['max_depth'])}_\"\n",
    "    f\"lr_{round(best_xgb['learning_rate'], 2)}_\"\n",
    "    f\"ss_{round(best_xgb['subsample'], 2)}_\"\n",
    "    f\"csb_{round(best_xgb['colsample_bytree'], 2)}_\"\n",
    "    f\"cb_level_{round(best_xgb['colsample_bylevel'], 2)}_\"\n",
    "    f\"cb_node_{round(best_xgb['colsample_bynode'], 2)}_\"\n",
    "    f\"mcw_{round(best_xgb['min_child_weight'], 2)}_\"\n",
    "    f\"gamma_{round(best_xgb['gamma'], 2)}_\"\n",
    "    f\"ra_{round(best_xgb['reg_alpha'], 2)}_\"\n",
    "    f\"rl_{round(best_xgb['reg_lambda'], 2)}_\"\n",
    "    f\"spw_{round(best_xgb['scale_pos_weight'], 2)}_\"\n",
    "    f\"gp_{grow_policy_options[best_xgb['grow_policy']]}_\"\n",
    "    f\"tm_{tree_method_options[best_xgb['tree_method']]}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_xgb.to_csv(os.path.join(\"submits\", file_name_xgb), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_xgb}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDVI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
