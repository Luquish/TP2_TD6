{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 43992294\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.impute import SimpleImputer\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "import scipy.stats as ss\n",
    "import os\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import ast\n",
    "\n",
    "import warnings\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Cython imports\n",
    "from tools import agrupar_categorias_cython, custom_one_hot_encoder_cython, boolean_features_ohe_cython, agrupar_edades_cython, expand_action_list_0_cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "tqdm.pandas()\n",
    "\n",
    "# Agrega filas que contienen categorías desconocidas o raras (poca frecuencia) al conjunto de entrenamiento\n",
    "def augment_train_data(main_train_df, supplementary_df, umbral_raras=100):\n",
    "    \"\"\"\n",
    "    Agrega filas del dataset suplementario al conjunto de entrenamiento principal\n",
    "    basándose en categorías desconocidas y raras, evitando la duplicación de filas.\n",
    "\n",
    "    Parámetros:\n",
    "    - main_train_df (pd.DataFrame): DataFrame principal de entrenamiento.\n",
    "    - supplementary_df (pd.DataFrame): DataFrame suplementario del cual se extraerán las filas.\n",
    "    - umbral_raras (int): Umbral de frecuencia para considerar una categoría como rara.\n",
    "\n",
    "    Retorna:\n",
    "    - main_train_df (pd.DataFrame): DataFrame de entrenamiento actualizado.\n",
    "    - categorias_desconocidas (dict): Diccionario actualizado de categorías desconocidas.\n",
    "    - categorias_raras (dict): Diccionario actualizado de categorías raras.\n",
    "    \"\"\"\n",
    "    # Definir columnas que no deseas tratar como categóricas\n",
    "    columns_to_exclude = ['auction_list_0', 'action_list_1', 'action_list_2', 'auction_time']\n",
    "\n",
    "    # Identificar columnas categóricas excluyendo las especificadas\n",
    "    categorical_features = main_train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    categorical_features = [col for col in categorical_features if col not in columns_to_exclude]\n",
    "    \n",
    "    # Crear un diccionario para almacenar las categorías desconocidas por columna\n",
    "    categorias_desconocidas = {}\n",
    "    \n",
    "    # Iterar a través de cada columna categórica para identificar categorías desconocidas\n",
    "    for columna in categorical_features:\n",
    "        # Obtener las categorías únicas en el conjunto de entrenamiento\n",
    "        categorias_train = set(main_train_df[columna].dropna().unique())\n",
    "        \n",
    "        # Obtener las categorías únicas en el dataset suplementario\n",
    "        categorias_suplementario = set(supplementary_df[columna].dropna().unique())\n",
    "        \n",
    "        # Identificar las categorías en el dataset suplementario que no están en el entrenamiento\n",
    "        desconocidas = categorias_suplementario - categorias_train\n",
    "        \n",
    "        # Almacenar las categorías desconocidas en el diccionario como una lista\n",
    "        categorias_desconocidas[columna] = list(desconocidas)\n",
    "    \n",
    "    # Inicializar el diccionario para almacenar las categorías raras por columna\n",
    "    categorias_raras = {}\n",
    "    \n",
    "    # Identificar categorías raras en el conjunto de entrenamiento\n",
    "    for columna in categorical_features:\n",
    "        # Contar la frecuencia de cada categoría\n",
    "        frecuencia = main_train_df[columna].value_counts()\n",
    "        \n",
    "        # Identificar categorías que aparecen menos de umbral_raras veces\n",
    "        raras = frecuencia[frecuencia < umbral_raras].index.tolist()\n",
    "        \n",
    "        # Almacenar en el diccionario\n",
    "        categorias_raras[columna] = raras\n",
    "    \n",
    "    # Crear una máscara booleana para filas con categorías desconocidas o raras\n",
    "    mask_desconocidas = pd.Series([False] * len(supplementary_df))\n",
    "    mask_raras = pd.Series([False] * len(supplementary_df))\n",
    "    \n",
    "    for columna in categorical_features:\n",
    "        # Actualizar la máscara para categorías desconocidas\n",
    "        if categorias_desconocidas[columna]:\n",
    "            mask_desconocidas = mask_desconocidas | supplementary_df[columna].isin(categorias_desconocidas[columna])\n",
    "        \n",
    "        # Actualizar la máscara para categorías raras\n",
    "        if categorias_raras[columna]:\n",
    "            mask_raras = mask_raras | supplementary_df[columna].isin(categorias_raras[columna])\n",
    "    \n",
    "    # Combinar ambas máscaras\n",
    "    mask_total = mask_desconocidas | mask_raras\n",
    "    \n",
    "    # Filtrar filas únicas a agregar\n",
    "    filas_a_agregar = supplementary_df[mask_total].drop_duplicates()\n",
    "    \n",
    "    # Mostrar información de agregación\n",
    "    total_agregadas = len(filas_a_agregar)\n",
    "    print(f\"\\nAgregando {total_agregadas} filas del dataset suplementario basadas en categorías desconocidas o raras.\")\n",
    "    \n",
    "    # Agregar las filas al conjunto de entrenamiento\n",
    "    main_train_df = pd.concat([main_train_df, filas_a_agregar], ignore_index=True)\n",
    "    \n",
    "    # Actualizar los diccionarios eliminando las categorías que ya han sido agregadas\n",
    "    for columna in categorical_features:\n",
    "        # Actualizar categorías desconocidas\n",
    "        if categorias_desconocidas[columna]:\n",
    "            categorias_agregadas = filas_a_agregar[columna].unique().tolist()\n",
    "            categorias_desconocidas[columna] = [cat for cat in categorias_desconocidas[columna] if cat not in categorias_agregadas]\n",
    "        \n",
    "        # Actualizar categorías raras\n",
    "        if categorias_raras[columna]:\n",
    "            # Recontar la frecuencia después de agregar\n",
    "            frecuencia = main_train_df[columna].value_counts()\n",
    "            nuevas_raras = frecuencia[frecuencia < umbral_raras].index.tolist()\n",
    "            categorias_raras[columna] = nuevas_raras\n",
    "    \n",
    "    return main_train_df, categorias_desconocidas, categorias_raras\n",
    "\n",
    "# Agrega como columnas binarias las listas de la columna 'auction_list_0', 'action_list_1' y 'action_list_2'\n",
    "def expand_list_dummies_cython(df, column, delimiter='|', prefix=None, suffix=None):\n",
    "    \"\"\"\n",
    "    Expande una columna que contiene listas en múltiples columnas binarias usando un one-hot encoder optimizado con Cython.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame de pandas.\n",
    "    - column (str): Nombre de la columna a expandir.\n",
    "    - delimiter (str): Delimitador a usar en get_dummies (por defecto '|').\n",
    "    - prefix (str, optional): Prefijo para las nuevas columnas binarias.\n",
    "    - suffix (str, optional): Sufijo para las nuevas columnas binarias.\n",
    "    \n",
    "    Returns:\n",
    "    - df_expanded (pd.DataFrame): DataFrame con las nuevas columnas binarias añadidas y la columna original eliminada.\n",
    "    \"\"\"\n",
    "    print(f\"Comenzando la expansión de la columna: '{column}'\")\n",
    "    \n",
    "    # Copiar el DataFrame para evitar modificar el original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Reemplazar NaN por listas vacías\n",
    "    print(f\"Reemplazando NaN en la columna '{column}' por listas vacías.\")\n",
    "    df[column] = df[column].fillna('[]')\n",
    "    \n",
    "    # Definir la función de parsing con impresión de errores\n",
    "    def parse_list(x):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(x)\n",
    "            if isinstance(parsed, list):\n",
    "                # Convertir todos los elementos a strings\n",
    "                return [str(item) for item in parsed]\n",
    "            else:\n",
    "                # Si no es una lista, tratar como un solo elemento\n",
    "                return [str(x)]\n",
    "        except (ValueError, SyntaxError):\n",
    "            # En caso de error al parsear, retornar una lista vacía\n",
    "            return []\n",
    "    \n",
    "    # Aplicar la función de parsing con una barra de progreso\n",
    "    df[column] = df[column].progress_apply(parse_list)\n",
    "    \n",
    "    # Convertir la columna en una lista de listas para pasarla a la función Cythonizada\n",
    "    data_list = df[column].tolist()\n",
    "    \n",
    "    # Llamar a la función optimizada en Cython\n",
    "    unique_categories, binary_matrix = custom_one_hot_encoder_cython(data_list)\n",
    "    print(f\"Codificación completada. {len(unique_categories)} categorías únicas encontradas.\")\n",
    "    \n",
    "    # Crear un DataFrame binario usando la matriz devuelta por Cython\n",
    "    binary_df = pd.DataFrame(binary_matrix, index=df.index, columns=unique_categories)\n",
    "\n",
    "    if prefix:\n",
    "        binary_df = binary_df.add_prefix(f\"{prefix}_\")\n",
    "    if suffix:\n",
    "        binary_df = binary_df.add_suffix(f\"_{suffix}\")\n",
    "    \n",
    "    # Concatenar las columnas binarias al DataFrame original con una barra de progreso\n",
    "    print(\"Concatenando las columnas binarias al DataFrame original.\")\n",
    "    for col in tqdm(binary_df.columns, desc=\"Concatenando columnas binarias\"):\n",
    "        if col in df.columns:\n",
    "            df[col] = np.where((df[col] == 1) | (binary_df[col] == 1), 1, 0)\n",
    "        else:\n",
    "            df[col] = binary_df[col]\n",
    "    \n",
    "    # Eliminar la columna original ya que ha sido expandida\n",
    "    print(f\"Eliminando la columna original '{column}' del DataFrame.\")\n",
    "    df = df.drop(columns=[column])\n",
    "    \n",
    "    print(f\"Expansión de la columna '{column}' completada exitosamente.\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# One-hot encode de columnas booleanas utilizando Cython\n",
    "def boolean_features_ohe(df, columns_to_encode=['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2']):\n",
    "    \"\"\"\n",
    "    Realiza one-hot encoding en columnas booleanas especificadas utilizando una función optimizada con Cython.\n",
    "    Además, muestra el progreso del procesamiento utilizando tqdm y añade comentarios explicativos.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame original que contiene las columnas booleanas a codificar.\n",
    "    - columns_to_encode (list): Lista de nombres de columnas booleanas a codificar.\n",
    "\n",
    "    Retorna:\n",
    "    - df_expanded (pd.DataFrame): DataFrame con las nuevas columnas codificadas añadidas y las columnas booleanas originales eliminadas.\n",
    "    \"\"\"\n",
    "    # Copiar el DataFrame para evitar modificar el original\n",
    "    df = df.copy()\n",
    "    print(\"Inicio del proceso de one-hot encoding para las columnas booleanas especificadas.\")\n",
    "\n",
    "    # Paso 1: Encontrar todos los valores únicos en las columnas a codificar\n",
    "    unique_values_set = set()\n",
    "    print(\"Recopilando valores únicos de las columnas a codificar:\")\n",
    "    for col in tqdm(columns_to_encode, desc=\"Procesando columnas para valores únicos\"):\n",
    "        unique_vals_col = df[col].dropna().unique()\n",
    "        unique_values_set.update(unique_vals_col)\n",
    "    unique_values = sorted(unique_values_set)\n",
    "    print(f\"Valores únicos encontrados: {unique_values}\")\n",
    "\n",
    "    # Paso 2: Convertir las columnas a listas de listas para ser procesadas en Cython\n",
    "    list_data = []\n",
    "    print(\"Convirtiendo las columnas booleanas a listas de listas para Cython:\")\n",
    "    for col in tqdm(columns_to_encode, desc=\"Convertir columnas a listas\"):\n",
    "        column_list = df[col].astype(str).tolist()  # Mantener los valores como strings\n",
    "        list_data.append(column_list)\n",
    "    print(\"Conversión completada.\")\n",
    "\n",
    "    # Paso 3: Procesar los datos con la función optimizada en Cython\n",
    "    print(\"Realizando one-hot encoding utilizando la función optimizada en Cython:\")\n",
    "    ohe_result = boolean_features_ohe_cython(list_data, unique_values)\n",
    "    print(\"One-hot encoding completado.\")\n",
    "\n",
    "    # Paso 4: Convertir el resultado de Cython a un DataFrame, alineando el índice con df\n",
    "    print(\"Creando el DataFrame de columnas codificadas:\")\n",
    "    ohe_df = pd.DataFrame(ohe_result, columns=unique_values, index=df.index)\n",
    "    print(f\"DataFrame de one-hot encoding creado con {len(ohe_df.columns)} columnas y {ohe_df.shape[0]} filas.\")\n",
    "\n",
    "    # Paso 5: Concatenar las nuevas columnas codificadas al DataFrame original\n",
    "    print(\"Concatenando las columnas codificadas al DataFrame original:\")\n",
    "    df_expanded = pd.concat([df, ohe_df], axis=1)\n",
    "    print(f\"Concatenación completada. El DataFrame ahora tiene {df_expanded.shape[1]} columnas y {df_expanded.shape[0]} filas.\")\n",
    "\n",
    "    # Paso 6: Eliminar las columnas booleanas originales del DataFrame\n",
    "    print(\"Eliminando las columnas booleanas originales del DataFrame:\")\n",
    "    df_expanded.drop(columns=columns_to_encode, inplace=True)\n",
    "    print(f\"Columnas eliminadas: {columns_to_encode}\")\n",
    "\n",
    "    print(\"Proceso de one-hot encoding finalizado exitosamente.\\n\")\n",
    "\n",
    "    return df_expanded\n",
    "\n",
    "# Extensión de características temporales (día de la semana, momento del día, etc.) y festividades\n",
    "def time_features_extension(df):\n",
    "    \"\"\"\n",
    "    Procesa las características temporales del DataFrame y agrega nuevas columnas derivadas relacionadas con el tiempo y festividades.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convertir 'auction_time' de timestamp a una fecha legible\n",
    "    df['auction_time'] = pd.to_datetime(df['auction_time'], unit='s')\n",
    "\n",
    "    # Reemplazar NaN en 'timezone_offset' por 0\n",
    "    df['timezone_offset'] = df['timezone_offset'].fillna(0)\n",
    "\n",
    "    # Ajustar la hora según el 'timezone_offset' para obtener la hora local\n",
    "    df['auction_time_local'] = df.apply(\n",
    "        lambda row: row['auction_time'] + pd.DateOffset(hours=row['timezone_offset']), axis=1\n",
    "    )\n",
    "\n",
    "    # Crear la columna 'week_day' (1 para lunes, 7 para domingo)\n",
    "    df['week_day'] = df['auction_time_local'].dt.weekday + 1\n",
    "\n",
    "    # Crear la columna 'moment_of_the_day' (1 para temprano, 2 para tarde, 3 para noche)\n",
    "    df['moment_of_the_day'] = pd.cut(df['auction_time_local'].dt.hour, \n",
    "                                     bins=[0, 12, 18, 24], labels=[1, 2, 3], include_lowest=True, right=False)\n",
    "\n",
    "    # Eliminar las columnas originales 'auction_time', 'timezone_offset' y 'auction_time_local'\n",
    "    df.drop(columns=['auction_time', 'timezone_offset', 'auction_time_local'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Agrupación de edades en rangos numéricos\n",
    "def age_group(df, columna_edad):\n",
    "    \"\"\"\n",
    "    Agrupa las edades en rangos numéricos utilizando Cython para mejorar el rendimiento.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene la columna de edades.\n",
    "    - columna_edad (str): Nombre de la columna que contiene las edades.\n",
    "\n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame con la nueva columna 'age_group' que representa el rango de edad.\n",
    "    \"\"\"\n",
    "    # Convertir la columna de edad a una lista\n",
    "    edades = df[columna_edad].tolist()\n",
    "\n",
    "    # Usar la función Cythonizada para agrupar las edades\n",
    "    df['age_group'] = agrupar_edades_cython(edades)\n",
    "\n",
    "    # Eliminar la columna original de edades\n",
    "    df.drop(columns=[columna_edad], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Agrupo action_list_0 a auction_list_0\n",
    "def expand_action_list_0(df):\n",
    "    \"\"\"\n",
    "    Expande la columna 'action_list_0' en valores únicos y marca con 1 las columnas existentes o las crea si es necesario.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene la columna 'action_list_0' y otras columnas de listas ya expandidas.\n",
    "\n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame actualizado con las columnas de valores únicos de 'action_list_0'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convertir la columna 'action_list_0' y las columnas existentes a listas\n",
    "    action_list_0 = df['action_list_0'].tolist()\n",
    "    existing_columns = df.columns.tolist()\n",
    "    \n",
    "    # Inicializar la matriz actual\n",
    "    current_matrix = df.values.tolist()\n",
    "\n",
    "    # Llamar a la función Cythonizada\n",
    "    updated_matrix = expand_action_list_0_cython(action_list_0, existing_columns, current_matrix)\n",
    "\n",
    "    # Convertir la matriz actualizada de vuelta a un DataFrame\n",
    "    df_updated = pd.DataFrame(updated_matrix, columns=existing_columns)\n",
    "\n",
    "    # Eliminar la columna 'action_list_0'\n",
    "    df_updated.drop(columns=['action_list_0'], inplace=True)\n",
    "\n",
    "    return df_updated\n",
    "\n",
    "# Concateno las categorias de cada nivel\n",
    "def create_level_combination(df):\n",
    "    \"\"\"\n",
    "    Creates a new column 'level_combination' by concatenating the first three characters \n",
    "    of each 'action_categorical' level columns and removes the original level columns.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame containing the columns:\n",
    "      'action_categorical_0', 'action_categorical_1', 'action_categorical_2', \n",
    "      'action_categorical_3', 'action_categorical_4'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with the new 'level_combination' column and without the original level columns.\n",
    "    \"\"\"\n",
    "    level_columns = [\n",
    "        'action_categorical_0',\n",
    "        'action_categorical_1',\n",
    "        'action_categorical_2',\n",
    "        'action_categorical_3',\n",
    "        'action_categorical_4'\n",
    "    ]\n",
    "    df['level_combination'] = df[level_columns].astype(str).apply(\n",
    "        lambda x: ''.join([s[:3] for s in x]), axis=1\n",
    "    )\n",
    "    df.drop(columns=level_columns, inplace=True)\n",
    "    return df\n",
    "\n",
    "# Heigh x Width a columna\n",
    "def hxw_column(df):\n",
    "    \"\"\"\n",
    "    Crea una nueva columna 'hxw' multiplicando 'creative_height' y 'creative_width'.\n",
    "    Si alguno de los dos tiene un NaN, 'hxw' se establece en 0.\n",
    "    Elimina las columnas originales 'creative_height' y 'creative_width'.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene las columnas 'creative_height' y 'creative_width'.\n",
    "    \n",
    "    Retorna:\n",
    "    - pd.DataFrame: DataFrame con la nueva columna 'hxw' añadida y las columnas originales eliminadas.\n",
    "    \"\"\"\n",
    "    df['hxw'] = df['creative_height'] * df['creative_width']\n",
    "    df.loc[df['creative_height'].isna() | df['creative_width'].isna(), 'hxw'] = 0\n",
    "    df.drop(columns=['creative_height', 'creative_width'], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Gender to number\n",
    "def encode_gender(df):\n",
    "    \"\"\"\n",
    "    Reemplaza los valores de la columna 'gender' de la siguiente manera:\n",
    "    'f' -> 1, 'm' -> 2, 'o' -> 0 y NaN -> -1.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene la columna 'gender'.\n",
    "    \n",
    "    Retorna:\n",
    "    - pd.DataFrame: DataFrame con la columna 'gender' codificada.\n",
    "    \"\"\"\n",
    "    df['gender'] = df['gender'].map({'f': 1, 'm': 2, 'o': 0}).fillna(-1).astype(int)\n",
    "    return df\n",
    "\n",
    "# creative_categorical_11, creative_categorical_9 y creative_categorical_10 a dos columnas\n",
    "def creatives2unique(df):\n",
    "    \"\"\"\n",
    "    Crea o actualiza dos columnas en el DataFrame, una para cada valor único en las columnas\n",
    "    'creative_categorical_11', 'creative_categorical_10', y 'creative_categorical_9'.\n",
    "    Si las columnas ya existen, actualiza los valores a 1 donde ese valor aparece en alguna\n",
    "    de las tres columnas en esa fila.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene las columnas 'creative_categorical_11',\n",
    "                         'creative_categorical_10', y 'creative_categorical_9'.\n",
    "\n",
    "    Retorna:\n",
    "    - pd.DataFrame: DataFrame con las nuevas columnas añadidas o actualizadas.\n",
    "    \"\"\"\n",
    "    unique_values = {'65dcab89', '43c867fd'}\n",
    "    columns_to_check = [\n",
    "        'creative_categorical_11',\n",
    "        'creative_categorical_10',\n",
    "        'creative_categorical_9'\n",
    "    ]\n",
    "\n",
    "    for val in unique_values:\n",
    "        if val in df.columns:\n",
    "            # Si la columna ya existe, actualizamos los valores a 1 donde corresponde\n",
    "            df[val] = df[val] | df[columns_to_check].eq(val).any(axis=1).astype(int)\n",
    "        else:\n",
    "            # Si no existe, creamos la columna con 1 donde corresponde\n",
    "            df[val] = df[columns_to_check].eq(val).any(axis=1).astype(int)\n",
    "\n",
    "    # Eliminar las columnas originales\n",
    "    df.drop(columns=columns_to_check, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Procesamiento optimizado de un DataFrame\n",
    "def process_optimized(df):\n",
    "    \"\"\"\n",
    "    Aplica una serie de transformaciones al DataFrame utilizando una función Cython optimizada.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame a procesar.\n",
    "    \n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame procesado.\n",
    "    \"\"\"\n",
    "    # Definir el número total de pasos para la barra de progreso\n",
    "    total_steps = 13\n",
    "    \n",
    "    # Inicializar la barra de progreso\n",
    "    with tqdm(total=total_steps, desc=\"Procesando DataFrame\", unit=\"paso\") as pbar:\n",
    "        \n",
    "        print(\"Comenzando el procesamiento optimizado del DataFrame.\")\n",
    "        print(\"Eliminando columnas innecesarias.\")\n",
    "        df = df.drop('action_categorical_5', axis=1)\n",
    "        df = df.drop('auction_categorical_2', axis=1)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Expansión de columnas temporales\")\n",
    "        df = time_features_extension(df)\n",
    "        pbar.update(1)\n",
    "\n",
    "        df = age_group(df, 'auction_age')\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Agrupando columnas de nivel\")\n",
    "        df = create_level_combination(df)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Modificando columnas de genero\")\n",
    "        df = encode_gender(df)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        print(\"Modificando columna de video\")\n",
    "        df['has_video'] = df['has_video'].apply(lambda x: 1 if x == True else 0)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Juntando medidas\")\n",
    "        df = hxw_column(df)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Expansión de columnas booleanas.\")\n",
    "        df = boolean_features_ohe(df)\n",
    "        pbar.update(1)\n",
    "\n",
    "        print(\"Creando columnas de creatividad\")\n",
    "        df = creatives2unique(df)\n",
    "        pbar.update(1)\n",
    "        \n",
    "        columns_to_expand = ['auction_list_0', 'action_list_1']\n",
    "\n",
    "        # Bucle para realizar las operaciones\n",
    "        for col in columns_to_expand:\n",
    "            print(f\"Expansión de columnas de listas para {col}.\")\n",
    "            \n",
    "            # Crear la variable 'idx_position' con la última columna antes de la expansión\n",
    "            idx_position = df.columns.get_loc(df.columns[-1])\n",
    "            \n",
    "            # Expansión de la columna\n",
    "            df = expand_list_dummies_cython(df, col)\n",
    "            pbar.update(1)\n",
    "\n",
    "            if col == 'action_list_1':\n",
    "                df = expand_list_dummies_cython(df,'action_list_2')\n",
    "                pbar.update(1)\n",
    "            \n",
    "            if col == 'auction_list_0':\n",
    "                # Bucle para recorrer las columnas que empiezan con 'AND' o 'APL'\n",
    "                if 'AND-APL' not in df.columns:\n",
    "                    df['AND-APL'] = 0  # Inicializar la columna 'AND-APL'\n",
    "\n",
    "                for column in df.columns:\n",
    "                    if column.startswith('AND') or column.startswith('APL'):\n",
    "                        # Poner un 1 en 'AND-APL' si la columna actual tiene un 1 en esa fila\n",
    "                        df['AND-APL'] = df['AND-APL'] | df[column]\n",
    "\n",
    "                # Eliminar todas las columnas que empiezan con 'AND' o 'APL' excepto la columna 'AND-APL'\n",
    "                columns_to_drop = [column for column in df.columns if (column.startswith('AND') or column.startswith('APL')) and column != 'AND-APL']\n",
    "                df.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "                print(\"Complementamos con la columna 'action_list_0'\")\n",
    "                df = expand_action_list_0(df)\n",
    "                pbar.update(1)\n",
    "\n",
    "            # Crear la lista de columnas numéricas a partir de la siguiente columna después de 'idx'\n",
    "            categorical_num = df.iloc[:, idx_position:].select_dtypes(include=['number']).columns\n",
    "\n",
    "            # Eliminar columnas numéricas con menos de 1000 valores iguales a 1\n",
    "            for column in categorical_num:\n",
    "                if (df[column] == 1).sum() < 1000 and column != 'AND-APL':\n",
    "                    print(f\"Eliminando columna '{column}' por tener menos de 1000 valores iguales a 1.\")\n",
    "                    df.drop(column, axis=1, inplace=True)\n",
    "    \n",
    "        \n",
    "        print(\"Agrupando categorias poco frecuentes\")\n",
    "\n",
    "        categorical_str = df.select_dtypes(include=['object']).columns\n",
    "        categorical_str = categorical_str[categorical_str != 'device_id']\n",
    "        \n",
    "        categorical_num = df.select_dtypes(include=['number']).columns\n",
    "        \n",
    "        # Convertir a matriz bidimensional\n",
    "        data_matrix = df[categorical_str].values.tolist()\n",
    "        data_matrix_cython = [list(row) for row in data_matrix]\n",
    "        \n",
    "        # Llamar a la función Cythonizada\n",
    "        df_cython_data = agrupar_categorias_cython(\n",
    "            categorical_features=categorical_str.tolist(),\n",
    "            data=data_matrix_cython,\n",
    "            umbral=100  # Umbral de frecuencia para considerar una categoría como rara\n",
    "        )\n",
    "\n",
    "        # Reasignar los datos al DataFrame\n",
    "        for i, col in enumerate(categorical_str):\n",
    "            df[col] = [row[i] for row in df_cython_data]\n",
    "\n",
    "        \n",
    "        print(\"Tratamos los valores faltantes de manera diferenciada\")\n",
    "        # Manejar los valores faltantes de manera diferenciada\n",
    "        # Aplicar fillna de manera individual para cada columna de tipo 'str' (objetos)\n",
    "        for col in categorical_str:\n",
    "            df[col] = df[col].fillna('Desconocido')\n",
    "\n",
    "        # Aplicar fillna de manera individual para cada columna numérica\n",
    "        for col in categorical_num:\n",
    "            df[col] = df[col].fillna(0)\n",
    "\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Función para ajustar el tipo de datos de una columna para que Dask tome Nan como valor válido\n",
    "def adjust_dtype(dtype):\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        return 'Int64'\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return 'float64'\n",
    "    elif pd.api.types.is_bool_dtype(dtype):\n",
    "        return 'boolean'\n",
    "    else:\n",
    "        return 'object'\n",
    "    \n",
    "# Procesamiento de datos con Dask\n",
    "def process_data_with_dask(df, npartitions=10, meta_df=None):\n",
    "    \"\"\"\n",
    "    Procesa un DataFrame utilizando Dask para distribuir el trabajo en varias particiones.\n",
    "    Aplica la función process_optimized a cada partición del DataFrame.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): El DataFrame de pandas a procesar.\n",
    "    - npartitions (int): Número de particiones en las que se dividirá el DataFrame para su procesamiento.\n",
    "\n",
    "    Retorna:\n",
    "    - final_df (pd.DataFrame): El DataFrame procesado y concatenado.\n",
    "    \"\"\"\n",
    "    # Convertir el DataFrame de pandas a Dask con el número de particiones especificado\n",
    "    dask_df = dd.from_pandas(df, npartitions=npartitions)\n",
    "    \n",
    "    # Crear el meta DataFrame con tipos ajustados\n",
    "    meta = df.head(0).copy()\n",
    "    for col in meta.columns:\n",
    "        meta[col] = meta[col].astype(adjust_dtype(df[col].dtype))\n",
    "        \n",
    "    # Aplicar la función con map_partitions y especificar el meta\n",
    "    dask_df = dask_df.map_partitions(\n",
    "        lambda df_partition: process_optimized(df_partition),\n",
    "        meta=meta\n",
    "    )\n",
    "\n",
    "    # Ejecutar el cálculo distribuido y convertir el resultado a pandas\n",
    "    final_df = dask_df.compute()\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# Función para calcular la estadística de Cramér's V\n",
    "def cramers_v(confusion_matrix):\n",
    "    \"\"\"\n",
    "    Calcula la estadística de Cramér's V para medir la asociación entre dos variables categóricas.\n",
    "\n",
    "    Parameters:\n",
    "    - confusion_matrix: Matriz de confusión (tabla de contingencia) entre dos variables.\n",
    "\n",
    "    Returns:\n",
    "    - Cramér's V: Valor entre 0 y 1 que indica la fuerza de la asociación.\n",
    "    \"\"\"\n",
    "    # Calcular el estadístico chi-cuadrado\n",
    "    chi2 = chi2_contingency(confusion_matrix, correction=False)[0]\n",
    "    # Número total de observaciones\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    # Obtener el número de filas y columnas de la matriz de confusión\n",
    "    r, k = confusion_matrix.shape\n",
    "    if min(r, k) == 1:\n",
    "        return np.nan  # Evitar dividir por cero\n",
    "    # Calcular Cramér's V\n",
    "    return np.sqrt(chi2 / (n * (min(r, k) - 1)))\n",
    "\n",
    "def handle_none(value):\n",
    "    return 'None' if value is None else str(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenamos datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_21 = pd.read_csv('data/ctr_21.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_20 = pd.read_csv('data/ctr_20.csv')\n",
    "train_data_19 = pd.read_csv('data/ctr_19.csv')\n",
    "train_data_18 = pd.read_csv('data/ctr_18.csv')\n",
    "train_data_17 = pd.read_csv('data/ctr_17.csv')\n",
    "train_data_16 = pd.read_csv('data/ctr_16.csv')\n",
    "train_data_15 = pd.read_csv('data/ctr_15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplementary_datasets = [\n",
    "    ('1', train_data_20),\n",
    "    ('2', train_data_19),\n",
    "    ('3', train_data_18),\n",
    "    ('4', train_data_17),\n",
    "    ('5', train_data_16),\n",
    "    ('6', train_data_15)\n",
    "]\n",
    "\n",
    "train_data = train_data_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nombre, dataset in supplementary_datasets:\n",
    "    print(f\"\\nProcesando dataset {nombre}/{len(supplementary_datasets)}\")\n",
    "    train_data, categorias_desconocidas, categorias_raras = augment_train_data(train_data, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape[0])\n",
    "\n",
    "print(train_data_20.shape[0] + train_data_19.shape[0] + train_data_18.shape[0] + train_data_17.shape[0] + train_data_16.shape[0] + train_data_15.shape[0] + train_data_21.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('train_data_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingenieria de atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_data_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/ctr_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorias numericas\n",
    "numeric_columns = train_data.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Categorias categóricas\n",
    "categorical_features = train_data.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribucion de 'Label' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir la cantidad de filas del dataset combinado\n",
    "print(f\"Cantidad de filas en el dataset combinado: {train_data.shape[0]}\")\n",
    "print(f\"Cantidad de columnas en el dataset combinado: {train_data.shape[1]}\")\n",
    "\n",
    "# Ver porcentaje de clics vs no clics en la columna Label\n",
    "label_counts = train_data['Label'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPorcentaje de clics (1) y no clics (0):\")\n",
    "print(label_counts)\n",
    "\n",
    "# Cantidad de clics (1) y no clics (0)\n",
    "label_counts_abs = train_data['Label'].value_counts()\n",
    "print(\"\\nCantidad de clics (1) y no clics (0):\")\n",
    "print(label_counts_abs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlaciones con 'Label'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis with 'Label'\n",
    "numeric_data = train_data[numeric_columns]\n",
    "correlation_with_label = numeric_data.corr()['Label'].sort_values(key=abs, ascending=False)\n",
    "\n",
    "# Print correlations with 'Label'\n",
    "print(\"Correlation of features with 'Label' (sorted by absolute value):\")\n",
    "for feature, corr in correlation_with_label.items():\n",
    "    if feature != 'Label':\n",
    "        print(f\"{feature}: {corr:.4f}\")\n",
    "\n",
    "# Visualize top correlations with 'Label'\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_correlations = correlation_with_label.drop('Label').abs()\n",
    "sns.barplot(x=top_correlations.values, y=top_correlations.index)\n",
    "plt.title('Numerical Features Correlated with Label')\n",
    "plt.xlabel('Absolute Correlation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cramer's V for each categorical feature with 'Label'\n",
    "cramer_v_results = {}\n",
    "for col in categorical_features:\n",
    "    confusion_matrix = pd.crosstab(train_data[col], train_data['Label'])\n",
    "    cramer_v_results[col] = cramers_v(confusion_matrix)\n",
    "\n",
    "# Sort results\n",
    "cramer_v_results = dict(sorted(cramer_v_results.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Visualize top Cramer's V results\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = dict(list(cramer_v_results.items())[:20])\n",
    "sns.barplot(x=list(top_features.values()), y=list(top_features.keys()))\n",
    "plt.title(\"Top 20 Categorical Features by Cramer's V with Label\")\n",
    "plt.xlabel(\"Cramer's V\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print results\n",
    "print(\"Cramer's V for categorical features with 'Label':\")\n",
    "for feature, v in top_features.items():\n",
    "    print(f\"{feature}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de datos faltantes (sin cambios)\n",
    "missing_data = train_data.isnull().sum() / len(train_data) * 100\n",
    "missing_data = missing_data[missing_data > 10].sort_values(ascending=False)  # Filtrar las columnas con más del 10% de datos faltantes\n",
    "\n",
    "# Visualizar datos faltantes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=missing_data.values, y=missing_data.index)\n",
    "plt.title('Percentage of Missing Data by Numerical Feature (>10%)')\n",
    "plt.xlabel('Percentage Missing')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"\\nPercentage of Missing Data by Feature (>10% missing):\")\n",
    "for feature, percentage in missing_data.items():\n",
    "    print(f\"{feature}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame de ejemplo\n",
    "data = {\n",
    "    'auction_boolean_0': ['47980dda', '43c867fd', None, '79ceee49', '47980dda', 'unknown'],\n",
    "    'auction_boolean_1': ['79ceee49', None, '79ceee49', None, '1', '0'],\n",
    "    'auction_boolean_2': ['65dcab89', None, '43c867fd', '65dcab89', '65dcab89', '43c867fd'],\n",
    "    'Label': [0, 1, 0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "# Crear el DataFrame\n",
    "train_data_combined = pd.DataFrame(data)\n",
    "\n",
    "# Seleccionar solo las columnas relevantes\n",
    "df = train_data_combined[['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2', 'Label']]\n",
    "\n",
    "print(\"DataFrame Original:\")\n",
    "print(df)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Convertir el DataFrame a una lista de listas\n",
    "data_list = df.values.tolist()\n",
    "\n",
    "# Definir columnas categóricas y columnas a excluir\n",
    "categorical_features = ['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2']\n",
    "columns_to_exclude = []  # No se excluye ninguna en este ejemplo\n",
    "\n",
    "# Aplicar la función de agrupación de categorías en Cython\n",
    "data_processed = agrupar_categorias_cython(categorical_features, data_list, umbral=2)\n",
    "\n",
    "# Convertir la lista de listas de nuevo a DataFrame para visualizar\n",
    "df_processed = pd.DataFrame(data_processed, columns=['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2', 'Label'])\n",
    "\n",
    "print(\"DataFrame después de Agrupar Categorías con Cython:\")\n",
    "print(df_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valores unicos y su frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train_data.columns:\n",
    "    if train_data[column].value_counts() < 1000:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario para almacenar los valores únicos de cada columna\n",
    "unique_values = {col: set(train_data[col].dropna().unique()) for col in categorical_features}\n",
    "\n",
    "# Diccionario para almacenar las columnas que tienen valores comunes\n",
    "common_columns = {}\n",
    "\n",
    "# Comparar las columnas entre sí para ver qué valores comparten\n",
    "for i in range(len(categorical_features)):\n",
    "    for j in range(i + 1, len(categorical_features)):\n",
    "        col1 = categorical_features[i]\n",
    "        col2 = categorical_features[j]\n",
    "        \n",
    "        # Ver los valores que se repiten entre las dos columnas\n",
    "        common_values = unique_values[col1].intersection(unique_values[col2])\n",
    "        \n",
    "        if common_values:\n",
    "            # Almacenar las columnas con valores comunes\n",
    "            if (col1, col2) not in common_columns:\n",
    "                common_columns[(col1, col2)] = common_values\n",
    "\n",
    "# Ver las columnas que tienen valores comunes y sus unique values\n",
    "for col_pair, common_vals in common_columns.items():\n",
    "    col1, col2 = col_pair\n",
    "    print(f\"\\nValores comunes entre {col1} y {col2}: {common_vals}\")\n",
    "    print(f\"Valores únicos de {col1}: {len(unique_values[col1])}\")\n",
    "    print(f\"Valores únicos de {col2}: {len(unique_values[col2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### action_categorical_5 (motivos para eliminarlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_to_check = '6bc0e29c'\n",
    "filtered_rows = test[test['action_categorical_5'] == value_to_check]\n",
    "\n",
    "# Verificar si el valor aparece en otras columnas categóricas\n",
    "for col in categorical_features:\n",
    "    if col != 'action_categorical_5':  # Evitar verificar la columna original\n",
    "        matching_rows = filtered_rows[filtered_rows[col] == value_to_check]\n",
    "        if not matching_rows.empty:\n",
    "            print(f\"El valor '{value_to_check}' también aparece en la columna '{col}'\")\n",
    "\n",
    "# Ver filas donde el valor aparece solo en action_categorical_5\n",
    "only_in_action_5 = filtered_rows[~filtered_rows[categorical_features].isin([value_to_check]).any(axis=1)]\n",
    "print(f\"Filas donde el valor '{value_to_check}' solo aparece en 'action_categorical_5':\")\n",
    "print(only_in_action_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_to_check = '79ceee49'\n",
    "filtered_rows = test[test['action_categorical_5'] == value_to_check]\n",
    "\n",
    "# Verificar si el valor aparece en otras columnas categóricas\n",
    "for col in categorical_features:\n",
    "    if col != 'action_categorical_5':  # Evitar verificar la columna original\n",
    "        matching_rows = filtered_rows[filtered_rows[col] == value_to_check]\n",
    "        if not matching_rows.empty:\n",
    "            print(f\"El valor '{value_to_check}' también aparece en la columna '{col}'\")\n",
    "\n",
    "# Ver filas donde el valor aparece solo en action_categorical_5\n",
    "only_in_action_5 = filtered_rows[~filtered_rows[categorical_features].isin([value_to_check]).any(axis=1)]\n",
    "print(f\"Filas donde el valor '{value_to_check}' solo aparece en 'action_categorical_5':\")\n",
    "print(only_in_action_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### auction_categorical_2 (motivos para eliminarlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show unique values of auction_categorical_2\n",
    "print(\"Unique values of 'auction_categorical_2':\")\n",
    "print(train_data['auction_categorical_2'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Levels features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_features = ['action_categorical_0', 'action_categorical_1', 'action_categorical_2', 'action_categorical_3', 'action_categorical_4']\n",
    "\n",
    "print(f\"Valores únicos y su frecuencia:\")\n",
    "for column in train_data.columns:\n",
    "    if column in level_features:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle para contar los NaNs en cada columna\n",
    "for column in level_features:\n",
    "    num_nans = train_data[column].isna().sum()\n",
    "    print(f\"Columna {column} tiene {num_nans} valores NaN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una columna con las combinaciones únicas de las 5 columnas pero tomando solo las dos primeras letras de cada valor\n",
    "truncated_cols = train_data[level_features].applymap(lambda x: x[:3])\n",
    "\n",
    "# Concatenar las columnas truncadas\n",
    "train_data['combination'] = truncated_cols.apply(lambda row: ''.join(row.values), axis=1)\n",
    "\n",
    "# Obtener las combinaciones únicas y sus frecuencias\n",
    "combination_counts = train_data['combination'].value_counts().reset_index()\n",
    "combination_counts.columns = ['Combination', 'Frequency']\n",
    "\n",
    "# Mostrar las primeras 10 combinaciones más comunes\n",
    "print(combination_counts.head(25))\n",
    "print(f\"De: {len(combination_counts)} combinaciones únicas.\")\n",
    "\n",
    "# Graficar las combinaciones más frecuentes\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_combinations = combination_counts.head(25)  # Mostrar las 25 combinaciones más comunes\n",
    "sns.barplot(x='Frequency', y='Combination', data=top_combinations)\n",
    "plt.title(\"Top 25 Combinations of Action Categorical Columns out of {} Unique Combinations\".format(len(combination_counts)))\n",
    "plt.xlabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinaciones menos comunes (< 10000 veces)\n",
    "print(combination_counts[combination_counts['Frequency'] <= 1])\n",
    "\n",
    "# Suma de todas las frecuencias de las combinaciones menos comunes\n",
    "print(f\"Suma de las frecuencias de las combinaciones menos comunes: {combination_counts[combination_counts['Frequency'] <= 1]['Frequency'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels_df = create_level_combination(train_data)\n",
    "\n",
    "levels_df['level_combination']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boolean Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_features = ['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2']\n",
    "\n",
    "for column in train_data.columns:\n",
    "    if column in boolean_features:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle para contar los NaNs en cada columna\n",
    "for column in boolean_features:\n",
    "    num_nans = train_data[column].isna().sum()\n",
    "    print(f\"Columna {column} tiene {num_nans} valores NaN.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una funcion para resolverl el problema de los valores booleanos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame de ejemplo\n",
    "data = {\n",
    "    'auction_boolean_0': ['47980dda', '43c867fd', None, '79ceee49'],\n",
    "    'auction_boolean_1': ['79ceee49', None, '79ceee49', None],\n",
    "    'auction_boolean_2': ['65dcab89', None, '43c867fd', '65dcab89'],\n",
    "    'Label': [0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "# Crear el DataFrame\n",
    "train_data = pd.DataFrame(data)\n",
    "\n",
    "# Imprimir el DataFrame original\n",
    "print(\"DataFrame Original:\")\n",
    "print(train_data)\n",
    "\n",
    "# Aplicar la función personalizada de one-hot encoding\n",
    "train_data_encoded = boolean_features_ohe(train_data)\n",
    "\n",
    "# Imprimir el DataFrame resultante\n",
    "print(\"\\nDataFrame después del One-Hot Encoding personalizado:\")\n",
    "print(train_data_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_features = ['auction_time', 'auction_age', 'timezone_offset']\n",
    "\n",
    "for column in train_data.columns:\n",
    "    if column in time_features:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle para contar los NaNs en cada columna\n",
    "for column in time_features:\n",
    "    num_nans = train_data[column].isna().sum()\n",
    "    print(f\"Columna {column} tiene {num_nans} valores NaN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegúrate de tener la columna 'auction_age' en tu train\n",
    "if 'auction_age' in train_data.columns:\n",
    "    # Obtener los valores únicos y su frecuencia\n",
    "    unique_ages = train_data['auction_age'].value_counts().sort_index()\n",
    "\n",
    "    # Imprimir cada edad y su frecuencia\n",
    "    for age, frequency in unique_ages.items():\n",
    "        print(f\"Edad: {age}, Frecuencia: {frequency}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funcion para crear atributos temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_features_extension\n",
    "\n",
    "data = {\n",
    "    'auction_time': [\n",
    "        1545676800,  # 24 de Diciembre de 2018, 22:00:00\n",
    "        1483228800,  # 31 de Diciembre de 2016, 23:00:00\n",
    "        1288396800,  # 30 de Octubre de 2010, 10:00:00\n",
    "        1412899200,  # 10 de Octubre de 2014, 15:00:00\n",
    "        1483574400   # 5 de Enero de 2017, 09:00:00\n",
    "    ],\n",
    "    'timezone_offset': [1, -2, 3, 0, 5]  # Diferentes zonas horarias\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = time_features_extension(df)\n",
    "\n",
    "# Imprimir el DataFrame resultante\n",
    "print(\"\\nDataFrame después de procesar el tiempo de subasta:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age_group\n",
    "\n",
    "data = {\n",
    "    'auction_age': [\n",
    "        -1, 15, 25, 35, 50, 65, 105, 80, 18, 99, 0, 579\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = age_group(df, 'auction_age')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quiero tomar las filas que sean de tipo label 1 y que me digan si esta cerca de una festividad o no con mi train_data\n",
    "\n",
    "# Crea un dataframe solo con las filas que tienen Label 1 a partir de train_data\n",
    "\n",
    "df = train_data[train_data['Label'] == 1]\n",
    "\n",
    "# Aplica la función time_features_extension al dataframe\n",
    "\n",
    "df = time_features_extension(df)\n",
    "\n",
    "# Imprimir las frecuencias de los valores que toman las columnas que se crearon con time_features_extension (week_day time_of_month moment_of_the_day  close_to_festivity)\n",
    "\n",
    "for column in ['week_day', 'time_of_month', 'moment_of_the_day']:\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos y su frecuencia:\")\n",
    "    print(df[column].value_counts())\n",
    "    print(\"-\" * 50)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica la función time_features_extension al dataframe\n",
    "\n",
    "test = time_features_extension(test)\n",
    "\n",
    "# Imprimir las frecuencias de los valores que toman las columnas que se crearon con time_features_extension (week_day time_of_month moment_of_the_day  close_to_festivity)\n",
    "\n",
    "for column in ['week_day', 'time_of_month', 'moment_of_the_day']:\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos y su frecuencia:\")\n",
    "    print(test[column].value_counts())\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna: action_list_0\n",
      "Valores únicos y su frecuencia:\n",
      "action_list_0\n",
      "IAB22-2    2205731\n",
      "IAB8-9     2028905\n",
      "IAB22      1884078\n",
      "IAB20-6    1410061\n",
      "IAB9-30     781816\n",
      "IAB20       145437\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: action_list_1\n",
      "Valores únicos y su frecuencia:\n",
      "action_list_1\n",
      "[-6779]                                                           353446\n",
      "[-6824, -6823]                                                    178293\n",
      "[-7195, 6902, 6903]                                               172602\n",
      "[6874, -7199, 7190, 6871]                                         153690\n",
      "[6874, 7190, -6871]                                               146716\n",
      "                                                                   ...  \n",
      "[-6118, -5736, -2606, -6613, 6902, -5559, -7033, -7195, -6780]         1\n",
      "[-6615, -6614, 6871, -7199]                                            1\n",
      "[-6614, -7190, 6615, -6454, -6871, 6618, -6874]                        1\n",
      "[6617, -5736, 6451, -6613, 6902, -5559, -7033, 7195]                   1\n",
      "[-6614, -6824, -7190, 6454, -6871, 6874, 6618]                         1\n",
      "Name: count, Length: 155636, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: action_list_2\n",
      "Valores únicos y su frecuencia:\n",
      "action_list_2\n",
      "[6871, -6543, -6544, -6946, -6820, -6822, -6823, -6825, -6454, -7111, -7112, -6218, -6220, -6224, -6865, -6226, -6867, -6615, -6618, -6875, 6876, -6621, -5603, -5604, -5605, -6119, -6866]                                                                           120675\n",
      "[-2560, -5902, -5613, -6800, -6547, -6548, 6938, -7195, -7196, -6309, -6451, -5559, -5560, -6848, -6849, -6850, -5576, -5577, -5578, -5579, -7126, -7127, -6616, -6617, -6620, -5469, -5470, -5471, -6118, -7143, -6125, -6770, -6774, -6775, -6904, -6905, -6780]    116454\n",
      "[-5902, -5613, -6800, -6547, -6548, 6938, -7195, -7196, -6309, -6451, -5559, -5560, -6848, -6849, -6850, -5576, -5577, -5578, -5579, -7126, -7127, -6616, -6617, -6620, -5469, -5470, -5471, -6118, -7143, -6125, -6770, -6774, -6775, -6904, -6905, -6780]           110686\n",
      "[-2560, -5902, -5613, -6800, -6547, -6548, -6938, -7195, 7196, -6309, -6451, -5559, -5560, -6848, -6849, -6850, -5576, -5577, -5578, -5579, -7126, -7127, -6616, -6617, -6620, -5469, -5470, -5471, -6118, -6125, -6770, -6774, -6775, -6904, -6905, -6780]            72178\n",
      "[-5603, -5604, -5605, -6217, -6219, -6223, 6226]                                                                                                                                                                                                                       71789\n",
      "                                                                                                                                                                                                                                                                       ...  \n",
      "[-5902, -6800, -6547, -6548, -6938, -7195, -7196, -6309, -5559, -5560, -6848, -6849, 6850, -5576, -5577, -5578, -7126, -7127, -6620, -5469, -5470, -5471, -6118, -7143, -6125, -6770, 6774, -6775, -6904]                                                                  1\n",
      "[-6871, -6543, 6544, -6929, -7190, -6946, -6823, -7111, -7112, -6218, -6220, -6224, -6865, -6866, -6615, -6618, -6875, -6876, -7263, -7264, -7265, -5603, -5604, -5605, -6119, -6226]                                                                                      1\n",
      "[-5902, -5613, -6800, -6547, -6548, 6938, -7195, -7196, -6309, -5559, -5560, -6848, -6849, -6850, -5576, -5577, -5578, -5579, -7126, -7127, -5469, -5470, -5471, -7143, -6125, 6770, -6774, -6775, -6904, -6905, -6780]                                                    1\n",
      "[-5902, -5613, -6800, -6547, -6548, 6938, -7195, -7196, -6309, -5559, -5560, -6849, -6850, -5576, -5577, -5579, -7127, -6620, -5469, -6118, -7143, -6125, -6770, -6904, -6905, -6780]                                                                                      1\n",
      "[-5577, -5579, -6800, -6547, -6548, -6774, -6616, -6905, 7196, -5471]                                                                                                                                                                                                      1\n",
      "Name: count, Length: 625353, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: auction_list_0\n",
      "Valores únicos y su frecuencia:\n",
      "auction_list_0\n",
      "[\"IAB3\",\"utilities\"]                                                                                                                                                                                                                                                                                                                        1053587\n",
      "[\"IAB3\",\"productivity\"]                                                                                                                                                                                                                                                                                                                      667649\n",
      "[\"social_networking\",\"IAB24\",\"lifestyle\",\"IAB14\"]                                                                                                                                                                                                                                                                                            597943\n",
      "[\"social_networking\",\"IAB24\"]                                                                                                                                                                                                                                                                                                                477676\n",
      "[\"IAB3\"]                                                                                                                                                                                                                                                                                                                                     340092\n",
      "                                                                                                                                                                                                                                                                                                                                             ...   \n",
      "[\"IAB19\",\"IAB9\",\"IAB19-29\",\"IAB9-23\",\"IAB9-22\",\"IAB9-21\",\"IAB9-20\",\"IAB9-27\",\"IAB9-26\",\"IAB9-25\",\"IAB9-24\",\"IAB9-29\",\"IAB9-28\",\"IAB9-8\",\"IAB9-9\",\"IAB9-1\",\"IAB9-2\",\"IAB9-3\",\"IAB9-4\",\"IAB9-5\",\"IAB9-6\",\"IAB9-7\",\"IAB9-30\",\"IAB9-31\",\"IAB9-18\",\"IAB9-19\",\"IAB9-12\",\"IAB9-13\",\"IAB9-10\",\"IAB9-11\",\"IAB9-16\",\"IAB9-17\",\"IAB9-14\",\"IAB9-15\"]          1\n",
      "[\"IAB18\",\"IAB19\",\"IAB19-3\",\"IAB2\",\"IAB15\",\"IAB1\",\"IAB19-30\",\"IAB3\",\"IAB19-18\",\"IAB19-19\",\"IAB6\",\"IAB9\",\"IAB9-1\",\"IAB4\",\"IAB5\"]                                                                                                                                                                                                                    1\n",
      "[\"IAB12\",\"IAB18\"]                                                                                                                                                                                                                                                                                                                                 1\n",
      "[\"IAB19\",\"IAB4-5\",\"IAB19-18\",\"IAB1\",\"IAB19-30\",\"IAB3\",\"IAB4\",\"IAB22\",\"IAB6\",\"IAB9-1\",\"IAB9\",\"IAB19-19\"]                                                                                                                                                                                                                                           1\n",
      "[\"IAB19\",\"IAB22\",\"IAB9-25\",\"IAB10\",\"IAB17\",\"IAB14\",\"IAB19-18\",\"IAB1\",\"IAB19-30\",\"IAB5-15\",\"IAB10-3\",\"IAB19-19\",\"IAB17-35\",\"IAB9\",\"IAB5\",\"IAB9-3\",\"IAB1-5\",\"IAB9-5\",\"IAB3\",\"IAB9-7\"]                                                                                                                                                               1\n",
      "Name: count, Length: 443, dtype: int64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "list_features = ['auction_list_0', 'action_list_1', 'action_list_2', 'action_list_0']\n",
    "\n",
    "for column in train_data.columns:\n",
    "    if column in list_features:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle para contar los NaNs en cada columna\n",
    "for column in list_features:\n",
    "    num_nans = train_data[column].isna().sum()\n",
    "    print(f\"Columna {column} tiene {num_nans} valores NaN.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funcion para hacer columnas de listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame de prueba más pequeño\n",
    "data = {\n",
    "    'auction_list_0': ['[\"IAB3\",\"utilities\", \"IAB22-2\"]', '[\"IAB19\",\"IAB4-5\", \"IAB8-9\"]', None, '[\"IAB3\",\"utilities\", \"IAB22-2\"]'],\n",
    "    'action_list_1': ['[-6779]', '[-6824, -6823]', None, '[-6824, -6823]' ],\n",
    "    'action_list_2': ['[6871, -6543, -6544]', '[-2560, -5902]', '[-6779]', None],\n",
    "    'action_list_0': ['IAB8-9', 'IAB22-2', 'IAB9-30', 'IAB9-30']\n",
    "}\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print('Antes de utilizar la función expand_list_dummies_cython:')\n",
    "print(df)\n",
    "\n",
    "# Aplicar la función expand_list_dummies_cython en 'auction_list_0', 'action_list_1' y 'action_list_2'\n",
    "# Aquí se asume que ya has definido y aplicado `expand_list_dummies_cython`\n",
    "df = expand_list_dummies_cython(df, 'auction_list_0')\n",
    "df = expand_list_dummies_cython(df, 'action_list_1')\n",
    "df = expand_list_dummies_cython(df, 'action_list_2')\n",
    "\n",
    "print('Antes de utilizar la función expand_action_list_0:')\n",
    "print(df)\n",
    "\n",
    "print()\n",
    "\n",
    "print('Después de utilizar la función expand_action_list_0:')\n",
    "df = expand_action_list_0(df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pixels features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna: creative_height\n",
      "Valores únicos y su frecuencia:\n",
      "creative_height\n",
      "50.0      4674216\n",
      "250.0     1935919\n",
      "480.0      406386\n",
      "320.0       74053\n",
      "90.0        49704\n",
      "768.0        6697\n",
      "1024.0       4781\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: creative_width\n",
      "Valores únicos y su frecuencia:\n",
      "creative_width\n",
      "320.0     5080520\n",
      "300.0     1936001\n",
      "480.0       74053\n",
      "728.0       49704\n",
      "1024.0       6697\n",
      "768.0        4781\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pixels_features = ['creative_height', 'creative_width']\n",
    "\n",
    "for column in train_data.columns:\n",
    "    if column in pixels_features:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle para contar los NaNs en cada columna\n",
    "for column in pixels_features:\n",
    "    num_nans = train_data[column].isna().sum()\n",
    "    print(f\"Columna {column} tiene {num_nans} valores NaN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar las filas donde ambas columnas tienen valores NaN\n",
    "nans_both_columns = train_data[pixels_features].isna().all(axis=1).sum()\n",
    "\n",
    "print(f\"El número de filas donde ambas columnas tienen valores NaN es: {nans_both_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volver a filtrar las filas donde ambas columnas son NaN\n",
    "filtered_rows = train_data[train_data['creative_height'].isna() & train_data['creative_width'].isna()]\n",
    "\n",
    "# Calcular el porcentaje de cada valor de 'Label' en las filas donde ambas columnas tienen valores NaN\n",
    "label_counts = filtered_rows['Label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Mostrar los resultados\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar las filas donde tanto creative_height como creative_width son NaN\n",
    "subset_nan = train_data[train_data['creative_height'].isna() & train_data['creative_width'].isna()]\n",
    "\n",
    "# Recorrer todas las columnas y mostrar particularidades en las 1,304,272 filas\n",
    "for column in train_data.columns:\n",
    "    unique_values = subset_nan[column].nunique()\n",
    "    num_nans = subset_nan[column].isna().sum()\n",
    "    \n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos: {unique_values}\")\n",
    "    print(f\"Cantidad de NaNs: {num_nans}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores unicos de la variable categorica auction_categorical_2\n",
    "unique_values = train_data['auction_categorical_2'].unique()\n",
    "\n",
    "unique_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creative features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = [\n",
    "    'creative_categorical_11',\n",
    "    'creative_categorical_10',\n",
    "    'creative_categorical_9'\n",
    "]\n",
    "unique_vals = {'65dcab89', '43c867fd'}\n",
    "\n",
    "mask = train_data[columns_to_check].apply(lambda row: set(row).issubset(unique_vals) and len(set(row)) == 1, axis=1)\n",
    "\n",
    "train_data[mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No podemos eliminar las columnas pero si podemos crear dos nuevas que tomen los nombres de los dos valores unicos de estas columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auction categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_id  = [\"auction_categorical_0\", \"auction_categorical_1\", \"auction_categorical_7\", \"auction_categorical_8\", \"auction_categorical_9\", \"auction_categorical_11\"]\n",
    "\n",
    "auction_categorical_variable = [\"auction_categorical_2\", \"auction_categorical_3\", \"auction_categorical_4\", \"auction_categorical_5\", \"auction_categorical_6\", \"auction_categorical_10\", \"auction_categorical_12\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train_data.columns:\n",
    "    if column in entity_id:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in train_data.columns:\n",
    "    if column in auction_categorical_variable:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis: Test vs Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En test pero no en train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_exclude = ['auction_list_0', 'action_list_1', 'action_list_2', 'auction_time', 'auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2']\n",
    "categorical_features_ = [col for col in categorical_features if col not in columns_to_exclude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un diccionario para almacenar las categorías desconocidas por columna\n",
    "categorias_desconocidas = {}\n",
    "\n",
    "# Iterar a través de cada columna categórica\n",
    "for columna in categorical_features_:\n",
    "    # Obtener las categorías únicas en el conjunto de entrenamiento\n",
    "    categorias_train = set(train_data[columna].dropna().unique())\n",
    "    \n",
    "    # Obtener las categorías únicas en el conjunto de prueba\n",
    "    categorias_test = set(test[columna].dropna().unique())\n",
    "    \n",
    "    # Identificar las categorías en test que no están en train\n",
    "    desconocidas = categorias_test - categorias_train\n",
    "    \n",
    "    # Almacenar las categorías desconocidas en el diccionario como una lista\n",
    "    categorias_desconocidas[columna] = list(desconocidas)\n",
    "\n",
    "    # Inicializamos una máscara booleana que será True si la fila tiene una categoría desconocida en cualquier columna\n",
    "mask = pd.Series([False] * len(test))\n",
    "\n",
    "# Iteramos a través de cada columna categórica\n",
    "for columna in categorias_desconocidas:\n",
    "    # Verificamos si el valor en la columna está dentro de las categorías desconocidas\n",
    "    mask = mask | test[columna].isin(categorias_desconocidas[columna])\n",
    "\n",
    "# Contamos el número de filas donde al menos una categoría es desconocida\n",
    "num_filas_desconocidas = mask.sum()\n",
    "\n",
    "print(f\"Cantidad de filas con al menos una categoría desconocida (existe en test y no en train): {num_filas_desconocidas} ({num_filas_desconocidas / len(test) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En train pero no en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un diccionario para almacenar las categorías desconocidas por columna\n",
    "categorias_conocidas = {}\n",
    "\n",
    "# Iterar a través de cada columna categórica\n",
    "for columna in categorical_features_:\n",
    "    # Obtener las categorías únicas en el conjunto de entrenamiento\n",
    "    categorias_test = set(test[columna].dropna().unique())\n",
    "    \n",
    "    # Obtener las categorías únicas en el conjunto de prueba\n",
    "    categorias_train = set(train_data[columna].dropna().unique())\n",
    "    \n",
    "    # Identificar las categorías en test que no están en train\n",
    "    conocidas = categorias_train - categorias_test\n",
    "    \n",
    "    # Almacenar las categorías desconocidas en el diccionario como una lista\n",
    "    categorias_conocidas[columna] = list(conocidas)\n",
    "\n",
    "# Inicializamos una máscara booleana que será True si la fila tiene una categoría desconocida en cualquier columna\n",
    "mask = pd.Series([False] * len(test))\n",
    "\n",
    "# Iteramos a través de cada columna categórica\n",
    "for columna in categorias_conocidas:\n",
    "    # Verificamos si el valor en la columna está dentro de las categorías desconocidas\n",
    "    mask = mask | train_data[columna].isin(categorias_conocidas[columna])\n",
    "\n",
    "# Contamos el número de filas donde al menos una categoría es desconocida\n",
    "num_filas_conocidas = mask.sum()\n",
    "\n",
    "print(f\"Cantidad de filas con al menos una categoría que no existe en test: {num_filas_conocidas} ({num_filas_conocidas / len(train_data) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_combined, _ = train_test_split(\n",
    "    train_data, \n",
    "    train_size=50000, \n",
    "    random_state=random_state, \n",
    "    stratify=train_data['Label']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proporción en el conjunto de entrenamiento:\n",
      "Label\n",
      "0    0.987689\n",
      "1    0.012311\n",
      "Name: proportion, dtype: float64\n",
      "Proporción en el conjunto de validación:\n",
      "Label\n",
      "0    0.9876\n",
      "1    0.0124\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1233"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dividir los datos en entrenamiento y validación con estratificación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data_combined.drop(columns='Label'),  # Características\n",
    "    train_data_combined['Label'],               # Variable objetivo\n",
    "    test_size=0.1,                              # 10% para validación\n",
    "    stratify=train_data_combined['Label'],      # Estratificación basada en la variable objetivo\n",
    "    random_state=random_state                   # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "# Verificar la proporción de clases en el conjunto de entrenamiento y validación\n",
    "print(\"Proporción en el conjunto de entrenamiento:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"Proporción en el conjunto de validación:\")\n",
    "print(y_val.value_counts(normalize=True))\n",
    "\n",
    "del train_data_combined, train_data, y_val, X_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b781193ad7e0438a863666f7ca2874b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando DataFrame:   0%|          | 0/13 [00:00<?, ?paso/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando el procesamiento optimizado del DataFrame.\n",
      "Eliminando columnas innecesarias.\n",
      "Expansión de columnas temporales\n",
      "Agrupando columnas de nivel\n",
      "Modificando columnas de genero\n",
      "Modificando columna de video\n",
      "Juntando medidas\n",
      "Expansión de columnas booleanas.\n",
      "Inicio del proceso de one-hot encoding para las columnas booleanas especificadas.\n",
      "Recopilando valores únicos de las columnas a codificar:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350b442095674085880d8b47e5a3ff32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Procesando columnas para valores únicos:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores únicos encontrados: ['43c867fd', '47980dda', '65dcab89', '79ceee49']\n",
      "Convirtiendo las columnas booleanas a listas de listas para Cython:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9fdfd439ad0471b8944ef8bff2cdffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Convertir columnas a listas:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversión completada.\n",
      "Realizando one-hot encoding utilizando la función optimizada en Cython:\n",
      "One-hot encoding completado.\n",
      "Creando el DataFrame de columnas codificadas:\n",
      "DataFrame de one-hot encoding creado con 4 columnas y 45000 filas.\n",
      "Concatenando las columnas codificadas al DataFrame original:\n",
      "Concatenación completada. El DataFrame ahora tiene 48 columnas y 45000 filas.\n",
      "Eliminando las columnas booleanas originales del DataFrame:\n",
      "Columnas eliminadas: ['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2']\n",
      "Proceso de one-hot encoding finalizado exitosamente.\n",
      "\n",
      "Creando columnas de creatividad\n",
      "Expansión de columnas de listas para auction_list_0.\n",
      "Comenzando la expansión de la columna: 'auction_list_0'\n",
      "Reemplazando NaN en la columna 'auction_list_0' por listas vacías.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643f1ba7b10141ef97bf9e2faf669d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codificación completada. 129 categorías únicas encontradas.\n",
      "Concatenando las columnas binarias al DataFrame original.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fec13fa11104b90938ec1f9181f77be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Concatenando columnas binarias:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminando la columna original 'auction_list_0' del DataFrame.\n",
      "Expansión de la columna 'auction_list_0' completada exitosamente.\n",
      "\n",
      "Complementamos con la columna 'action_list_0'\n",
      "Eliminando columna 'IAB1-1' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB1-2' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB1-3' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB1-4' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB1-5' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB1-7' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB10-1' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB10-3' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB10-5' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB10-7' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB12-1' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB12-2' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB13' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB14-1' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB14-3' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB15' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB15-10' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB16' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB17-1' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB17-12' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB17-44' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB19' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB19-14' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB19-17' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB19-18' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB19-19' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB19-29' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB19-30' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB19-5' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB19-6' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB2' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB20-3' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB21' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB21-3' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB5-1' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB5-8' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB6' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB6-6' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB6-9' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-1' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-10' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-11' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-12' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-13' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-14' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-15' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-16' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-17' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-18' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-19' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-2' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-20' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-21' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-22' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-23' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-24' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-25' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-26' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-27' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-28' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-29' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-3' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-30' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-31' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-32' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-33' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-34' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-35' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-36' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-37' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-38' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-39' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-4' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-40' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-41' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-42' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-43' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-44' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-45' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-5' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-6' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-7' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-8' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB7-9' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB8' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB9-1' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB9-2' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB9-24' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB9-25' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB9-26' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB9-5' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB9-7' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'IAB9-8' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'books' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'business' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'education' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'finance' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'healthcare_and_fitness' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'medical' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'music' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'navigation' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'reference' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'travel' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna 'weather' por tener menos de 1000 valores iguales a 1.\n",
      "Expansión de columnas de listas para action_list_1.\n",
      "Comenzando la expansión de la columna: 'action_list_1'\n",
      "Reemplazando NaN en la columna 'action_list_1' por listas vacías.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4f00a35c73444f82504740efeea9eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codificación completada. 58 categorías únicas encontradas.\n",
      "Concatenando las columnas binarias al DataFrame original.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7129b1fa02f459282f5ed5b4baddbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Concatenando columnas binarias:   0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminando la columna original 'action_list_1' del DataFrame.\n",
      "Expansión de la columna 'action_list_1' completada exitosamente.\n",
      "\n",
      "Comenzando la expansión de la columna: 'action_list_2'\n",
      "Reemplazando NaN en la columna 'action_list_2' por listas vacías.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc38932eb3944d3aca0ec5eb1422ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codificación completada. 145 categorías únicas encontradas.\n",
      "Concatenando las columnas binarias al DataFrame original.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd0239ff16e41f69ed8640de77a24db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Concatenando columnas binarias:   0%|          | 0/145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminando la columna original 'action_list_2' del DataFrame.\n",
      "Expansión de la columna 'action_list_2' completada exitosamente.\n",
      "\n",
      "Eliminando columna '2606' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '5559' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '5577' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '5736' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6118' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6119' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6451' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6613' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6614' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6616' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6779' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6780' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6824' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '7033' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '7194' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '7199' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '-7262' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '2560' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '5469' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '5470' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '5471' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '5560' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '5576' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '5603' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '5604' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '5605' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '5613' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '5902' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6125' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6217' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6218' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6219' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6220' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6223' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6224' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6226' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6309' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6543' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6548' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6620' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6621' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6770' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6774' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6775' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6820' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6825' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6848' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6849' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6850' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6865' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6866' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6867' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '6905' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '7111' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '7112' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '7126' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '7127' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '7263' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '7264' por tener menos de 1000 valores iguales a 1.\n",
      "Eliminando columna '7265' por tener menos de 1000 valores iguales a 1.\n",
      "Agrupando categorias poco frecuentes\n",
      "Tratamos los valores faltantes de manera diferenciada\n"
     ]
    }
   ],
   "source": [
    "X_train = process_optimized(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['action_categorical_6',\n",
       " 'action_categorical_7',\n",
       " 'auction_bidfloor',\n",
       " 'auction_categorical_0',\n",
       " 'auction_categorical_1',\n",
       " 'auction_categorical_10',\n",
       " 'auction_categorical_11',\n",
       " 'auction_categorical_12',\n",
       " 'auction_categorical_3',\n",
       " 'auction_categorical_4',\n",
       " 'auction_categorical_5',\n",
       " 'auction_categorical_6',\n",
       " 'auction_categorical_7',\n",
       " 'auction_categorical_8',\n",
       " 'auction_categorical_9',\n",
       " 'creative_categorical_0',\n",
       " 'creative_categorical_1',\n",
       " 'creative_categorical_12',\n",
       " 'creative_categorical_2',\n",
       " 'creative_categorical_3',\n",
       " 'creative_categorical_4',\n",
       " 'creative_categorical_5',\n",
       " 'creative_categorical_6',\n",
       " 'creative_categorical_7',\n",
       " 'creative_categorical_8',\n",
       " 'device_id',\n",
       " 'device_id_type',\n",
       " 'gender',\n",
       " 'has_video',\n",
       " 'week_day',\n",
       " 'moment_of_the_day',\n",
       " 'age_group',\n",
       " 'level_combination',\n",
       " 'hxw',\n",
       " '43c867fd',\n",
       " '47980dda',\n",
       " '65dcab89',\n",
       " '79ceee49',\n",
       " 'IAB-5',\n",
       " 'IAB1',\n",
       " 'IAB1-6',\n",
       " 'IAB12',\n",
       " 'IAB14',\n",
       " 'IAB17',\n",
       " 'IAB18',\n",
       " 'IAB20',\n",
       " 'IAB22',\n",
       " 'IAB24',\n",
       " 'IAB3',\n",
       " 'IAB5',\n",
       " 'IAB9',\n",
       " 'IAB9-23',\n",
       " 'IAB9-30',\n",
       " 'entertainment',\n",
       " 'games',\n",
       " 'lifestyle',\n",
       " 'news',\n",
       " 'photography',\n",
       " 'productivity',\n",
       " 'social_networking',\n",
       " 'sports',\n",
       " 'utilities',\n",
       " 'AND-APL',\n",
       " 'IAB20-6',\n",
       " 'IAB8-9',\n",
       " 'IAB22-2',\n",
       " '-2606',\n",
       " '-5559',\n",
       " '-5577',\n",
       " '-5578',\n",
       " '-5579',\n",
       " '-5736',\n",
       " '-6118',\n",
       " '-6119',\n",
       " '-6451',\n",
       " '-6454',\n",
       " '-6613',\n",
       " '-6614',\n",
       " '-6615',\n",
       " '-6616',\n",
       " '-6617',\n",
       " '-6618',\n",
       " '-6779',\n",
       " '-6780',\n",
       " '-6823',\n",
       " '-6824',\n",
       " '-6871',\n",
       " '-6874',\n",
       " '-6902',\n",
       " '-6903',\n",
       " '-7033',\n",
       " '-7190',\n",
       " '-7194',\n",
       " '-7195',\n",
       " '-7199',\n",
       " '5578',\n",
       " '5579',\n",
       " '6454',\n",
       " '6615',\n",
       " '6617',\n",
       " '6618',\n",
       " '6823',\n",
       " '6871',\n",
       " '6874',\n",
       " '6902',\n",
       " '6903',\n",
       " '7190',\n",
       " '7195',\n",
       " '-2560',\n",
       " '-5469',\n",
       " '-5470',\n",
       " '-5471',\n",
       " '-5560',\n",
       " '-5576',\n",
       " '-5603',\n",
       " '-5604',\n",
       " '-5605',\n",
       " '-5613',\n",
       " '-5902',\n",
       " '-6125',\n",
       " '-6217',\n",
       " '-6218',\n",
       " '-6219',\n",
       " '-6220',\n",
       " '-6223',\n",
       " '-6224',\n",
       " '-6226',\n",
       " '-6309',\n",
       " '-6543',\n",
       " '-6544',\n",
       " '-6547',\n",
       " '-6548',\n",
       " '-6620',\n",
       " '-6621',\n",
       " '-6770',\n",
       " '-6774',\n",
       " '-6775',\n",
       " '-6800',\n",
       " '-6820',\n",
       " '-6822',\n",
       " '-6825',\n",
       " '-6848',\n",
       " '-6849',\n",
       " '-6850',\n",
       " '-6865',\n",
       " '-6866',\n",
       " '-6867',\n",
       " '-6875',\n",
       " '-6876',\n",
       " '-6904',\n",
       " '-6905',\n",
       " '-6929',\n",
       " '-6938',\n",
       " '-6946',\n",
       " '-7111',\n",
       " '-7112',\n",
       " '-7126',\n",
       " '-7127',\n",
       " '-7143',\n",
       " '-7196',\n",
       " '-7263',\n",
       " '-7264',\n",
       " '-7265',\n",
       " '6544',\n",
       " '6547',\n",
       " '6800',\n",
       " '6822',\n",
       " '6875',\n",
       " '6876',\n",
       " '6904',\n",
       " '6929',\n",
       " '6938',\n",
       " '6946',\n",
       " '7143',\n",
       " '7196']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columna: action_categorical_6\n",
      "Valores únicos y su frecuencia:\n",
      "action_categorical_6\n",
      "59638795    44837\n",
      "4f7786a1      163\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: action_categorical_7\n",
      "Valores únicos y su frecuencia:\n",
      "action_categorical_7\n",
      "e2538fca    27646\n",
      "31b31f57    17354\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: auction_bidfloor\n",
      "Valores únicos y su frecuencia:\n",
      "auction_bidfloor\n",
      "0.090000    11800\n",
      "0.020000     3112\n",
      "0.070000     1472\n",
      "0.170000     1404\n",
      "0.100000     1236\n",
      "            ...  \n",
      "0.479000        1\n",
      "1.071428        1\n",
      "2.419000        1\n",
      "0.585000        1\n",
      "0.401606        1\n",
      "Name: count, Length: 940, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: auction_categorical_0\n",
      "Valores únicos y su frecuencia:\n",
      "auction_categorical_0\n",
      "61559b0f       6063\n",
      "220eba28       5366\n",
      "3d496348       4789\n",
      "6e069bec       4463\n",
      "431302a4       3034\n",
      "ea238f7b       2762\n",
      "0e28039c       2716\n",
      "0ef62fd9       2154\n",
      "Otro           2078\n",
      "df9868ac       2057\n",
      "2344f8ca       1413\n",
      "3e4b21a0       1114\n",
      "87f54743        636\n",
      "d3c7863f        609\n",
      "ff25306d        555\n",
      "6856f984        533\n",
      "3124f99c        502\n",
      "f2400ab2        476\n",
      "2a9c5636        439\n",
      "94bcceb7        400\n",
      "7b7a6b4c        375\n",
      "5631509c        371\n",
      "0a5fea98        337\n",
      "c6382961        274\n",
      "Desconocido     191\n",
      "3df9c1bd        177\n",
      "4f17add5        168\n",
      "29b77816        165\n",
      "b5162e4d        155\n",
      "72541507        152\n",
      "fcf406ac        132\n",
      "24c5c9b3        120\n",
      "974b0af5        116\n",
      "10c46747        108\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: auction_categorical_1\n",
      "Valores únicos y su frecuencia:\n",
      "auction_categorical_1\n",
      "714a9147    35738\n",
      "1be29569     7624\n",
      "ababfacb     1638\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: auction_categorical_10\n",
      "Valores únicos y su frecuencia:\n",
      "auction_categorical_10\n",
      "d6496210    44999\n",
      "Otro            1\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: auction_categorical_11\n",
      "Valores únicos y su frecuencia:\n",
      "auction_categorical_11\n",
      "Otro        13539\n",
      "c5abc8fa     9004\n",
      "1ea53035     3596\n",
      "f65ed99c     2024\n",
      "ea5bfdf9     1767\n",
      "96fe5027     1537\n",
      "ef20d227     1201\n",
      "f45760f0     1090\n",
      "e7e5ef8c      834\n",
      "15f36bd8      756\n",
      "5098dc9a      744\n",
      "21c52ab4      473\n",
      "13e1c4f4      412\n",
      "922325e3      409\n",
      "bc2ea7c7      375\n",
      "ce04f021      355\n",
      "a75afb39      312\n",
      "0f40c06b      291\n",
      "77201adc      284\n",
      "9efce19a      274\n",
      "f936a787      250\n",
      "857b03df      234\n",
      "21e00c7d      232\n",
      "80253976      228\n",
      "26cbe8ca      222\n",
      "777f2dce      221\n",
      "9e55df10      217\n",
      "0eaceca2      216\n",
      "01566d09      213\n",
      "96bfa073      197\n",
      "67555e9a      181\n",
      "3b26dcb0      178\n",
      "e7c67c20      175\n",
      "19998e8d      175\n",
      "518f127b      173\n",
      "f0504648      170\n",
      "3474d33c      163\n",
      "1af546ea      150\n",
      "92a99bcb      144\n",
      "ea1be7a8      144\n",
      "9ddf0de9      140\n",
      "d299e829      131\n",
      "73de9646      129\n",
      "2d3a2bdc      125\n",
      "b08b0036      114\n",
      "d42bf4a2      114\n",
      "cd60c786      113\n",
      "01c836b3      113\n",
      "b50305c8      112\n",
      "398592e5      110\n",
      "b91470d8      110\n",
      "4cb70e24      109\n",
      "ee8ca339      107\n",
      "bc9a9066      107\n",
      "40ab5992      104\n",
      "db4cfb47      102\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: auction_categorical_12\n",
      "Valores únicos y su frecuencia:\n",
      "auction_categorical_12\n",
      "Otro           11485\n",
      "Desconocido     9343\n",
      "b4e520dd        2225\n",
      "9c3324c7        1863\n",
      "e9e0850e        1355\n",
      "               ...  \n",
      "373200a1         117\n",
      "799809fe         110\n",
      "f9ddb3c2         109\n",
      "a9b459d0         108\n",
      "4f105ec4         104\n",
      "Name: count, Length: 68, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: auction_categorical_3\n",
      "Valores únicos y su frecuencia:\n",
      "auction_categorical_3\n",
      "Otro           44328\n",
      "Desconocido      193\n",
      "973025d7         150\n",
      "db992017         113\n",
      "6d136195         109\n",
      "75bcbeb3         107\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: auction_categorical_4\n",
      "Valores únicos y su frecuencia:\n",
      "auction_categorical_4\n",
      "ef64698f       31425\n",
      "7db9dc73       13414\n",
      "Desconocido      161\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: auction_categorical_5\n",
      "Valores únicos y su frecuencia:\n",
      "auction_categorical_5\n",
      "c99696ad    26405\n",
      "ec8edf0b    18595\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: auction_categorical_6\n",
      "Valores únicos y su frecuencia:\n",
      "auction_categorical_6\n",
      "ed14c0c3       10678\n",
      "434922cf        5274\n",
      "0fa26b70        5244\n",
      "Desconocido     4129\n",
      "fccb6ea5        3932\n",
      "3117d02e        3231\n",
      "7d7776ec        2900\n",
      "e70caf72        2650\n",
      "6f5a8fcb        2320\n",
      "1c95d1e5        1121\n",
      "b42d829e         958\n",
      "2f74fc92         669\n",
      "ababfacb         436\n",
      "79ac9638         422\n",
      "c20b2db7         387\n",
      "5271393e         250\n",
      "d2fee5de         232\n",
      "Otro             167\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: auction_categorical_7\n",
      "Valores únicos y su frecuencia:\n",
      "auction_categorical_7\n",
      "Otro        15186\n",
      "b04e4d07     2441\n",
      "034e01d7     1854\n",
      "8aca96c3     1268\n",
      "05a781fa     1254\n",
      "            ...  \n",
      "b13c76e2      106\n",
      "6a11ab19      105\n",
      "4fa2d925      103\n",
      "dec00e70      102\n",
      "d81e16f7      100\n",
      "Name: count, Length: 90, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: auction_categorical_8\n",
      "Valores únicos y su frecuencia:\n",
      "auction_categorical_8\n",
      "198c733d    29947\n",
      "e361d5bc     5776\n",
      "856f1b75     5430\n",
      "9685087a     1752\n",
      "fe917183      700\n",
      "ee36ba80      529\n",
      "d7a49a2e      329\n",
      "62d6343e      284\n",
      "97fd48df      181\n",
      "Otro           72\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: auction_categorical_9\n",
      "Valores únicos y su frecuencia:\n",
      "auction_categorical_9\n",
      "Otro           15935\n",
      "Desconocido    13417\n",
      "11105871        2184\n",
      "d4eea9a2        1722\n",
      "0f493a39         939\n",
      "1b26c761         838\n",
      "79ceee49         694\n",
      "e10064eb         478\n",
      "94e43af2         476\n",
      "bea5c21c         430\n",
      "8e9f619f         422\n",
      "2d142ca9         303\n",
      "e609fa5a         302\n",
      "3c4be72e         287\n",
      "105a7850         259\n",
      "ff29e19d         258\n",
      "7e952598         249\n",
      "25776807         249\n",
      "e10525ac         244\n",
      "8293f7bc         242\n",
      "a5c30fc9         217\n",
      "a803a2e6         200\n",
      "0d660d9a         199\n",
      "89b2cd84         185\n",
      "f690df59         175\n",
      "4a8ff226         174\n",
      "22e0670e         160\n",
      "62ae5ddb         156\n",
      "d3a7d8ba         155\n",
      "fae6eae7         152\n",
      "81cf1a0e         148\n",
      "f0369c26         147\n",
      "b108b520         142\n",
      "8663f240         137\n",
      "34a5b5e1         136\n",
      "955b4015         136\n",
      "b139ea74         134\n",
      "a1a5ced2         133\n",
      "d81fe3e2         133\n",
      "173ce000         126\n",
      "e36715bf         124\n",
      "7daeddab         123\n",
      "50921258         119\n",
      "28c10de5         119\n",
      "89a1e033         118\n",
      "e11ff413         117\n",
      "2784c5c0         117\n",
      "9f5b2744         115\n",
      "ee2cf71d         114\n",
      "93546afe         113\n",
      "98cb5201         112\n",
      "5c8425cc         111\n",
      "0d2be80d         108\n",
      "86efc936         106\n",
      "d2cfd751         105\n",
      "970c4b76         105\n",
      "b6c43d10         101\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: creative_categorical_0\n",
      "Valores únicos y su frecuencia:\n",
      "creative_categorical_0\n",
      "Otro        6573\n",
      "16005afa    3225\n",
      "7f776def    2677\n",
      "246cce9b    2292\n",
      "7acb895b    2256\n",
      "            ... \n",
      "f9c77d37     110\n",
      "6a275605     110\n",
      "ef6141da     103\n",
      "0e83d9f5     102\n",
      "c041644a     102\n",
      "Name: count, Length: 88, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: creative_categorical_1\n",
      "Valores únicos y su frecuencia:\n",
      "creative_categorical_1\n",
      "977c2300    28677\n",
      "35fcfcbb    16323\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: creative_categorical_12\n",
      "Valores únicos y su frecuencia:\n",
      "creative_categorical_12\n",
      "Desconocido    38411\n",
      "dd370958        1842\n",
      "5e6c4fa2        1188\n",
      "80819d2d        1095\n",
      "83dae26d         679\n",
      "a6d930ab         589\n",
      "8f6f5f41         473\n",
      "Otro             376\n",
      "546044ce         208\n",
      "99df4af8         139\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: creative_categorical_2\n",
      "Valores únicos y su frecuencia:\n",
      "creative_categorical_2\n",
      "Desconocido    37819\n",
      "451daa93        1849\n",
      "d97f9242        1188\n",
      "eacc48c8        1148\n",
      "50e522c2        1016\n",
      "76d66943         494\n",
      "857e02b0         420\n",
      "51499fde         389\n",
      "1d2fa540         281\n",
      "eec28017         212\n",
      "e5892738         160\n",
      "Otro              24\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: creative_categorical_3\n",
      "Valores únicos y su frecuencia:\n",
      "creative_categorical_3\n",
      "Desconocido    42447\n",
      "095cc02c        1188\n",
      "11a9f48f         478\n",
      "5f68c5e6         403\n",
      "a5b257c7         142\n",
      "d75eba3c         134\n",
      "28712cad         118\n",
      "Otro              90\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: creative_categorical_4\n",
      "Valores únicos y su frecuencia:\n",
      "creative_categorical_4\n",
      "150d94b7       22555\n",
      "7f1dcf83       15264\n",
      "Desconocido     6974\n",
      "f3b258d1         207\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: creative_categorical_5\n",
      "Valores únicos y su frecuencia:\n",
      "creative_categorical_5\n",
      "Desconocido    38034\n",
      "654a0207        6343\n",
      "Otro             320\n",
      "310dd70a         192\n",
      "52ea3f88         111\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: creative_categorical_6\n",
      "Valores únicos y su frecuencia:\n",
      "creative_categorical_6\n",
      "Desconocido    38500\n",
      "5a6ad423         648\n",
      "05e210ae         623\n",
      "c785677e         600\n",
      "88a3d2fd         594\n",
      "9041f1b3         565\n",
      "adf97351         478\n",
      "356a814d         473\n",
      "Otro             359\n",
      "6a3c2b88         305\n",
      "379a8766         304\n",
      "fa1a6c06         241\n",
      "d11f018a         212\n",
      "534e1302         198\n",
      "ca48f685         185\n",
      "735ae96a         179\n",
      "868a1c2d         170\n",
      "4ba9f278         139\n",
      "6d4f0cb8         114\n",
      "04cae3c5         113\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: creative_categorical_7\n",
      "Valores únicos y su frecuencia:\n",
      "creative_categorical_7\n",
      "Desconocido    38546\n",
      "8bc11a1e        1842\n",
      "b98125c8        1568\n",
      "ff858f87         970\n",
      "2b22dd6f         623\n",
      "2b235cde         589\n",
      "f1b6690d         565\n",
      "a7e34545         167\n",
      "6b5d903f         130\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: creative_categorical_8\n",
      "Valores únicos y su frecuencia:\n",
      "creative_categorical_8\n",
      "b6910b48    37819\n",
      "b00371d3     6371\n",
      "40ceda44      595\n",
      "d9d53fe0      207\n",
      "Otro            8\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: device_id\n",
      "Valores únicos y su frecuencia:\n",
      "device_id\n",
      "efe238b1    19\n",
      "758da2b1    14\n",
      "9cd0f5e4    11\n",
      "7218a218    11\n",
      "a95801d7    10\n",
      "            ..\n",
      "560348f4     1\n",
      "88ba2694     1\n",
      "ab2c4898     1\n",
      "12bc725b     1\n",
      "4c1478cd     1\n",
      "Name: count, Length: 40355, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: device_id_type\n",
      "Valores únicos y su frecuencia:\n",
      "device_id_type\n",
      "6324b367    26272\n",
      "c1d12c8e    18595\n",
      "42080e25      133\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: has_video\n",
      "Valores únicos y su frecuencia:\n",
      "has_video\n",
      "0    44262\n",
      "1      738\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: age_group\n",
      "Valores únicos y su frecuencia:\n",
      "age_group\n",
      "5    36300\n",
      "2     4118\n",
      "3     3414\n",
      "4      772\n",
      "1      385\n",
      "0       11\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: level_combination\n",
      "Valores únicos y su frecuencia:\n",
      "level_combination\n",
      "Otro               8175\n",
      "11bac02f2cb82c3    2440\n",
      "c2e3071ae62c971    1733\n",
      "604f71378242d1c    1664\n",
      "604f712e6242502    1536\n",
      "                   ... \n",
      "7f0f4c1765eb8cb     112\n",
      "11b0d1beff51853     111\n",
      "e35e228f1e9cf05     104\n",
      "e35e22880e9cee5     104\n",
      "e35e22c50e9ceb2     100\n",
      "Name: count, Length: 95, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: hxw\n",
      "Valores únicos y su frecuencia:\n",
      "hxw\n",
      "16000.0     24965\n",
      "75000.0     10188\n",
      "0.0          6946\n",
      "153600.0     2553\n",
      "65520.0       288\n",
      "786432.0       60\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: IAB-5\n",
      "Valores únicos y su frecuencia:\n",
      "IAB-5\n",
      "0    44996\n",
      "1        4\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: AND-APL\n",
      "Valores únicos y su frecuencia:\n",
      "AND-APL\n",
      "0    45000\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n",
      "Columna: -2606\n",
      "Valores únicos y su frecuencia:\n",
      "-2606\n",
      "0    44120\n",
      "1      880\n",
      "Name: count, dtype: int64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for column in X_train.columns:\n",
    "    # Verifica si la suma de las frecuencias de los valores únicos es menor a 1000\n",
    "    if (X_train[column] == 1).sum() < 1000:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(X_train[column].value_counts())\n",
    "        print(\"-\" * 50)  # Separador entre columnas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/ctr_test.csv')\n",
    "\n",
    "# X_test = process_data_with_dask(test_data, npartitions=10)\n",
    "\n",
    "X_test = process_optimized(test_data)\n",
    "\n",
    "# Excluir 'id' de las columnas de X_test para el reordenamiento\n",
    "common_columns = [col for col in X_test.columns if col != 'id']\n",
    "\n",
    "# Asegúrate de que las columnas en X_train coincidan con las de X_test (sin 'id')\n",
    "missing_cols = set(common_columns) - set(X_train.columns)\n",
    "for col in missing_cols:\n",
    "    X_train[col] = 0\n",
    "\n",
    "# Eliminar columnas extra en X_train que no están en X_test (sin contar 'id')\n",
    "extra_cols = set(X_train.columns) - set(common_columns)\n",
    "X_train.drop(columns=extra_cols, inplace=True)\n",
    "\n",
    "# Reordenar X_train para que tenga el mismo orden de columnas que X_test (sin 'id')\n",
    "X_train = X_train[common_columns]\n",
    "\n",
    "# Verificar el número de columnas en X_train\n",
    "print(X_train.shape[1])\n",
    "print(X_test['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in X_train.columns:\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos y su frecuencia:\")\n",
    "    print(X_train[column].value_counts())\n",
    "    print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las columnas categóricas y numéricas\n",
    "categorical_features_to_encode = X_train.select_dtypes(include=['object']).columns\n",
    "numeric_features = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Preprocesador común para imputación y codificación\n",
    "common_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Imputar valores numéricos con la media\n",
    "        ('num', SimpleImputer(strategy='mean'), numeric_features),\n",
    "        \n",
    "        # Imputar valores categóricos con 'Desconocido' y aplicar One-Hot Encoding\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='Desconocido')),\n",
    "            ('onehot', OneHotEncoder(drop='first', sparse_output=True, handle_unknown='ignore'))\n",
    "        ]), categorical_features_to_encode)\n",
    "    ],\n",
    "    remainder='drop'  # Excluir columnas no especificadas\n",
    ")\n",
    "\n",
    "# Definir el modelo XGBoost\n",
    "model_xgb = XGBClassifier(\n",
    "    random_state=random_state, \n",
    "    use_label_encoder=False, \n",
    "    eval_metric='auc'\n",
    ")\n",
    "\n",
    "# Pipeline para XGBoost (mantiene matrices dispersas)\n",
    "pipeline_xgb = Pipeline(steps=[\n",
    "    ('preprocessor', common_preprocessor),\n",
    "    ('classifier', model_xgb),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperot XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el espacio de búsqueda para los hiperparámetros de XGBoost\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 10)), \n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]), \n",
    "    'gamma': hp.uniform('gamma', 0, 0.5),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0.5, 2),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 1),  # L1 regularización\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 1),  # L2 regularización\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1),  # Muestreo a nivel de split\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.5, 1),  # Muestreo por nodo\n",
    "    'grow_policy': hp.choice('grow_policy', ['depthwise', 'lossguide']),  # Estrategia de crecimiento\n",
    "    'tree_method': hp.choice('tree_method', ['auto', 'approx', 'hist'])  # Métodos de construcción del árbol\n",
    "}\n",
    "\n",
    "# Función objetivo para Hyperopt utilizando validación cruzada\n",
    "def objective_xgb(params):\n",
    "    \n",
    "    # Asegurar que los parámetros sean del tipo correcto\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['min_child_weight'] = int(params['min_child_weight'])\n",
    "    \n",
    "    # Definir el modelo con los hiperparámetros actuales\n",
    "    model_xgb = XGBClassifier(\n",
    "        max_depth=params['max_depth'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        n_estimators=params['n_estimators'],\n",
    "        gamma=params['gamma'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        scale_pos_weight=params['scale_pos_weight'],\n",
    "        reg_alpha=params['reg_alpha'],  # L1 regularización\n",
    "        reg_lambda=params['reg_lambda'],  # L2 regularización\n",
    "        colsample_bylevel=params['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "        colsample_bynode=params['colsample_bynode'],  # Muestreo por nodo\n",
    "        grow_policy=params['grow_policy'],  # Estrategia de crecimiento\n",
    "        tree_method=params['tree_method'],  # Método de construcción del árbol\n",
    "        use_label_encoder=False,  # Evitar advertencias en versiones más recientes de XGBoost\n",
    "        eval_metric='auc',\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Pipeline para XGBoost (mantiene matrices dispersas)\n",
    "    pipeline_xgb = Pipeline(steps=[\n",
    "        ('preprocessor', common_preprocessor),\n",
    "        ('classifier', model_xgb),\n",
    "    ])\n",
    "    \n",
    "    # Definir la validación cruzada\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Calcular el AUC utilizando validación cruzada\n",
    "    auc = cross_val_score(pipeline_xgb, X_train, y_train, cv=cv, scoring='roc_auc').mean()\n",
    "    \n",
    "    # Opcional: imprimir los hiperparámetros y el AUC actual\n",
    "    print(f\"Hiperparámetros: {params}, AUC: {auc:.4f}\")\n",
    "    \n",
    "    # Retornar el valor a minimizar (1 - AUC)\n",
    "    return {'loss': 1 - auc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la optimización\n",
    "trials = Trials()\n",
    "\n",
    "best_xgb = fmin(\n",
    "    fn=objective_xgb,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=5,\n",
    "    trials=trials,\n",
    "    rstate=np.random.default_rng(random_state)  # Asegurar reproducibilidad\n",
    ")\n",
    "\n",
    "# No es necesario volver a mapear los hiperparámetros aquí, ya se hizo dentro de la función objetivo\n",
    "print(\"Mejores hiperparámetros para XGBoost:\")\n",
    "print(best_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRUEBA CON TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices\n",
    "tree_method_options = ['auto', 'exact', 'approx', 'hist']\n",
    "grow_policy_options = ['depthwise', 'lossguide']\n",
    "n_estimators_options = [100, 200, 300, 400, 500]\n",
    "\n",
    "# Reconstruir el modelo de XGBoost con los mejores hiperparámetros\n",
    "best_model_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=n_estimators_options[best_xgb['n_estimators']],\n",
    "    max_depth=best_xgb['max_depth'],\n",
    "    learning_rate=best_xgb['learning_rate'],\n",
    "    subsample=best_xgb['subsample'],\n",
    "    colsample_bytree=best_xgb['colsample_bytree'],\n",
    "    min_child_weight=best_xgb['min_child_weight'],\n",
    "    gamma=best_xgb['gamma'],\n",
    "    scale_pos_weight=best_xgb['scale_pos_weight'],\n",
    "    reg_alpha=best_xgb['reg_alpha'],  # L1 regularización\n",
    "    reg_lambda=best_xgb['reg_lambda'],  # L2 regularización\n",
    "    colsample_bylevel=best_xgb['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "    colsample_bynode=best_xgb['colsample_bynode'],  # Muestreo por nodo\n",
    "    grow_policy=grow_policy_options[best_xgb['grow_policy']],  # Estrategia de crecimiento\n",
    "    tree_method=tree_method_options[best_xgb['tree_method']],  # Método de construcción del árbol\n",
    "    random_state=random_state,\n",
    "    use_label_encoder=False,  # Evitar advertencias en versiones más recientes de XGBoost\n",
    "    eval_metric=\"auc\"  # Establecer AUC como métrica de evaluación\n",
    ")\n",
    "\n",
    "# Crear un nuevo pipeline reutilizando el preprocesador original y el mejor modelo\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', common_preprocessor),\n",
    "    ('classifier', model_xgb),\n",
    "])\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "xgb_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir en el conjunto de testeo\n",
    "y_preds_xgb_test = xgb_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_xgb = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_xgb_test})\n",
    "submission_df_xgb[\"id\"] = submission_df_xgb[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros\n",
    "file_name_xgb = (\n",
    "    f\"xgboost_preds_ne_{best_xgb['n_estimators']}_\"\n",
    "    f\"md_{handle_none(best_xgb['max_depth'])}_\"\n",
    "    f\"lr_{round(best_xgb['learning_rate'], 2)}_\"\n",
    "    f\"ss_{round(best_xgb['subsample'], 2)}_\"\n",
    "    f\"csb_{round(best_xgb['colsample_bytree'], 2)}_\"\n",
    "    f\"cb_level_{round(best_xgb['colsample_bylevel'], 2)}_\"\n",
    "    f\"cb_node_{round(best_xgb['colsample_bynode'], 2)}_\"\n",
    "    f\"mcw_{round(best_xgb['min_child_weight'], 2)}_\"\n",
    "    f\"gamma_{round(best_xgb['gamma'], 2)}_\"\n",
    "    f\"ra_{round(best_xgb['reg_alpha'], 2)}_\"\n",
    "    f\"rl_{round(best_xgb['reg_lambda'], 2)}_\"\n",
    "    f\"spw_{round(best_xgb['scale_pos_weight'], 2)}_\"\n",
    "    f\"gp_{grow_policy_options[best_xgb['grow_policy']]}_\"\n",
    "    f\"tm_{tree_method_options[best_xgb['tree_method']]}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_xgb.to_csv(os.path.join(\"submits\", file_name_xgb), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_xgb}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDVI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
