{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 43992294\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.impute import SimpleImputer\n",
    "import seaborn as sns\n",
    "from scipy.stats import chi2_contingency\n",
    "import scipy.stats as ss\n",
    "import os\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import ast\n",
    "\n",
    "import warnings\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Cython imports\n",
    "from tools import agrupar_categorias_cython, custom_one_hot_encoder_cython, boolean_features_ohe_cython, agrupar_edades_cython, expand_action_list_0_cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desactivar warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Agrega filas que contienen categorías desconocidas o raras (poca frecuencia) al conjunto de entrenamiento\n",
    "def augment_train_data(main_train_df, supplementary_df, umbral_raras=100):\n",
    "    \"\"\"\n",
    "    Agrega filas del dataset suplementario al conjunto de entrenamiento principal\n",
    "    basándose en categorías desconocidas y raras, evitando la duplicación de filas.\n",
    "\n",
    "    Parámetros:\n",
    "    - main_train_df (pd.DataFrame): DataFrame principal de entrenamiento.\n",
    "    - supplementary_df (pd.DataFrame): DataFrame suplementario del cual se extraerán las filas.\n",
    "    - umbral_raras (int): Umbral de frecuencia para considerar una categoría como rara.\n",
    "\n",
    "    Retorna:\n",
    "    - main_train_df (pd.DataFrame): DataFrame de entrenamiento actualizado.\n",
    "    - categorias_desconocidas (dict): Diccionario actualizado de categorías desconocidas.\n",
    "    - categorias_raras (dict): Diccionario actualizado de categorías raras.\n",
    "    \"\"\"\n",
    "    # Definir columnas que no deseas tratar como categóricas\n",
    "    columns_to_exclude = ['auction_list_0', 'action_list_1', 'action_list_2', 'auction_time']\n",
    "\n",
    "    # Identificar columnas categóricas excluyendo las especificadas\n",
    "    categorical_features = main_train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    categorical_features = [col for col in categorical_features if col not in columns_to_exclude]\n",
    "    \n",
    "    # Crear un diccionario para almacenar las categorías desconocidas por columna\n",
    "    categorias_desconocidas = {}\n",
    "    \n",
    "    # Iterar a través de cada columna categórica para identificar categorías desconocidas\n",
    "    for columna in categorical_features:\n",
    "        # Obtener las categorías únicas en el conjunto de entrenamiento\n",
    "        categorias_train = set(main_train_df[columna].dropna().unique())\n",
    "        \n",
    "        # Obtener las categorías únicas en el dataset suplementario\n",
    "        categorias_suplementario = set(supplementary_df[columna].dropna().unique())\n",
    "        \n",
    "        # Identificar las categorías en el dataset suplementario que no están en el entrenamiento\n",
    "        desconocidas = categorias_suplementario - categorias_train\n",
    "        \n",
    "        # Almacenar las categorías desconocidas en el diccionario como una lista\n",
    "        categorias_desconocidas[columna] = list(desconocidas)\n",
    "    \n",
    "    # Inicializar el diccionario para almacenar las categorías raras por columna\n",
    "    categorias_raras = {}\n",
    "    \n",
    "    # Identificar categorías raras en el conjunto de entrenamiento\n",
    "    for columna in categorical_features:\n",
    "        # Contar la frecuencia de cada categoría\n",
    "        frecuencia = main_train_df[columna].value_counts()\n",
    "        \n",
    "        # Identificar categorías que aparecen menos de umbral_raras veces\n",
    "        raras = frecuencia[frecuencia < umbral_raras].index.tolist()\n",
    "        \n",
    "        # Almacenar en el diccionario\n",
    "        categorias_raras[columna] = raras\n",
    "    \n",
    "    # Crear una máscara booleana para filas con categorías desconocidas o raras\n",
    "    mask_desconocidas = pd.Series([False] * len(supplementary_df))\n",
    "    mask_raras = pd.Series([False] * len(supplementary_df))\n",
    "    \n",
    "    for columna in categorical_features:\n",
    "        # Actualizar la máscara para categorías desconocidas\n",
    "        if categorias_desconocidas[columna]:\n",
    "            mask_desconocidas = mask_desconocidas | supplementary_df[columna].isin(categorias_desconocidas[columna])\n",
    "        \n",
    "        # Actualizar la máscara para categorías raras\n",
    "        if categorias_raras[columna]:\n",
    "            mask_raras = mask_raras | supplementary_df[columna].isin(categorias_raras[columna])\n",
    "    \n",
    "    # Combinar ambas máscaras\n",
    "    mask_total = mask_desconocidas | mask_raras\n",
    "    \n",
    "    # Filtrar filas únicas a agregar\n",
    "    filas_a_agregar = supplementary_df[mask_total].drop_duplicates()\n",
    "    \n",
    "    # Mostrar información de agregación\n",
    "    total_agregadas = len(filas_a_agregar)\n",
    "    print(f\"\\nAgregando {total_agregadas} filas del dataset suplementario basadas en categorías desconocidas o raras.\")\n",
    "    \n",
    "    # Agregar las filas al conjunto de entrenamiento\n",
    "    main_train_df = pd.concat([main_train_df, filas_a_agregar], ignore_index=True)\n",
    "    \n",
    "    # Actualizar los diccionarios eliminando las categorías que ya han sido agregadas\n",
    "    for columna in categorical_features:\n",
    "        # Actualizar categorías desconocidas\n",
    "        if categorias_desconocidas[columna]:\n",
    "            categorias_agregadas = filas_a_agregar[columna].unique().tolist()\n",
    "            categorias_desconocidas[columna] = [cat for cat in categorias_desconocidas[columna] if cat not in categorias_agregadas]\n",
    "        \n",
    "        # Actualizar categorías raras\n",
    "        if categorias_raras[columna]:\n",
    "            # Recontar la frecuencia después de agregar\n",
    "            frecuencia = main_train_df[columna].value_counts()\n",
    "            nuevas_raras = frecuencia[frecuencia < umbral_raras].index.tolist()\n",
    "            categorias_raras[columna] = nuevas_raras\n",
    "    \n",
    "    return main_train_df, categorias_desconocidas, categorias_raras\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Agrega como columnas binarias las listas de la columna 'auction_list_0', 'action_list_1' y 'action_list_2'\n",
    "def expand_list_dummies_cython(df, column, delimiter='|', prefix=None, suffix=None):\n",
    "    \"\"\"\n",
    "    Expande una columna que contiene listas en múltiples columnas binarias usando un one-hot encoder optimizado con Cython.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame de pandas.\n",
    "    - column (str): Nombre de la columna a expandir.\n",
    "    - delimiter (str): Delimitador a usar en get_dummies (por defecto '|').\n",
    "    - prefix (str, optional): Prefijo para las nuevas columnas binarias.\n",
    "    - suffix (str, optional): Sufijo para las nuevas columnas binarias.\n",
    "    \n",
    "    Returns:\n",
    "    - df_expanded (pd.DataFrame): DataFrame con las nuevas columnas binarias añadidas y la columna original eliminada.\n",
    "    \"\"\"\n",
    "    print(f\"Comenzando la expansión de la columna: '{column}'\")\n",
    "    \n",
    "    # Copiar el DataFrame para evitar modificar el original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Reemplazar NaN por listas vacías\n",
    "    print(f\"Reemplazando NaN en la columna '{column}' por listas vacías.\")\n",
    "    df[column] = df[column].fillna('[]')\n",
    "    \n",
    "    # Definir la función de parsing con impresión de errores\n",
    "    def parse_list(x):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(x)\n",
    "            if isinstance(parsed, list):\n",
    "                # Convertir todos los elementos a strings\n",
    "                return [str(item) for item in parsed]\n",
    "            else:\n",
    "                # Si no es una lista, tratar como un solo elemento\n",
    "                return [str(x)]\n",
    "        except (ValueError, SyntaxError):\n",
    "            # En caso de error al parsear, retornar una lista vacía\n",
    "            print(f\"Error al parsear el valor: {x}. Retornando lista vacía.\")\n",
    "            return []\n",
    "    \n",
    "    # Aplicar la función de parsing con una barra de progreso\n",
    "    print(f\"Aplicando la función de parsing a la columna '{column}'.\")\n",
    "    df[column] = df[column].progress_apply(parse_list)\n",
    "    \n",
    "    # Convertir la columna en una lista de listas para pasarla a la función Cythonizada\n",
    "    print(f\"Convirtiendo la columna '{column}' a una lista de listas para el codificador Cython.\")\n",
    "    data_list = df[column].tolist()\n",
    "    \n",
    "    # Llamar a la función optimizada en Cython\n",
    "    print(\"Llamando a la función optimizada en Cython para obtener categorías únicas y matriz binaria.\")\n",
    "    unique_categories, binary_matrix = custom_one_hot_encoder_cython(data_list)\n",
    "    print(f\"Codificación Cython completada. {len(unique_categories)} categorías únicas encontradas.\")\n",
    "    \n",
    "    # Crear un DataFrame binario usando la matriz devuelta por Cython\n",
    "    print(\"Creando el DataFrame binario a partir de la matriz binaria.\")\n",
    "    binary_df = pd.DataFrame(binary_matrix, index=df.index, columns=unique_categories)\n",
    "    \n",
    "    print(f\"Añadiendo prefijo y/o sufijo si se especifica\")\n",
    "    if prefix:\n",
    "        binary_df = binary_df.add_prefix(f\"{prefix}_\")\n",
    "    if suffix:\n",
    "        binary_df = binary_df.add_suffix(f\"_{suffix}\")\n",
    "    \n",
    "    # Concatenar las columnas binarias al DataFrame original con una barra de progreso\n",
    "    print(\"Concatenando las columnas binarias al DataFrame original.\")\n",
    "    for col in tqdm(binary_df.columns, desc=\"Concatenando columnas binarias\"):\n",
    "        if col in df.columns:\n",
    "            df[col] = np.where((df[col] == 1) | (binary_df[col] == 1), 1, 0)\n",
    "        else:\n",
    "            df[col] = binary_df[col]\n",
    "    \n",
    "    # Eliminar la columna original ya que ha sido expandida\n",
    "    print(f\"Eliminando la columna original '{column}' del DataFrame.\")\n",
    "    df = df.drop(columns=[column])\n",
    "    \n",
    "    print(f\"Expansión de la columna '{column}' completada exitosamente.\\n\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# One-hot encode de columnas booleanas utilizando Cython\n",
    "def boolean_features_ohe(df, columns_to_encode=['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2']):\n",
    "    \"\"\"\n",
    "    Realiza one-hot encoding en columnas booleanas especificadas utilizando una función optimizada con Cython.\n",
    "    Además, muestra el progreso del procesamiento utilizando tqdm y añade comentarios explicativos.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame original que contiene las columnas booleanas a codificar.\n",
    "    - columns_to_encode (list): Lista de nombres de columnas booleanas a codificar.\n",
    "\n",
    "    Retorna:\n",
    "    - df_expanded (pd.DataFrame): DataFrame con las nuevas columnas codificadas añadidas y las columnas booleanas originales eliminadas.\n",
    "    \"\"\"\n",
    "    # Copiar el DataFrame para evitar modificar el original\n",
    "    df = df.copy()\n",
    "    print(\"Inicio del proceso de one-hot encoding para las columnas booleanas especificadas.\")\n",
    "\n",
    "    # Paso 1: Encontrar todos los valores únicos en las columnas a codificar\n",
    "    unique_values_set = set()\n",
    "    print(\"Recopilando valores únicos de las columnas a codificar:\")\n",
    "    for col in tqdm(columns_to_encode, desc=\"Procesando columnas para valores únicos\"):\n",
    "        unique_vals_col = df[col].dropna().unique()\n",
    "        unique_values_set.update(unique_vals_col)\n",
    "    unique_values = sorted(unique_values_set)\n",
    "    print(f\"Valores únicos encontrados: {unique_values}\")\n",
    "\n",
    "    # Paso 2: Convertir las columnas a listas de listas para ser procesadas en Cython\n",
    "    list_data = []\n",
    "    print(\"Convirtiendo las columnas booleanas a listas de listas para Cython:\")\n",
    "    for col in tqdm(columns_to_encode, desc=\"Convertir columnas a listas\"):\n",
    "        column_list = df[col].astype(str).tolist()  # Mantener los valores como strings\n",
    "        list_data.append(column_list)\n",
    "    print(\"Conversión completada.\")\n",
    "\n",
    "    # Paso 3: Procesar los datos con la función optimizada en Cython\n",
    "    print(\"Realizando one-hot encoding utilizando la función optimizada en Cython:\")\n",
    "    ohe_result = boolean_features_ohe_cython(list_data, unique_values)\n",
    "    print(\"One-hot encoding completado.\")\n",
    "\n",
    "    # Paso 4: Convertir el resultado de Cython a un DataFrame, alineando el índice con df\n",
    "    print(\"Creando el DataFrame de columnas codificadas:\")\n",
    "    ohe_df = pd.DataFrame(ohe_result, columns=unique_values, index=df.index)\n",
    "    print(f\"DataFrame de one-hot encoding creado con {len(ohe_df.columns)} columnas y {ohe_df.shape[0]} filas.\")\n",
    "\n",
    "    # Paso 5: Concatenar las nuevas columnas codificadas al DataFrame original\n",
    "    print(\"Concatenando las columnas codificadas al DataFrame original:\")\n",
    "    df_expanded = pd.concat([df, ohe_df], axis=1)\n",
    "    print(f\"Concatenación completada. El DataFrame ahora tiene {df_expanded.shape[1]} columnas y {df_expanded.shape[0]} filas.\")\n",
    "\n",
    "    # Paso 6: Eliminar las columnas booleanas originales del DataFrame\n",
    "    print(\"Eliminando las columnas booleanas originales del DataFrame:\")\n",
    "    df_expanded.drop(columns=columns_to_encode, inplace=True)\n",
    "    print(f\"Columnas eliminadas: {columns_to_encode}\")\n",
    "\n",
    "    print(\"Proceso de one-hot encoding finalizado exitosamente.\\n\")\n",
    "\n",
    "    return df_expanded\n",
    "\n",
    "# Extensión de características temporales (día de la semana, momento del día, etc.) y festividades\n",
    "def time_features_extension(df):\n",
    "    \"\"\"\n",
    "    Procesa las características temporales del DataFrame y agrega nuevas columnas derivadas relacionadas con el tiempo y festividades.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convertir 'auction_time' de timestamp a una fecha legible\n",
    "    df['auction_time'] = pd.to_datetime(df['auction_time'], unit='s')\n",
    "\n",
    "    # Reemplazar NaN en 'timezone_offset' por 0\n",
    "    df['timezone_offset'] = df['timezone_offset'].fillna(0)\n",
    "\n",
    "    # Ajustar la hora según el 'timezone_offset' para obtener la hora local\n",
    "    df['auction_time_local'] = df.apply(\n",
    "        lambda row: row['auction_time'] + pd.DateOffset(hours=row['timezone_offset']), axis=1\n",
    "    )\n",
    "\n",
    "    # Crear la columna 'week_day' (1 para lunes, 7 para domingo)\n",
    "    df['week_day'] = df['auction_time_local'].dt.weekday + 1\n",
    "\n",
    "    # Crear la columna 'moment_of_the_day' (1 para temprano, 2 para tarde, 3 para noche)\n",
    "    df['moment_of_the_day'] = pd.cut(df['auction_time_local'].dt.hour, \n",
    "                                     bins=[0, 12, 18, 24], labels=[1, 2, 3], include_lowest=True, right=False)\n",
    "\n",
    "    # Eliminar las columnas originales 'auction_time', 'timezone_offset' y 'auction_time_local'\n",
    "    df.drop(columns=['auction_time', 'timezone_offset', 'auction_time_local'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Agrupación de edades en rangos numéricos\n",
    "def age_group(df, columna_edad):\n",
    "    \"\"\"\n",
    "    Agrupa las edades en rangos numéricos utilizando Cython para mejorar el rendimiento.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene la columna de edades.\n",
    "    - columna_edad (str): Nombre de la columna que contiene las edades.\n",
    "\n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame con la nueva columna 'age_group' que representa el rango de edad.\n",
    "    \"\"\"\n",
    "    # Convertir la columna de edad a una lista\n",
    "    edades = df[columna_edad].tolist()\n",
    "\n",
    "    # Usar la función Cythonizada para agrupar las edades\n",
    "    df['age_group'] = agrupar_edades_cython(edades)\n",
    "\n",
    "    # Eliminar la columna original de edades\n",
    "    df.drop(columns=[columna_edad], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Agrupo action_list_0 a auction_list_0\n",
    "def expand_action_list_0(df):\n",
    "    \"\"\"\n",
    "    Expande la columna 'action_list_0' en valores únicos y marca con 1 las columnas existentes o las crea si es necesario.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame que contiene la columna 'action_list_0' y otras columnas de listas ya expandidas.\n",
    "\n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame actualizado con las columnas de valores únicos de 'action_list_0'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convertir la columna 'action_list_0' y las columnas existentes a listas\n",
    "    action_list_0 = df['action_list_0'].tolist()\n",
    "    existing_columns = df.columns.tolist()\n",
    "    \n",
    "    # Inicializar la matriz actual\n",
    "    current_matrix = df.values.tolist()\n",
    "\n",
    "    # Llamar a la función Cythonizada\n",
    "    updated_matrix = expand_action_list_0_cython(action_list_0, existing_columns, current_matrix)\n",
    "\n",
    "    # Convertir la matriz actualizada de vuelta a un DataFrame\n",
    "    df_updated = pd.DataFrame(updated_matrix, columns=existing_columns)\n",
    "\n",
    "    # Eliminar la columna 'action_list_0'\n",
    "    df_updated.drop(columns=['action_list_0'], inplace=True)\n",
    "\n",
    "    return df_updated\n",
    "\n",
    "# Procesamiento optimizado de un DataFrame\n",
    "def process_optimized(df):\n",
    "    \"\"\"\n",
    "    Aplica una serie de transformaciones al DataFrame utilizando una función Cython optimizada.\n",
    "    \n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): DataFrame a procesar.\n",
    "    \n",
    "    Retorna:\n",
    "    - df (pd.DataFrame): DataFrame procesado.\n",
    "    - categories (set): Conjunto de categorías creadas durante el procesamiento.\n",
    "    \"\"\"\n",
    "    # Definir el número total de pasos para la barra de progreso\n",
    "    total_steps = 7\n",
    "    \n",
    "    # Inicializar la barra de progreso\n",
    "    with tqdm(total=total_steps, desc=\"Procesando DataFrame\", unit=\"paso\") as pbar:\n",
    "        \n",
    "        # Eliminar columnas no deseadas\n",
    "        \n",
    "        df = df.drop('action_categorical_5', axis=1)\n",
    "        pbar.update(1)\n",
    "\n",
    "        df = boolean_features_ohe(df)\n",
    "        \n",
    "        pbar.update(1)\n",
    "\n",
    "        df = expand_list_dummies_cython(df, 'auction_list_0')\n",
    "        pbar.update(1)\n",
    "        \n",
    "        df = expand_list_dummies_cython(df, 'action_list_1')\n",
    "        pbar.update(1)\n",
    "        \n",
    "        df = expand_list_dummies_cython(df, 'action_list_2')\n",
    "        pbar.update(1)\n",
    "        \n",
    "        df = time_features_extension(df)\n",
    "        pbar.update(1)\n",
    "\n",
    "        df = age_group(df, 'auction_age')\n",
    "        pbar.update(1)\n",
    "\n",
    "        columns_to_exclude = ['auction_list_0', 'action_list_1', 'action_list_2', 'auction_time', 'timezone_offset' 'auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2', 'auction_age']\n",
    "        categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "        categorical_features = [col for col in categorical_features if col not in columns_to_exclude]\n",
    "        \n",
    "        # Convertir a matriz bidimensional\n",
    "        data_matrix = df[categorical_features].values.tolist()\n",
    "        data_matrix_cython = [list(row) for row in data_matrix]\n",
    "        \n",
    "        # Llamar a la función Cythonizada\n",
    "        df_cython_data = agrupar_categorias_cython(\n",
    "            categorical_features=categorical_features,\n",
    "            columns_to_exclude=columns_to_exclude,\n",
    "            data=data_matrix_cython,\n",
    "            umbral=0                             # Con un umbral de 0, modifico los valores NaN\n",
    "        )\n",
    "        \n",
    "        # Reasignar los datos al DataFrame\n",
    "        for i, col in enumerate(categorical_features):\n",
    "            df[col] = [row[i] for row in df_cython_data]\n",
    "        \n",
    "        pbar.update(1)  # Actualizar la barra de progreso\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Función para ajustar el tipo de datos de una columna para que Dask tome Nan como valor válido\n",
    "def adjust_dtype(dtype):\n",
    "    if pd.api.types.is_integer_dtype(dtype):\n",
    "        return 'Int64'\n",
    "    elif pd.api.types.is_float_dtype(dtype):\n",
    "        return 'float64'\n",
    "    elif pd.api.types.is_bool_dtype(dtype):\n",
    "        return 'boolean'\n",
    "    else:\n",
    "        return 'object'\n",
    "    \n",
    "# Procesamiento de datos con Dask\n",
    "def process_data_with_dask(df, npartitions=10, meta_df=None):\n",
    "    \"\"\"\n",
    "    Procesa un DataFrame utilizando Dask para distribuir el trabajo en varias particiones.\n",
    "    Aplica la función process_optimized a cada partición del DataFrame.\n",
    "\n",
    "    Parámetros:\n",
    "    - df (pd.DataFrame): El DataFrame de pandas a procesar.\n",
    "    - npartitions (int): Número de particiones en las que se dividirá el DataFrame para su procesamiento.\n",
    "\n",
    "    Retorna:\n",
    "    - final_df (pd.DataFrame): El DataFrame procesado y concatenado.\n",
    "    \"\"\"\n",
    "    # Convertir el DataFrame de pandas a Dask con el número de particiones especificado\n",
    "    dask_df = dd.from_pandas(df, npartitions=npartitions)\n",
    "    \n",
    "    # Crear el meta DataFrame con tipos ajustados\n",
    "    meta = df.head(0).copy()\n",
    "    for col in meta.columns:\n",
    "        meta[col] = meta[col].astype(adjust_dtype(df[col].dtype))\n",
    "        \n",
    "    # Aplicar la función con map_partitions y especificar el meta\n",
    "    dask_df = dask_df.map_partitions(\n",
    "        lambda df_partition: process_optimized(df_partition),\n",
    "        meta=meta\n",
    "    )\n",
    "\n",
    "    # Ejecutar el cálculo distribuido y convertir el resultado a pandas\n",
    "    final_df = dask_df.compute()\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# Función para calcular la estadística de Cramér's V\n",
    "def cramers_v(confusion_matrix):\n",
    "    \"\"\"\n",
    "    Calcula la estadística de Cramér's V para medir la asociación entre dos variables categóricas.\n",
    "\n",
    "    Parameters:\n",
    "    - confusion_matrix: Matriz de confusión (tabla de contingencia) entre dos variables.\n",
    "\n",
    "    Returns:\n",
    "    - Cramér's V: Valor entre 0 y 1 que indica la fuerza de la asociación.\n",
    "    \"\"\"\n",
    "    # Calcular el estadístico chi-cuadrado\n",
    "    chi2 = chi2_contingency(confusion_matrix, correction=False)[0]\n",
    "    # Número total de observaciones\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    # Obtener el número de filas y columnas de la matriz de confusión\n",
    "    r, k = confusion_matrix.shape\n",
    "    if min(r, k) == 1:\n",
    "        return np.nan  # Evitar dividir por cero\n",
    "    # Calcular Cramér's V\n",
    "    return np.sqrt(chi2 / (n * (min(r, k) - 1)))\n",
    "\n",
    "def handle_none(value):\n",
    "    return 'None' if value is None else str(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancatenamos datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_21 = pd.read_csv('data/ctr_21.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_20 = pd.read_csv('data/ctr_20.csv')\n",
    "train_data_19 = pd.read_csv('data/ctr_19.csv')\n",
    "train_data_18 = pd.read_csv('data/ctr_18.csv')\n",
    "train_data_17 = pd.read_csv('data/ctr_17.csv')\n",
    "train_data_16 = pd.read_csv('data/ctr_16.csv')\n",
    "train_data_15 = pd.read_csv('data/ctr_15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplementary_datasets = [\n",
    "    ('1', train_data_20),\n",
    "    ('2', train_data_19),\n",
    "    ('3', train_data_18),\n",
    "    ('4', train_data_17),\n",
    "    ('5', train_data_16),\n",
    "    ('6', train_data_15)\n",
    "]\n",
    "\n",
    "train_data = train_data_21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nombre, dataset in supplementary_datasets:\n",
    "    print(f\"\\nProcesando dataset {nombre}/{len(supplementary_datasets)}\")\n",
    "    train_data, categorias_desconocidas, categorias_raras = augment_train_data(train_data, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape[0])\n",
    "\n",
    "print(train_data_20.shape[0] + train_data_19.shape[0] + train_data_18.shape[0] + train_data_17.shape[0] + train_data_16.shape[0] + train_data_15.shape[0] + train_data_21.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('train_data_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingenieria de atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_data_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/ctr_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorias numericas\n",
    "numeric_columns = train_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Categorias categóricas\n",
    "categorical_features = train_data.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribucion de 'Label' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir la cantidad de filas del dataset combinado\n",
    "print(f\"Cantidad de filas en el dataset combinado: {train_data.shape[0]}\")\n",
    "print(f\"Cantidad de columnas en el dataset combinado: {train_data.shape[1]}\")\n",
    "\n",
    "# Ver porcentaje de clics vs no clics en la columna Label\n",
    "label_counts = train_data['Label'].value_counts(normalize=True) * 100\n",
    "print(\"\\nPorcentaje de clics (1) y no clics (0):\")\n",
    "print(label_counts)\n",
    "\n",
    "# Cantidad de clics (1) y no clics (0)\n",
    "label_counts_abs = train_data['Label'].value_counts()\n",
    "print(\"\\nCantidad de clics (1) y no clics (0):\")\n",
    "print(label_counts_abs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlaciones con 'Label'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis with 'Label'\n",
    "numeric_data = train_data[numeric_columns]\n",
    "correlation_with_label = numeric_data.corr()['Label'].sort_values(key=abs, ascending=False)\n",
    "\n",
    "# Print correlations with 'Label'\n",
    "print(\"Correlation of features with 'Label' (sorted by absolute value):\")\n",
    "for feature, corr in correlation_with_label.items():\n",
    "    if feature != 'Label':\n",
    "        print(f\"{feature}: {corr:.4f}\")\n",
    "\n",
    "# Visualize top correlations with 'Label'\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_correlations = correlation_with_label.drop('Label').abs()\n",
    "sns.barplot(x=top_correlations.values, y=top_correlations.index)\n",
    "plt.title('Numerical Features Correlated with Label')\n",
    "plt.xlabel('Absolute Correlation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cramer's V for each categorical feature with 'Label'\n",
    "cramer_v_results = {}\n",
    "for col in categorical_features:\n",
    "    confusion_matrix = pd.crosstab(train_data[col], train_data['Label'])\n",
    "    cramer_v_results[col] = cramers_v(confusion_matrix)\n",
    "\n",
    "# Sort results\n",
    "cramer_v_results = dict(sorted(cramer_v_results.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Visualize top Cramer's V results\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = dict(list(cramer_v_results.items())[:20])\n",
    "sns.barplot(x=list(top_features.values()), y=list(top_features.keys()))\n",
    "plt.title(\"Top 20 Categorical Features by Cramer's V with Label\")\n",
    "plt.xlabel(\"Cramer's V\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print results\n",
    "print(\"Cramer's V for categorical features with 'Label':\")\n",
    "for feature, v in top_features.items():\n",
    "    print(f\"{feature}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de datos faltantes (sin cambios)\n",
    "missing_data = train_data.isnull().sum() / len(train_data) * 100\n",
    "missing_data = missing_data[missing_data > 10].sort_values(ascending=False)  # Filtrar las columnas con más del 10% de datos faltantes\n",
    "\n",
    "# Visualizar datos faltantes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=missing_data.values, y=missing_data.index)\n",
    "plt.title('Percentage of Missing Data by Numerical Feature (>10%)')\n",
    "plt.xlabel('Percentage Missing')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"\\nPercentage of Missing Data by Feature (>10% missing):\")\n",
    "for feature, percentage in missing_data.items():\n",
    "    print(f\"{feature}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valores unicos y su frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre cada columna y mostrar los valores únicos y la cantidad de veces que aparecen\n",
    "for column in train_data.columns:\n",
    "    if column not in ['auction_list_0', 'action_list_1', 'action_list_2', 'auction_time']:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario para almacenar los valores únicos de cada columna\n",
    "unique_values = {col: set(train_data[col].dropna().unique()) for col in categorical_features}\n",
    "\n",
    "# Diccionario para almacenar las columnas que tienen valores comunes\n",
    "common_columns = {}\n",
    "\n",
    "# Comparar las columnas entre sí para ver qué valores comparten\n",
    "for i in range(len(categorical_features)):\n",
    "    for j in range(i + 1, len(categorical_features)):\n",
    "        col1 = categorical_features[i]\n",
    "        col2 = categorical_features[j]\n",
    "        \n",
    "        # Ver los valores que se repiten entre las dos columnas\n",
    "        common_values = unique_values[col1].intersection(unique_values[col2])\n",
    "        \n",
    "        if common_values:\n",
    "            # Almacenar las columnas con valores comunes\n",
    "            if (col1, col2) not in common_columns:\n",
    "                common_columns[(col1, col2)] = common_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver las columnas que tienen valores comunes y sus unique values\n",
    "for col_pair, common_vals in common_columns.items():\n",
    "    col1, col2 = col_pair\n",
    "    print(f\"\\nValores comunes entre {col1} y {col2}: {common_vals}\")\n",
    "    print(f\"Valores únicos de {col1}: {len(unique_values[col1])}\")\n",
    "    print(f\"Valores únicos de {col2}: {len(unique_values[col2])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### action_categorical_5 (motivos para eliminarlo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_to_check = '6bc0e29c'\n",
    "filtered_rows = train_data[train_data['action_categorical_5'] == value_to_check]\n",
    "\n",
    "# Verificar si el valor aparece en otras columnas categóricas\n",
    "for col in categorical_features:\n",
    "    if col != 'action_categorical_5':  # Evitar verificar la columna original\n",
    "        matching_rows = filtered_rows[filtered_rows[col] == value_to_check]\n",
    "        if not matching_rows.empty:\n",
    "            print(f\"El valor '{value_to_check}' también aparece en la columna '{col}'\")\n",
    "\n",
    "# Ver filas donde el valor aparece solo en action_categorical_5\n",
    "only_in_action_5 = filtered_rows[~filtered_rows[categorical_features].isin([value_to_check]).any(axis=1)]\n",
    "print(f\"Filas donde el valor '{value_to_check}' solo aparece en 'action_categorical_5':\")\n",
    "print(only_in_action_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_to_check = '6bc0e29c'\n",
    "filtered_rows = test[test['action_categorical_5'] == value_to_check]\n",
    "\n",
    "# Verificar si el valor aparece en otras columnas categóricas\n",
    "for col in categorical_features:\n",
    "    if col != 'action_categorical_5':  # Evitar verificar la columna original\n",
    "        matching_rows = filtered_rows[filtered_rows[col] == value_to_check]\n",
    "        if not matching_rows.empty:\n",
    "            print(f\"El valor '{value_to_check}' también aparece en la columna '{col}'\")\n",
    "\n",
    "# Ver filas donde el valor aparece solo en action_categorical_5\n",
    "only_in_action_5 = filtered_rows[~filtered_rows[categorical_features].isin([value_to_check]).any(axis=1)]\n",
    "print(f\"Filas donde el valor '{value_to_check}' solo aparece en 'action_categorical_5':\")\n",
    "print(only_in_action_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_to_check = '79ceee49'\n",
    "filtered_rows = test[test['action_categorical_5'] == value_to_check]\n",
    "\n",
    "# Verificar si el valor aparece en otras columnas categóricas\n",
    "for col in categorical_features:\n",
    "    if col != 'action_categorical_5':  # Evitar verificar la columna original\n",
    "        matching_rows = filtered_rows[filtered_rows[col] == value_to_check]\n",
    "        if not matching_rows.empty:\n",
    "            print(f\"El valor '{value_to_check}' también aparece en la columna '{col}'\")\n",
    "\n",
    "# Ver filas donde el valor aparece solo en action_categorical_5\n",
    "only_in_action_5 = filtered_rows[~filtered_rows[categorical_features].isin([value_to_check]).any(axis=1)]\n",
    "print(f\"Filas donde el valor '{value_to_check}' solo aparece en 'action_categorical_5':\")\n",
    "print(only_in_action_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Levels features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_features = ['action_categorical_0', 'action_categorical_1', 'action_categorical_2', 'action_categorical_3', 'action_categorical_4']\n",
    "\n",
    "print(f\"Valores únicos y su frecuencia:\")\n",
    "for column in train_data.columns:\n",
    "    if column in level_features:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)  # Separador entre columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle para contar los NaNs en cada columna\n",
    "for column in level_features:\n",
    "    num_nans = train_data[column].isna().sum()\n",
    "    print(f\"Columna {column} tiene {num_nans} valores NaN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una columna con las combinaciones únicas de las 5 columnas\n",
    "train_data['combination'] = (train_data['action_categorical_0'].astype(str) + '_' +\n",
    "                             train_data['action_categorical_1'].astype(str) + '_' +\n",
    "                             train_data['action_categorical_2'].astype(str) + '_' +\n",
    "                             train_data['action_categorical_3'].astype(str) + '_' +\n",
    "                             train_data['action_categorical_4'].astype(str))\n",
    "\n",
    "# Obtener las combinaciones únicas y sus frecuencias\n",
    "combination_counts = train_data['combination'].value_counts().reset_index()\n",
    "combination_counts.columns = ['Combination', 'Frequency']\n",
    "\n",
    "# Mostrar las primeras 10 combinaciones más comunes\n",
    "print(combination_counts.head(25))\n",
    "print(f\"De: {len(combination_counts)} combinaciones únicas.\")\n",
    "\n",
    "# Graficar las combinaciones más frecuentes\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_combinations = combination_counts.head(25)  # Mostrar las 25 combinaciones más comunes\n",
    "sns.barplot(x='Frequency', y='Combination', data=top_combinations)\n",
    "plt.title(\"Top 25 Combinations of Action Categorical Columns out of {} Unique Combinations\".format(len(combination_counts)))\n",
    "plt.xlabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinaciones menos comunes (< 10000 veces)\n",
    "print(combination_counts[combination_counts['Frequency'] <= 10000])\n",
    "\n",
    "# Suma de todas las frecuencias de las combinaciones menos comunes\n",
    "print(f\"Suma de las frecuencias de las combinaciones menos comunes: {combination_counts[combination_counts['Frequency'] <= 10000]['Frequency'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boolean Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_features = ['auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2']\n",
    "\n",
    "for column in train_data.columns:\n",
    "    if column in boolean_features:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle para contar los NaNs en cada columna\n",
    "for column in boolean_features:\n",
    "    num_nans = train_data[column].isna().sum()\n",
    "    print(f\"Columna {column} tiene {num_nans} valores NaN.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una funcion para resolverl el problema de los valores booleanos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame de ejemplo\n",
    "data = {\n",
    "    'auction_boolean_0': ['47980dda', '43c867fd', None, '79ceee49'],\n",
    "    'auction_boolean_1': ['79ceee49', None, '79ceee49', None],\n",
    "    'auction_boolean_2': ['65dcab89', None, '43c867fd', '65dcab89'],\n",
    "    'Label': [0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "# Crear el DataFrame\n",
    "train_data = pd.DataFrame(data)\n",
    "\n",
    "# Imprimir el DataFrame original\n",
    "print(\"DataFrame Original:\")\n",
    "print(train_data)\n",
    "\n",
    "# Aplicar la función personalizada de one-hot encoding\n",
    "train_data_encoded = boolean_features_ohe(train_data)\n",
    "\n",
    "# Imprimir el DataFrame resultante\n",
    "print(\"\\nDataFrame después del One-Hot Encoding personalizado:\")\n",
    "print(train_data_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_features = ['auction_time', 'auction_age', 'timezone_offset']\n",
    "\n",
    "for column in train_data.columns:\n",
    "    if column in time_features:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle para contar los NaNs en cada columna\n",
    "for column in time_features:\n",
    "    num_nans = train_data[column].isna().sum()\n",
    "    print(f\"Columna {column} tiene {num_nans} valores NaN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegúrate de tener la columna 'auction_age' en tu train\n",
    "if 'auction_age' in train_data.columns:\n",
    "    # Obtener los valores únicos y su frecuencia\n",
    "    unique_ages = train_data['auction_age'].value_counts().sort_index()\n",
    "\n",
    "    # Imprimir cada edad y su frecuencia\n",
    "    for age, frequency in unique_ages.items():\n",
    "        print(f\"Edad: {age}, Frecuencia: {frequency}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funcion para crear atributos temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_features_extension\n",
    "\n",
    "data = {\n",
    "    'auction_time': [\n",
    "        1545676800,  # 24 de Diciembre de 2018, 22:00:00\n",
    "        1483228800,  # 31 de Diciembre de 2016, 23:00:00\n",
    "        1288396800,  # 30 de Octubre de 2010, 10:00:00\n",
    "        1412899200,  # 10 de Octubre de 2014, 15:00:00\n",
    "        1483574400   # 5 de Enero de 2017, 09:00:00\n",
    "    ],\n",
    "    'timezone_offset': [1, -2, 3, 0, 5]  # Diferentes zonas horarias\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = time_features_extension(df)\n",
    "\n",
    "# Imprimir el DataFrame resultante\n",
    "print(\"\\nDataFrame después de procesar el tiempo de subasta:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age_group\n",
    "\n",
    "data = {\n",
    "    'auction_age': [\n",
    "        -1, 15, 25, 35, 50, 65, 105, 80, 18, 99, 0, 579\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = age_group(df, 'auction_age')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quiero tomar las filas que sean de tipo label 1 y que me digan si esta cerca de una festividad o no con mi train_data\n",
    "\n",
    "# Crea un dataframe solo con las filas que tienen Label 1 a partir de train_data\n",
    "\n",
    "df = train_data[train_data['Label'] == 1]\n",
    "\n",
    "# Aplica la función time_features_extension al dataframe\n",
    "\n",
    "df = time_features_extension(df)\n",
    "\n",
    "# Imprimir las frecuencias de los valores que toman las columnas que se crearon con time_features_extension (week_day time_of_month moment_of_the_day  close_to_festivity)\n",
    "\n",
    "for column in ['week_day', 'time_of_month', 'moment_of_the_day']:\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos y su frecuencia:\")\n",
    "    print(df[column].value_counts())\n",
    "    print(\"-\" * 50)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica la función time_features_extension al dataframe\n",
    "\n",
    "test = time_features_extension(test)\n",
    "\n",
    "# Imprimir las frecuencias de los valores que toman las columnas que se crearon con time_features_extension (week_day time_of_month moment_of_the_day  close_to_festivity)\n",
    "\n",
    "for column in ['week_day', 'time_of_month', 'moment_of_the_day']:\n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos y su frecuencia:\")\n",
    "    print(test[column].value_counts())\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_features = ['auction_list_0', 'action_list_1', 'action_list_2', 'action_list_0']\n",
    "\n",
    "for column in train_data.columns:\n",
    "    if column in list_features:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle para contar los NaNs en cada columna\n",
    "for column in list_features:\n",
    "    num_nans = train_data[column].isna().sum()\n",
    "    print(f\"Columna {column} tiene {num_nans} valores NaN.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Funcion para hacer columnas de listas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataFrame de prueba más pequeño\n",
    "data = {\n",
    "    'auction_list_0': ['[\"IAB3\",\"utilities\", \"IAB22-2\"]', '[\"IAB19\",\"IAB4-5\", \"IAB8-9\"]', None, '[\"IAB3\",\"utilities\", \"IAB22-2\"]'],\n",
    "    'action_list_1': ['[-6779]', '[-6824, -6823]', None, '[-6824, -6823]' ],\n",
    "    'action_list_2': ['[6871, -6543, -6544]', '[-2560, -5902]', '[-6779]', None],\n",
    "    'action_list_0': ['IAB8-9', 'IAB22-2', 'IAB9-30', 'IAB9-30']\n",
    "}\n",
    "\n",
    "# Crear el DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print('Antes de utilizar la función expand_list_dummies_cython:')\n",
    "print(df)\n",
    "\n",
    "# Aplicar la función expand_list_dummies_cython en 'auction_list_0', 'action_list_1' y 'action_list_2'\n",
    "# Aquí se asume que ya has definido y aplicado `expand_list_dummies_cython`\n",
    "df = expand_list_dummies_cython(df, 'auction_list_0')\n",
    "df = expand_list_dummies_cython(df, 'action_list_1')\n",
    "df = expand_list_dummies_cython(df, 'action_list_2')\n",
    "\n",
    "print('Antes de utilizar la función expand_action_list_0:')\n",
    "print(df)\n",
    "\n",
    "print()\n",
    "\n",
    "print('Después de utilizar la función expand_action_list_0:')\n",
    "df = expand_action_list_0(df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pixels features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixels_features = ['creative_height', 'creative_width']\n",
    "\n",
    "for column in train_data.columns:\n",
    "    if column in pixels_features:\n",
    "        print(f\"Columna: {column}\")\n",
    "        print(f\"Valores únicos y su frecuencia:\")\n",
    "        print(train_data[column].value_counts())\n",
    "        print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucle para contar los NaNs en cada columna\n",
    "for column in pixels_features:\n",
    "    num_nans = train_data[column].isna().sum()\n",
    "    print(f\"Columna {column} tiene {num_nans} valores NaN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar las filas donde ambas columnas tienen valores NaN\n",
    "nans_both_columns = train_data[pixels_features].isna().all(axis=1).sum()\n",
    "\n",
    "print(f\"El número de filas donde ambas columnas tienen valores NaN es: {nans_both_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volver a filtrar las filas donde ambas columnas son NaN\n",
    "filtered_rows = train_data[train_data['creative_height'].isna() & train_data['creative_width'].isna()]\n",
    "\n",
    "# Calcular el porcentaje de cada valor de 'Label' en las filas donde ambas columnas tienen valores NaN\n",
    "label_counts = filtered_rows['Label'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Mostrar los resultados\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar las filas donde tanto creative_height como creative_width son NaN\n",
    "subset_nan = train_data[train_data['creative_height'].isna() & train_data['creative_width'].isna()]\n",
    "\n",
    "# Recorrer todas las columnas y mostrar particularidades en las 1,304,272 filas\n",
    "for column in train_data.columns:\n",
    "    unique_values = subset_nan[column].nunique()\n",
    "    num_nans = subset_nan[column].isna().sum()\n",
    "    \n",
    "    print(f\"Columna: {column}\")\n",
    "    print(f\"Valores únicos: {unique_values}\")\n",
    "    print(f\"Cantidad de NaNs: {num_nans}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis: Test vs Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En test pero no en train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_exclude = ['auction_list_0', 'action_list_1', 'action_list_2', 'auction_time', 'auction_boolean_0', 'auction_boolean_1', 'auction_boolean_2']\n",
    "categorical_features_ = [col for col in categorical_features if col not in columns_to_exclude]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un diccionario para almacenar las categorías desconocidas por columna\n",
    "categorias_desconocidas = {}\n",
    "\n",
    "# Iterar a través de cada columna categórica\n",
    "for columna in categorical_features_:\n",
    "    # Obtener las categorías únicas en el conjunto de entrenamiento\n",
    "    categorias_train = set(train_data[columna].dropna().unique())\n",
    "    \n",
    "    # Obtener las categorías únicas en el conjunto de prueba\n",
    "    categorias_test = set(test[columna].dropna().unique())\n",
    "    \n",
    "    # Identificar las categorías en test que no están en train\n",
    "    desconocidas = categorias_test - categorias_train\n",
    "    \n",
    "    # Almacenar las categorías desconocidas en el diccionario como una lista\n",
    "    categorias_desconocidas[columna] = list(desconocidas)\n",
    "\n",
    "    # Inicializamos una máscara booleana que será True si la fila tiene una categoría desconocida en cualquier columna\n",
    "mask = pd.Series([False] * len(test))\n",
    "\n",
    "# Iteramos a través de cada columna categórica\n",
    "for columna in categorias_desconocidas:\n",
    "    # Verificamos si el valor en la columna está dentro de las categorías desconocidas\n",
    "    mask = mask | test[columna].isin(categorias_desconocidas[columna])\n",
    "\n",
    "# Contamos el número de filas donde al menos una categoría es desconocida\n",
    "num_filas_desconocidas = mask.sum()\n",
    "\n",
    "print(f\"Cantidad de filas con al menos una categoría desconocida (existe en test y no en train): {num_filas_desconocidas} ({num_filas_desconocidas / len(test) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En train pero no en test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un diccionario para almacenar las categorías desconocidas por columna\n",
    "categorias_conocidas = {}\n",
    "\n",
    "# Iterar a través de cada columna categórica\n",
    "for columna in categorical_features_:\n",
    "    # Obtener las categorías únicas en el conjunto de entrenamiento\n",
    "    categorias_test = set(test[columna].dropna().unique())\n",
    "    \n",
    "    # Obtener las categorías únicas en el conjunto de prueba\n",
    "    categorias_train = set(train_data[columna].dropna().unique())\n",
    "    \n",
    "    # Identificar las categorías en test que no están en train\n",
    "    conocidas = categorias_train - categorias_test\n",
    "    \n",
    "    # Almacenar las categorías desconocidas en el diccionario como una lista\n",
    "    categorias_conocidas[columna] = list(conocidas)\n",
    "\n",
    "# Inicializamos una máscara booleana que será True si la fila tiene una categoría desconocida en cualquier columna\n",
    "mask = pd.Series([False] * len(test))\n",
    "\n",
    "# Iteramos a través de cada columna categórica\n",
    "for columna in categorias_conocidas:\n",
    "    # Verificamos si el valor en la columna está dentro de las categorías desconocidas\n",
    "    mask = mask | train_data[columna].isin(categorias_conocidas[columna])\n",
    "\n",
    "# Contamos el número de filas donde al menos una categoría es desconocida\n",
    "num_filas_conocidas = mask.sum()\n",
    "\n",
    "print(f\"Cantidad de filas con al menos una categoría que no existe en test: {num_filas_conocidas} ({num_filas_conocidas / len(train_data) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_combined, _ = train_test_split(\n",
    "    train_data, \n",
    "    train_size=1000000, \n",
    "    random_state=random_state, \n",
    "    stratify=train_data['Label']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y validación con estratificación\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_data_combined.drop(columns='Label'),  # Características\n",
    "    train_data_combined['Label'],               # Variable objetivo\n",
    "    test_size=0.2,                              # 20% para validación\n",
    "    stratify=train_data_combined['Label'],      # Estratificación basada en la variable objetivo\n",
    "    random_state=random_state                   # Semilla para reproducibilidad\n",
    ")\n",
    "\n",
    "# Verificar la proporción de clases en el conjunto de entrenamiento y validación\n",
    "print(\"Proporción en el conjunto de entrenamiento:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "\n",
    "print(\"Proporción en el conjunto de validación:\")\n",
    "print(y_val.value_counts(normalize=True))\n",
    "\n",
    "del train_data_combined, train_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = process_optimized(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = process_optimized(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las columnas categóricas y numéricas\n",
    "categorical_features_to_encode = X_train.select_dtypes(include=['object']).columns\n",
    "numeric_features = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Preprocesador común para imputación y codificación\n",
    "common_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Imputar valores numéricos con la media\n",
    "        ('num', SimpleImputer(strategy='mean'), numeric_features),\n",
    "        \n",
    "        # Imputar valores categóricos con 'Desconocido' y aplicar One-Hot Encoding\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='Desconocido')),\n",
    "            ('onehot', OneHotEncoder(drop='first', sparse_output=True, handle_unknown='ignore'))\n",
    "        ]), categorical_features_to_encode)\n",
    "    ],\n",
    "    remainder='drop'  # Excluir columnas no especificadas\n",
    ")\n",
    "\n",
    "# Definir el modelo XGBoost\n",
    "model_xgb = XGBClassifier(\n",
    "    random_state=random_state, \n",
    "    use_label_encoder=False, \n",
    "    eval_metric='auc'\n",
    ")\n",
    "\n",
    "# Pipeline para XGBoost (mantiene matrices dispersas)\n",
    "pipeline_xgb = Pipeline(steps=[\n",
    "    ('preprocessor', common_preprocessor),\n",
    "    ('classifier', model_xgb),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperot XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el espacio de búsqueda para los hiperparámetros de XGBoost\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 10)), \n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'n_estimators': hp.choice('n_estimators', [100, 200, 300, 400, 500]), \n",
    "    'gamma': hp.uniform('gamma', 0, 0.5),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "    'scale_pos_weight': hp.uniform('scale_pos_weight', 0.5, 2),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 1),  # L1 regularización\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 1),  # L2 regularización\n",
    "    'colsample_bylevel': hp.uniform('colsample_bylevel', 0.5, 1),  # Muestreo a nivel de split\n",
    "    'colsample_bynode': hp.uniform('colsample_bynode', 0.5, 1),  # Muestreo por nodo\n",
    "    'grow_policy': hp.choice('grow_policy', ['depthwise', 'lossguide']),  # Estrategia de crecimiento\n",
    "    'tree_method': hp.choice('tree_method', ['auto', 'approx', 'hist'])  # Métodos de construcción del árbol\n",
    "}\n",
    "\n",
    "# Función objetivo para Hyperopt utilizando validación cruzada\n",
    "def objective_xgb(params):\n",
    "    \n",
    "    # Asegurar que los parámetros sean del tipo correcto\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['min_child_weight'] = int(params['min_child_weight'])\n",
    "    \n",
    "    # Definir el modelo con los hiperparámetros actuales\n",
    "    model_xgb = XGBClassifier(\n",
    "        max_depth=params['max_depth'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        n_estimators=params['n_estimators'],\n",
    "        gamma=params['gamma'],\n",
    "        min_child_weight=params['min_child_weight'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        scale_pos_weight=params['scale_pos_weight'],\n",
    "        reg_alpha=params['reg_alpha'],  # L1 regularización\n",
    "        reg_lambda=params['reg_lambda'],  # L2 regularización\n",
    "        colsample_bylevel=params['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "        colsample_bynode=params['colsample_bynode'],  # Muestreo por nodo\n",
    "        grow_policy=params['grow_policy'],  # Estrategia de crecimiento\n",
    "        tree_method=params['tree_method'],  # Método de construcción del árbol\n",
    "        use_label_encoder=False,  # Evitar advertencias en versiones más recientes de XGBoost\n",
    "        eval_metric='auc',\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Pipeline para XGBoost (mantiene matrices dispersas)\n",
    "    pipeline_xgb = Pipeline(steps=[\n",
    "        ('preprocessor', common_preprocessor),\n",
    "        ('classifier', model_xgb),\n",
    "    ])\n",
    "    \n",
    "    # Definir la validación cruzada\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Calcular el AUC utilizando validación cruzada\n",
    "    auc = cross_val_score(pipeline_xgb, X_train, y_train, cv=cv, scoring='roc_auc').mean()\n",
    "    \n",
    "    # Opcional: imprimir los hiperparámetros y el AUC actual\n",
    "    print(f\"Hiperparámetros: {params}, AUC: {auc:.4f}\")\n",
    "    \n",
    "    # Retornar el valor a minimizar (1 - AUC)\n",
    "    return {'loss': 1 - auc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la optimización\n",
    "trials = Trials()\n",
    "\n",
    "best_xgb = fmin(\n",
    "    fn=objective_xgb,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=5,\n",
    "    trials=trials,\n",
    "    rstate=np.random.default_rng(random_state)  # Asegurar reproducibilidad\n",
    ")\n",
    "\n",
    "# No es necesario volver a mapear los hiperparámetros aquí, ya se hizo dentro de la función objetivo\n",
    "print(\"Mejores hiperparámetros para XGBoost:\")\n",
    "print(best_xgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRUEBA CON TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_val, y_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/ctr_test.csv')\n",
    "\n",
    "# X_test = process_data_with_dask(test_data, npartitions=10)\n",
    "\n",
    "X_test = process_optimized(test_data)\n",
    "del test_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparámetros: 'learning_rate': 0.1993070315803166, 'max_depth': 6, 'min_child_weight': 7, 'n_estimators': 100, 'reg_alpha': 0.32213467759367453, 'reg_lambda': 0.8036297375321274, 'scale_pos_weight': 1.4841116094332476, 'subsample': 0.8064682844784123, 'tree_method': 'approx'}, AUC: 0.8502\n",
    "\n",
    "# Reconstruir el modelo de XGBoost con los mejores hiperparámetros\n",
    "best_model_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1993070315803166,\n",
    "    subsample=0.8064682844784123,\n",
    "    colsample_bytree=0.6615659784543418,\n",
    "    min_child_weight=7,\n",
    "    scale_pos_weight=1.4841116094332476,\n",
    "    gamma= 0.10922672283247337,\n",
    "    reg_alpha=0.32213467759367453,  # L1 regularización\n",
    "    reg_lambda=0.8036297375321274,  # L2 regularización\n",
    "    colsample_bylevel=0.7846542046759237,  # Muestreo a nivel de split\n",
    "    colsample_bynode=0.8602953305997507,  # Muestreo por nodo\n",
    "    grow_policy='lossguide',  # Estrategia de crecimiento\n",
    "    tree_method='approx',  # Método de construcción del árbol\n",
    "    random_state=random_state,\n",
    "    use_label_encoder=False,  # Evitar advertencias en versiones más recientes de XGBoost\n",
    "    eval_metric=\"auc\"  # Establecer AUC como métrica de evaluación\n",
    ")\n",
    "\n",
    "# Crear un nuevo pipeline reutilizando el preprocesador original y el mejor modelo\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', common_preprocessor),\n",
    "    ('classifier', model_xgb),\n",
    "])\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "xgb_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices\n",
    "tree_method_options = ['auto', 'exact', 'approx', 'hist']\n",
    "grow_policy_options = ['depthwise', 'lossguide']\n",
    "n_estimators_options = [100, 200, 300, 400, 500]\n",
    "\n",
    "# Reconstruir el modelo de XGBoost con los mejores hiperparámetros\n",
    "best_model_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=n_estimators_options[best_xgb['n_estimators']],\n",
    "    max_depth=best_xgb['max_depth'],\n",
    "    learning_rate=best_xgb['learning_rate'],\n",
    "    subsample=best_xgb['subsample'],\n",
    "    colsample_bytree=best_xgb['colsample_bytree'],\n",
    "    min_child_weight=best_xgb['min_child_weight'],\n",
    "    gamma=best_xgb['gamma'],\n",
    "    scale_pos_weight=best_xgb['scale_pos_weight'],\n",
    "    reg_alpha=best_xgb['reg_alpha'],  # L1 regularización\n",
    "    reg_lambda=best_xgb['reg_lambda'],  # L2 regularización\n",
    "    colsample_bylevel=best_xgb['colsample_bylevel'],  # Muestreo a nivel de split\n",
    "    colsample_bynode=best_xgb['colsample_bynode'],  # Muestreo por nodo\n",
    "    grow_policy=grow_policy_options[best_xgb['grow_policy']],  # Estrategia de crecimiento\n",
    "    tree_method=tree_method_options[best_xgb['tree_method']],  # Método de construcción del árbol\n",
    "    random_state=random_state,\n",
    "    use_label_encoder=False,  # Evitar advertencias en versiones más recientes de XGBoost\n",
    "    eval_metric=\"auc\"  # Establecer AUC como métrica de evaluación\n",
    ")\n",
    "\n",
    "# Crear un nuevo pipeline reutilizando el preprocesador original y el mejor modelo\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', common_preprocessor),\n",
    "    ('classifier', model_xgb),\n",
    "])\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "xgb_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir en el conjunto de testeo\n",
    "y_preds_xgb_test = xgb_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Crear el archivo de envío\n",
    "submission_df_xgb = pd.DataFrame({\"id\": test_data[\"id\"], \"Label\": y_preds_xgb_test})\n",
    "submission_df_xgb[\"id\"] = submission_df_xgb[\"id\"].astype(int)\n",
    "\n",
    "# Crear el nombre del archivo basado en los mejores hiperparámetros\n",
    "file_name_xgb = (\n",
    "    f\"xgboost_preds_ne_{best_xgb['n_estimators']}_\"\n",
    "    f\"md_{handle_none(best_xgb['max_depth'])}_\"\n",
    "    f\"lr_{round(best_xgb['learning_rate'], 2)}_\"\n",
    "    f\"ss_{round(best_xgb['subsample'], 2)}_\"\n",
    "    f\"csb_{round(best_xgb['colsample_bytree'], 2)}_\"\n",
    "    f\"cb_level_{round(best_xgb['colsample_bylevel'], 2)}_\"\n",
    "    f\"cb_node_{round(best_xgb['colsample_bynode'], 2)}_\"\n",
    "    f\"mcw_{round(best_xgb['min_child_weight'], 2)}_\"\n",
    "    f\"gamma_{round(best_xgb['gamma'], 2)}_\"\n",
    "    f\"ra_{round(best_xgb['reg_alpha'], 2)}_\"\n",
    "    f\"rl_{round(best_xgb['reg_lambda'], 2)}_\"\n",
    "    f\"spw_{round(best_xgb['scale_pos_weight'], 2)}_\"\n",
    "    f\"gp_{grow_policy_options[best_xgb['grow_policy']]}_\"\n",
    "    f\"tm_{tree_method_options[best_xgb['tree_method']]}.csv\"\n",
    ")\n",
    "\n",
    "# Crear la carpeta \"submits\" si no existe\n",
    "os.makedirs(\"submits\", exist_ok=True)\n",
    "\n",
    "# Guardar el archivo de predicción en la carpeta 'submits'\n",
    "submission_df_xgb.to_csv(os.path.join(\"submits\", file_name_xgb), sep=\",\", index=False)\n",
    "\n",
    "print(f\"Archivo guardado en: submits/{file_name_xgb}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TDVI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
